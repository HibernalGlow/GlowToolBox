"""
Multiæ–‡ä»¶åˆ†æå™¨æ¨¡å—
æä¾›å¯¹å‹ç¼©åŒ…æ–‡ä»¶çš„å®½åº¦ã€é¡µæ•°å’Œæ¸…æ™°åº¦åˆ†æåŠŸèƒ½
æ”¯æŒå‘½ä»¤è¡Œå•ç‹¬è¿è¡Œ
"""

import os
import logging
from typing import List, Dict, Tuple, Union, Optional
from pathlib import Path
import zipfile
from PIL import Image, ImageFile
import pillow_avif
import pillow_jxl
import warnings
import cv2
import numpy as np
from io import BytesIO
import random
from concurrent.futures import ThreadPoolExecutor
from nodes.pics.calculate_hash_custom import ImageClarityEvaluator
from nodes.utils.number_shortener import shorten_number_cn
import re
from nodes.pics.group_analyzer import GroupAnalyzer

# æŠ‘åˆ¶æ‰€æœ‰è­¦å‘Š
warnings.filterwarnings('ignore')
# å…è®¸æˆªæ–­çš„å›¾åƒæ–‡ä»¶
ImageFile.LOAD_TRUNCATED_IMAGES = True
# è®¾ç½®OpenCVçš„é”™è¯¯å¤„ç†
 # é™åˆ¶OpenCVçº¿ç¨‹æ•°
 # åªæ˜¾ç¤ºé”™è¯¯æ—¥å¿—

logger = logging.getLogger(__name__)

class MultiAnalyzer:
    """Multiæ–‡ä»¶åˆ†æå™¨ï¼Œç”¨äºåˆ†æå‹ç¼©åŒ…ä¸­å›¾ç‰‡çš„å®½åº¦ã€é¡µæ•°å’Œæ¸…æ™°åº¦"""
    
    def __init__(self, sample_count: int = 3):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            sample_count: æ¯ä¸ªå‹ç¼©åŒ…æŠ½å–çš„å›¾ç‰‡æ ·æœ¬æ•°é‡
        """
        self.sample_count = sample_count
        self.supported_extensions = {
            '.jpg', '.jpeg', '.png', '.webp', '.avif', 
            '.jxl', '.gif', '.bmp', '.tiff', '.tif', 
            '.heic', '.heif'
        }
    
    def get_archive_info(self, archive_path: str) -> List[Tuple[str, int]]:
        """è·å–å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶ä¿¡æ¯"""
        try:
            image_files = []
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for info in zf.infolist():
                    ext = os.path.splitext(info.filename.lower())[1]
                    if ext in self.supported_extensions:
                        image_files.append((info.filename, info.file_size))
            return image_files
        except Exception as e:
            logger.error(f"è·å–å‹ç¼©åŒ…ä¿¡æ¯å¤±è´¥ {archive_path}: {str(e)}")
            return []

    def get_image_count(self, archive_path: str) -> int:
        """è®¡ç®—å‹ç¼©åŒ…ä¸­çš„å›¾ç‰‡æ€»æ•°"""
        image_files = self.get_archive_info(archive_path)
        return len(image_files)

    def _safe_open_image(self, img_data: bytes) -> Optional[Image.Image]:
        """å®‰å…¨åœ°æ‰“å¼€å›¾ç‰‡ï¼Œå¤„ç†å¯èƒ½çš„è§£ç é”™è¯¯
        
        Args:
            img_data: å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
            
        Returns:
            Optional[Image.Image]: æˆåŠŸåˆ™è¿”å›PILå›¾åƒå¯¹è±¡ï¼Œå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # é¦–å…ˆå°è¯•ç”¨PILç›´æ¥æ‰“å¼€
            img = Image.open(BytesIO(img_data))
            img.verify()  # éªŒè¯å›¾åƒå®Œæ•´æ€§
            return Image.open(BytesIO(img_data))  # é‡æ–°æ‰“å¼€ä»¥ä¾¿åç»­ä½¿ç”¨
        except Exception as e1:
            try:
                # å¦‚æœPILéªŒè¯å¤±è´¥ï¼Œå°è¯•ç”¨OpenCVæ‰“å¼€
                nparr = np.frombuffer(img_data, np.uint8)
                img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                if img is None:
                    raise ValueError("OpenCVæ— æ³•è§£ç å›¾åƒ")
                # è½¬æ¢ä¸ºRGB
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                return Image.fromarray(img_rgb)
            except Exception as e2:
                try:
                    # æœ€åå°è¯•ç›´æ¥ç”¨PILæ‰“å¼€è€Œä¸éªŒè¯
                    return Image.open(BytesIO(img_data))
                except Exception as e3:
                    logger.debug(f"å›¾åƒè§£ç å¤±è´¥: PIL1={str(e1)}, CV2={str(e2)}, PIL2={str(e3)}")
                    return None

    def calculate_representative_width(self, archive_path: str) -> int:
        """è®¡ç®—å‹ç¼©åŒ…ä¸­å›¾ç‰‡çš„ä»£è¡¨å®½åº¦ï¼ˆä½¿ç”¨æŠ½æ ·å’Œä¸­ä½æ•°ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            ext = os.path.splitext(archive_path)[1].lower()
            if ext not in {'.zip', '.cbz'}:  # åªå¤„ç†zipæ ¼å¼
                return 0

            # è·å–å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶ä¿¡æ¯
            image_files = []
            try:
                with zipfile.ZipFile(archive_path, 'r') as zf:
                    for info in zf.infolist():
                        if os.path.splitext(info.filename.lower())[1] in self.supported_extensions:
                            image_files.append((info.filename, info.file_size))
            except zipfile.BadZipFile:
                logger.error(f"æ— æ•ˆçš„ZIPæ–‡ä»¶: {archive_path}")
                return 0

            if not image_files:
                return 0

            # æŒ‰æ–‡ä»¶å¤§å°æ’åº
            image_files.sort(key=lambda x: x[1], reverse=True)
            
            # é€‰æ‹©æ ·æœ¬
            samples = []
            if image_files:
                samples.append(image_files[0][0])  # æœ€å¤§çš„æ–‡ä»¶
                if len(image_files) > 2:
                    samples.append(image_files[len(image_files)//2][0])  # ä¸­é—´çš„æ–‡ä»¶
                
                # ä»å‰30%é€‰æ‹©å‰©ä½™æ ·æœ¬
                top_30_percent = image_files[:max(3, len(image_files) // 3)]
                while len(samples) < self.sample_count and top_30_percent:
                    sample = random.choice(top_30_percent)[0]
                    if sample not in samples:
                        samples.append(sample)

            widths = []
            try:
                with zipfile.ZipFile(archive_path, 'r') as zf:
                    for sample in samples:
                        try:
                            with zf.open(sample) as file:
                                img_data = file.read()
                                img = self._safe_open_image(img_data)
                                if img is not None:
                                    widths.append(img.width)
                        except Exception as e:
                            logger.error(f"è¯»å–å›¾ç‰‡å®½åº¦å¤±è´¥ {sample}: {str(e)}")
                            continue
            except Exception as e:
                logger.error(f"æ‰“å¼€ZIPæ–‡ä»¶å¤±è´¥: {str(e)}")
                return 0

            if not widths:
                return 0

            # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºä»£è¡¨å®½åº¦
            return int(sorted(widths)[len(widths)//2])

        except Exception as e:
            logger.error(f"è®¡ç®—ä»£è¡¨å®½åº¦å¤±è´¥ {archive_path}: {str(e)}")
            return 0

    def calculate_clarity_score(self, archive_path: str) -> float:
        """è®¡ç®—å‹ç¼©åŒ…ä¸­å›¾ç‰‡çš„æ¸…æ™°åº¦è¯„åˆ†"""
        try:
            # è·å–å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶ä¿¡æ¯
            image_files = self.get_archive_info(archive_path)
            if not image_files:
                return 0.0

            # æŒ‰æ–‡ä»¶å¤§å°æ’åºå¹¶é€‰æ‹©æ ·æœ¬
            image_files.sort(key=lambda x: x[1], reverse=True)
            samples = []
            if image_files:
                samples.append(image_files[0][0])  # æœ€å¤§çš„æ–‡ä»¶
                if len(image_files) > 2:
                    samples.append(image_files[len(image_files)//2][0])  # ä¸­é—´çš„æ–‡ä»¶
                
                # ä»å‰30%é€‰æ‹©å‰©ä½™æ ·æœ¬
                top_30_percent = image_files[:max(3, len(image_files) // 3)]
                while len(samples) < self.sample_count and top_30_percent:
                    sample = random.choice(top_30_percent)[0]
                    if sample not in samples:
                        samples.append(sample)

            # è®¡ç®—æ ·æœ¬çš„æ¸…æ™°åº¦è¯„åˆ†
            scores = []
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for sample in samples:
                    try:
                        with zf.open(sample) as f:
                            img_data = f.read()
                            # ç›´æ¥ä¼ é€’äºŒè¿›åˆ¶æ•°æ®ç»™æ¸…æ™°åº¦è®¡ç®—å‡½æ•°
                            try:
                                score = ImageClarityEvaluator.calculate_definition(img_data)
                                if score and score > 0:  # ç¡®ä¿å¾—åˆ°æœ‰æ•ˆçš„åˆ†æ•°
                                    scores.append(score)
                            except Exception as e:
                                logger.debug(f"æ¸…æ™°åº¦è®¡ç®—å¤±è´¥ {sample}: {str(e)}")
                    except Exception as e:
                        logger.debug(f"å¤„ç†å›¾åƒå¤±è´¥ {sample}: {str(e)}")
                        continue

            # è¿”å›å¹³å‡æ¸…æ™°åº¦è¯„åˆ†
            return float(sum(scores) / len(scores)) if scores else 0.0

        except Exception as e:
            logger.error(f"è®¡ç®—æ¸…æ™°åº¦è¯„åˆ†å¤±è´¥ {archive_path}: {str(e)}")
            return 0.0

    def analyze_archive(self, archive_path: str) -> Dict[str, Union[int, float]]:
        """åˆ†æå‹ç¼©åŒ…ï¼Œè¿”å›å®½åº¦ã€é¡µæ•°å’Œæ¸…æ™°åº¦ä¿¡æ¯"""
        result = {
            'width': 0,
            'page_count': 0,
            'clarity_score': 0.0
        }
        
        try:
            # åˆ†åˆ«è®¡ç®—å„é¡¹æŒ‡æ ‡ï¼Œå¤±è´¥ä¸€é¡¹ä¸å½±å“å…¶ä»–é¡¹
            try:
                result['page_count'] = self.get_image_count(archive_path)
            except Exception as e:
                logger.error(f"è®¡ç®—é¡µæ•°å¤±è´¥ {archive_path}: {str(e)}")
                
            try:
                result['width'] = self.calculate_representative_width(archive_path)
            except Exception as e:
                logger.error(f"è®¡ç®—å®½åº¦å¤±è´¥ {archive_path}: {str(e)}")
                
            try:
                result['clarity_score'] = self.calculate_clarity_score(archive_path)
            except Exception as e:
                logger.error(f"è®¡ç®—æ¸…æ™°åº¦å¤±è´¥ {archive_path}: {str(e)}")
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ†æå‹ç¼©åŒ…å¤±è´¥ {archive_path}: {str(e)}")
            return result

    def format_analysis_result(self, result: Dict[str, Union[int, float]]) -> str:
        """æ ¼å¼åŒ–åˆ†æç»“æœä¸ºå­—ç¬¦ä¸²"""
        width = result['width']
        count = result['page_count']
        clarity = result['clarity_score']
        
        parts = []
        if width > 0:
            width_str = shorten_number_cn(width, use_w=True)
            parts.append(f"{width_str}@WD")
        if count > 0:
            count_str = shorten_number_cn(count, use_w=True)
            parts.append(f"{count_str}@PX")
        if clarity > 0:
            clarity_int = int(clarity)
            clarity_str = shorten_number_cn(clarity_int, use_w=True)
            parts.append(f"{clarity_str}@DE")
            
        return "{" + ",".join(parts) + "}" if parts else ""

    def process_file_with_count(self, file_path: str, base_dir: str = "") -> Tuple[str, str, Dict[str, Union[int, float]]]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŸå§‹è·¯å¾„ã€æ–°è·¯å¾„å’Œåˆ†æç»“æœ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            base_dir: åŸºç¡€ç›®å½•ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Tuple[str, str, Dict]: åŸå§‹è·¯å¾„ã€æ–°è·¯å¾„å’Œåˆ†æç»“æœçš„å…ƒç»„
        """
        # è·å–å®Œæ•´è·¯å¾„
        full_path = os.path.join(base_dir, file_path) if base_dir else file_path
        dir_name = os.path.dirname(full_path)
        file_name = os.path.basename(full_path)
        name, ext = os.path.splitext(file_name)
        
        # ç§»é™¤å·²æœ‰çš„æ ‡è®°
        name = re.sub(r'\{[^}]*@(?:PX|WD|DE)[^}]*\}', '', name)
        
        # åˆ†ææ–‡ä»¶
        result = self.analyze_archive(full_path)
        
        # æ„å»ºæ–°æ–‡ä»¶å
        formatted = self.format_analysis_result(result)
        if formatted:
            name = f"{name}{formatted}"
            
        # æ„å»ºæ–°çš„å®Œæ•´è·¯å¾„
        new_name = f"{name}{ext}"
        new_path = os.path.join(dir_name, new_name) if dir_name else new_name
        
        return full_path, new_path, result

    def process_directory_with_rename(self, input_path: str, do_rename: bool = False) -> List[Dict[str, Union[str, Dict[str, Union[int, float]]]]]:
        """å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¯é€‰æ‹©æ˜¯å¦é‡å‘½å"""
        results = []
        pending_renames = []  # å­˜å‚¨å¾…é‡å‘½åçš„æ–‡ä»¶ä¿¡æ¯
        group_analyzer = GroupAnalyzer()  # åˆ›å»ºç»„åˆ†æå™¨å®ä¾‹
        
        # ç”¨äºå­˜å‚¨æ–‡ä»¶ç»„
        file_groups = {}
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ–‡ä»¶å¹¶è¿›è¡Œåˆå§‹åˆ†æ
        if os.path.isfile(input_path):
            if input_path.lower().endswith(('.zip', '.cbz')):
                orig_path, new_path, analysis = self.process_file_with_count(input_path)
                result = {
                    'file': os.path.basename(input_path),
                    'orig_path': orig_path,
                    'analysis': analysis,
                    'formatted': self.format_analysis_result(analysis)
                }
                results.append(result)
                
        elif os.path.isdir(input_path):
            for root, _, files in os.walk(input_path):
                if 'trash' in root or 'multi' in root:
                    continue
                for file in files:
                    if file.lower().endswith(('.zip', '.cbz')):
                        file_path = os.path.join(root, file)
                        try:
                            orig_path, new_path, analysis = self.process_file_with_count(file_path)
                            result = {
                                'file': os.path.relpath(file_path, input_path),
                                'orig_path': orig_path,
                                'analysis': analysis,
                                'formatted': self.format_analysis_result(analysis)
                            }
                            results.append(result)
                            
                            # å°†æ–‡ä»¶æ·»åŠ åˆ°å¯¹åº”çš„ç»„
                            clean_name = group_analyzer.clean_filename(file)
                            if clean_name not in file_groups:
                                file_groups[clean_name] = []
                            file_groups[clean_name].append(result)
                            
                        except Exception as e:
                            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ¯ä¸ªæ–‡ä»¶ç»„ï¼Œæ‰¾å‡ºæœ€ä¼˜æŒ‡æ ‡
        for group_name, group_results in file_groups.items():
            if len(group_results) > 1:  # åªå¤„ç†æœ‰å¤šä¸ªæ–‡ä»¶çš„ç»„
                logger.info(f"ğŸ“¦ å¤„ç†æ–‡ä»¶ç»„: {group_name}")
                
                # æ‰¾å‡ºæœ€ä¼˜æŒ‡æ ‡
                best_metrics = {
                    'width': 0,  # æœ€å¤§å®½åº¦
                    'page_count': float('inf'),  # æœ€å°é¡µæ•°
                    'clarity_score': 0.0  # æœ€é«˜æ¸…æ™°åº¦
                }
                
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æŒ‡æ ‡éƒ½ç›¸åŒ
                metrics_same = {
                    'width': True,
                    'page_count': True,
                    'clarity_score': True
                }
                
                # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡å€¼
                all_metrics = {
                    'width': set(),
                    'page_count': set(),
                    'clarity_score': set()
                }
                
                # ç¬¬ä¸€è½®ï¼šæ”¶é›†æ‰€æœ‰å€¼å¹¶æ‰¾å‡ºæœ€ä¼˜å€¼
                for result in group_results:
                    analysis = result['analysis']
                    # æ”¶é›†æ‰€æœ‰å€¼
                    if analysis['width'] > 0:
                        all_metrics['width'].add(analysis['width'])
                    if analysis['page_count'] > 0:
                        all_metrics['page_count'].add(analysis['page_count'])
                    if analysis['clarity_score'] > 0:
                        all_metrics['clarity_score'].add(analysis['clarity_score'])
                    
                    # æ›´æ–°æœ€ä¼˜å€¼
                    best_metrics['width'] = max(best_metrics['width'], analysis['width'])
                    best_metrics['page_count'] = min(best_metrics['page_count'], analysis['page_count'])
                    best_metrics['clarity_score'] = max(best_metrics['clarity_score'], analysis['clarity_score'])
                
                # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡æ˜¯å¦éƒ½ç›¸åŒ
                metrics_same['width'] = len(all_metrics['width']) <= 1
                metrics_same['page_count'] = len(all_metrics['page_count']) <= 1
                metrics_same['clarity_score'] = len(all_metrics['clarity_score']) <= 1
                
                # è®°å½•æœ€ä¼˜æŒ‡æ ‡
                best_metrics_info = {
                    'width': best_metrics['width'],
                    'page_count': best_metrics['page_count'] if best_metrics['page_count'] != float('inf') else 0,
                    'clarity_score': best_metrics['clarity_score']
                }
                
                logger.info(f"ğŸ† ç»„æœ€ä¼˜æŒ‡æ ‡: å®½åº¦={best_metrics_info['width']}, é¡µæ•°={best_metrics_info['page_count']}, æ¸…æ™°åº¦={best_metrics_info['clarity_score']}")
                
                # ä¸ºæ¯ä¸ªæ–‡ä»¶æ›´æ–°æ ¼å¼åŒ–æŒ‡æ ‡
                for result in group_results:
                    analysis = result['analysis']
                    parts = []
                    
                    # æ·»åŠ å®½åº¦ï¼ˆå¦‚æœä¸æ˜¯ç»Ÿä¸€å€¼ä¸”æ˜¯æœ€ä¼˜å€¼åˆ™æ·»åŠ è¡¨æƒ…ï¼‰
                    if analysis['width'] > 0:
                        width_str = f"{analysis['width']}@WD"
                        if not metrics_same['width'] and analysis['width'] == best_metrics['width']:
                            width_str = f"ğŸ“{width_str}"
                        parts.append(width_str)
                    
                    # æ·»åŠ é¡µæ•°ï¼ˆå¦‚æœä¸æ˜¯ç»Ÿä¸€å€¼ä¸”æ˜¯æœ€ä¼˜å€¼åˆ™æ·»åŠ è¡¨æƒ…ï¼‰
                    if analysis['page_count'] > 0:
                        page_str = f"{analysis['page_count']}@PX"
                        if not metrics_same['page_count'] and analysis['page_count'] == best_metrics['page_count']:
                            page_str = f"ğŸ“„{page_str}"
                        parts.append(page_str)
                    
                    # æ·»åŠ æ¸…æ™°åº¦ï¼ˆå¦‚æœä¸æ˜¯ç»Ÿä¸€å€¼ä¸”æ˜¯æœ€ä¼˜å€¼åˆ™æ·»åŠ è¡¨æƒ…ï¼‰
                    if analysis['clarity_score'] > 0:
                        clarity_str = f"{int(analysis['clarity_score'])}@DE"
                        if not metrics_same['clarity_score'] and analysis['clarity_score'] == best_metrics['clarity_score']:
                            clarity_str = f"ğŸ”{clarity_str}"
                        parts.append(clarity_str)
                    
                    result['formatted'] = "{" + ",".join(parts) + "}" if parts else ""
        
        # ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡é‡å‘½åæ“ä½œ
        for result in results:
            orig_path = result['orig_path']
            dir_name = os.path.dirname(orig_path)
            file_name = os.path.basename(orig_path)
            name, ext = os.path.splitext(file_name)
            
            # ç§»é™¤å·²æœ‰çš„æ ‡è®°
            name = re.sub(r'\{[^}]*@(?:PX|WD|DE)[^}]*\}', '', name)
            
            # æ·»åŠ æ–°çš„æ ¼å¼åŒ–æŒ‡æ ‡
            if result['formatted']:
                name = f"{name}{result['formatted']}"
            
            # æ„å»ºæ–°çš„å®Œæ•´è·¯å¾„
            new_name = f"{name}{ext}"
            new_path = os.path.join(dir_name, new_name) if dir_name else new_name
            result['new_name'] = os.path.basename(new_path)
            
            if do_rename and orig_path != new_path:
                pending_renames.append((orig_path, new_path, result))
        
        # ç¬¬å››æ­¥ï¼šæ‰§è¡Œé‡å‘½åæ“ä½œ
        if do_rename and pending_renames:
            print("\nå¼€å§‹é‡å‘½åæ–‡ä»¶...")
            for orig_path, new_path, result in pending_renames:
                try:
                    if os.path.exists(orig_path):
                        os.rename(orig_path, new_path)
                        result['renamed'] = True
                        print(f"é‡å‘½åæˆåŠŸ: {os.path.basename(orig_path)} -> {os.path.basename(new_path)}")
                    else:
                        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {orig_path}")
                        result['renamed'] = False
                except Exception as e:
                    logger.error(f"é‡å‘½åå¤±è´¥ {orig_path}: {str(e)}")
                    result['renamed'] = False
                    print(f"é‡å‘½åå¤±è´¥: {os.path.basename(orig_path)} ({str(e)})")
                    
        return results

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºå‘½ä»¤è¡Œè¿è¡Œ"""
    import json
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO,
                       format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("=== Multiæ–‡ä»¶åˆ†æå™¨ ===")
    print("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„ï¼ˆè¾“å…¥qé€€å‡ºï¼‰ï¼š")
    
    while True:
        try:
            input_path = input("è·¯å¾„> ").strip()
            
            if input_path.lower() in ('q', 'quit', 'exit'):
                print("ç¨‹åºå·²é€€å‡º")
                break
                
            if not input_path:
                print("è¯·è¾“å…¥æœ‰æ•ˆè·¯å¾„")
                continue
                
            if not os.path.exists(input_path):
                print(f"é”™è¯¯ï¼šè·¯å¾„ '{input_path}' ä¸å­˜åœ¨")
                continue
            
            # è·å–æ ·æœ¬æ•°é‡
            sample_count = 3  # é»˜è®¤å€¼
            try:
                count_input = input("è¯·è¾“å…¥é‡‡æ ·æ•°é‡ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼3ï¼‰> ").strip()
                if count_input:
                    sample_count = int(count_input)
                    if sample_count < 1:
                        print("é‡‡æ ·æ•°é‡å¿…é¡»å¤§äº0ï¼Œä½¿ç”¨é»˜è®¤å€¼3")
                        sample_count = 3
            except ValueError:
                print("æ— æ•ˆçš„é‡‡æ ·æ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼3")
            
            # æ˜¯å¦æ‰§è¡Œé‡å‘½å
            do_rename = input("æ˜¯å¦é‡å‘½åæ–‡ä»¶ï¼Ÿ(y/N) ").strip().lower() == 'y'
            
            # æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶
            save_to_file = input("æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼Ÿ(y/N) ").strip().lower() == 'y'
            output_file = None
            if save_to_file:
                output_file = input("è¯·è¾“å…¥ä¿å­˜è·¯å¾„ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤è·¯å¾„ analysis_result.jsonï¼‰> ").strip()
                if not output_file:
                    output_file = "analysis_result.json"
            
            # æ‰§è¡Œåˆ†æ
            print("\nå¼€å§‹åˆ†æ...")
            analyzer = MultiAnalyzer(sample_count=sample_count)
            results = analyzer.process_directory_with_rename(input_path, do_rename)
            
            # è¾“å‡ºç»“æœ
            if output_file:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # æ€»æ˜¯åœ¨æ§åˆ¶å°æ˜¾ç¤ºç»“æœ
            print("\nåˆ†æç»“æœ:")
            for result in results:
                print(f"åŸæ–‡ä»¶: {result['file']}")
                if do_rename:
                    status = "æˆåŠŸ" if result.get('renamed', False) else "å¤±è´¥"
                    print(f"æ–°æ–‡ä»¶: {result['new_name']} (é‡å‘½å{status})")
                print(f"åˆ†æç»“æœ: {result['formatted']}")
                print("-" * 50)
            
            print("\nåˆ†æå®Œæˆï¼")
            print("\nè¯·è¾“å…¥æ–°çš„è·¯å¾„è¿›è¡Œåˆ†æï¼ˆè¾“å…¥qé€€å‡ºï¼‰ï¼š")
            
        except Exception as e:
            logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            print(f"é”™è¯¯: {str(e)}")
            print("\nè¯·è¾“å…¥æ–°çš„è·¯å¾„è¿›è¡Œåˆ†æï¼ˆè¾“å…¥qé€€å‡ºï¼‰ï¼š")

if __name__ == '__main__':
    main() 