from nodes.config.import_bundles import *

import fsspec

import importlib.util
import tempfile
# ----
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
spec = importlib.util.spec_from_file_location(
    "performance_config",
    # os.path.join(os.path.dirname(__file__), "configs/performance_config.py")
    r"D:\1VSCODE\GlowToolBox\src\nodes\config\performance_config.py"
)
performance_config = importlib.util.module_from_spec(spec)
spec.loader.exec_module(performance_config)
from nodes.config.performance_config import *
# ---
ConfigGUI = performance_config.ConfigGUI
from nodes.tui.textual_logger import TextualLoggerManager
vipshome = Path(r'D:\1VSCODE\1ehv\other\vips\bin')
if hasattr(os, 'add_dll_directory'):
    os.add_dll_directory(str(vipshome))
os.environ['PATH'] = str(vipshome) + ';' + os.environ['PATH']
import pyvips
# å…¨å±€é…ç½®
if hasattr(os, 'add_dll_directory'):
    os.add_dll_directory(str(vipshome))
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
# åœ¨å…¨å±€é…ç½®éƒ¨åˆ†æ·»åŠ ä»¥ä¸‹å†…å®¹
# ================= æ—¥å¿—é…ç½® =================
from nodes.record.logger_config import setup_logger

config = {
    'script_name': 'pics_convert',
    'console_enabled': False
}
logger, config_info = setup_logger(config)

# å…¨å±€å˜é‡
verbose_logging = True
use_direct_path_mode = True
restore_enabled = False
use_multithreading = True
filter_height_enabled = False
filter_white_enabled = False
handle_artbooks = False
add_processed_comment_enabled = False
add_processed_log_enabled = True
backup_removed_files_enabled = False
ignore_yaml_log = True
ignore_processed_log = True
wrap_log_lines = True  # æ§åˆ¶æ˜¯å¦å¯¹æ—¥å¿—è¿›è¡ŒæŠ˜è¡Œå¤„ç†
processed_files_yaml = 'E:\\1EHV\\processed_files.yaml'
artbook_keywords = []
exclude_paths = []
min_size = 639
white_threshold = 8
white_score_threshold = 0.92
threshold = 1
max_workers = min(4, os.cpu_count() or 4)

# å…¨å±€å¸¸é‡
INCLUDED_KEYWORDS = ['æ±‰åŒ–', 'å®˜æ–¹', 'ä¸­æ–‡', 'æ¼¢åŒ–', 'æƒ', 'ä¿®æ­£', 'åˆ¶', 'è­¯', 'ä¸ªäºº', 'ç¿»', 'è£½', 'åµŒ', 'è¨³', 'æ·«ä¹¦é¦†']
PERFORMANCE_CONFIG_PATH = r"D:\1VSCODE\1ehv\archive\config\performance_config.py"
ENCRYPTION_KEY = 'HibernalGlow'
FILENAME_MAPPING_FILE = 'filename_mapping.json'

# å›¾ç‰‡è½¬æ¢é…ç½®
IMAGE_CONVERSION_CONFIG = {
    'source_formats': {'.jpg', '.jpeg', '.png', '.webp', '.bmp', '.avif', '.jxl'},
    'target_format': '.avif',
    'webp_config': {
        'quality': 90,
        'method': 4,
        'lossless': False,
        'strip': True
    },
    'avif_config': {
        'quality': 90,
        'speed': 7,
        'chroma_quality': 100,
        'lossless': False,
        'strip': True
    },
    'jxl_config': {
        'quality': 90,
        'effort': 7,
        'lossless': False,
        'modular': False,
        'jpeg_recompression': False,
        'jpeg_lossless': False,
        'strip': True
    },
    'jpeg_config': {
        'quality': 90,
        'optimize': True,
        'strip': True
    },
    'png_config': {
        'optimize': True,
        'compress_level': 6,
        'strip': True
    }
}

# æ–‡ä»¶æ ¼å¼é…ç½®
SUPPORTED_ARCHIVE_FORMATS = {'.zip', '.cbz'}
VIDEO_FORMATS = {'.mp4', '.avi', '.mkv', '.wmv', '.flv', '.webm', '.mov', '.m4v', '.mpg', '.mpeg', '.3gp', '.rmvb'}
AUDIO_FORMATS = {'.mp3', '.wav', '.flac', '.m4a', '.ogg', '.aac', '.wma', '.opus', '.ape', '.alac'}
EXCLUDED_IMAGE_FORMATS = {'.jxl', '.avif', '.webp', '.gif', '.psd', '.ai', '.cdr', '.eps', '.svg', '.raw', '.cr2', '.nef', '.arw', '.dng', '.tif', '.tiff'}

# æ•ˆç‡æ£€æŸ¥é…ç½®
EFFICIENCY_CHECK_CONFIG = {
    'min_files_to_check': 3,
    'min_efficiency_threshold': 10,
    'max_inefficient_files': 3
}

# æ·»åŠ cjxlè·¯å¾„åˆ°å…¨å±€é…ç½®
CJXL_PATH = Path(r'D:\1VSCODE\1ehv\exe\jxl\cjxl.exe')
DJXL_PATH = Path(r'D:\1VSCODE\1ehv\exe\jxl\djxl.exe')

# æ›´æ–°å¸ƒå±€é…ç½®
LAYOUT_CONFIG = {
    "status": {
        "ratio": 1,
        "title": "ğŸ­ æ€»ä½“è¿›åº¦",
        "style": "lightblue"
    },
    "progress": {
        "ratio": 1,
        "title": "ğŸ”„ å½“å‰è¿›åº¦",
        "style": "lightgreen"
    },
    "performance": {
        "ratio": 1,
        "title": "âš¡ æ€§èƒ½é…ç½®",
        "style": "lightyellow"
    },
    "image": {
        "ratio": 2,
        "title": "ğŸ–¼ï¸ å›¾ç‰‡è½¬æ¢",
        "style": "lightsalmon"
    },   
    "archive": {
        "ratio": 2,
        "title": "ğŸ“¦ å‹ç¼©åŒ…å¤„ç†",
        "style": "lightpink"
    },
    "file": {
        "ratio": 2,
        "title": "ğŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightcyan"
    },

}

def init_layout():
    TextualLoggerManager.set_layout(LAYOUT_CONFIG, config_info['log_file'])
    # logger.info(f"[#performance]åˆå§‹åŒ–æ€§èƒ½é…ç½®é¢æ¿")
    # logger.info(f"[#file]åˆå§‹åŒ–æ–‡ä»¶æ“ä½œé¢æ¿")
    # logger.info(f"[#archive]åˆå§‹åŒ–å‹ç¼©åŒ…å¤„ç†é¢æ¿")


class FileSystem:
    """æ–‡ä»¶ç³»ç»Ÿæ“ä½œç±»"""

    def __init__(self):
        self.path_handler = PathHandler()
        self.fs = fsspec.filesystem('file')

    def ensure_directory_exists(self, directory):
        """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        try:
            if not self.fs.exists(directory):
                self.fs.makedirs(directory)
                logger.info(f"[#file]åˆ›å»ºç›®å½•: {directory}")
            return True
        except Exception as e:
            logger.info(f"[#file]åˆ›å»ºç›®å½•å¤±è´¥ {directory}: {e}")
            return False

    def safe_delete_file(self, file_path):
        """å®‰å…¨åˆ é™¤æ–‡ä»¶"""
        try:
            if self.fs.exists(file_path):
                self.fs.delete(file_path)
                logger.info(f"[#file]åˆ é™¤æ–‡ä»¶: {file_path}")
                return True
            return False
        except Exception as e:
            logger.info(f"[#file]åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False

    def safe_move_file(self, src_path, dst_path):
        """å®‰å…¨ç§»åŠ¨æ–‡ä»¶"""
        try:
            if not self.fs.exists(src_path):
                logger.info(f"[#file]æºæ–‡ä»¶ä¸å­˜åœ¨: {src_path}")
                return False
            if self.fs.exists(dst_path):
                logger.info(f"[#file]ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨: {dst_path}")
                return False
            self.fs.move(src_path, dst_path)
            logger.info(f"[#file]ç§»åŠ¨æ–‡ä»¶: {src_path} -> {dst_path}")
            return True
        except Exception as e:
            logger.info(f"[#file]ç§»åŠ¨æ–‡ä»¶å¤±è´¥ {src_path} -> {dst_path}: {e}")
            return False

    def safe_copy_file(self, src_path, dst_path):
        """å®‰å…¨å¤åˆ¶æ–‡ä»¶"""
        try:
            if not self.fs.exists(src_path):
                logger.info(f"[#file]æºæ–‡ä»¶ä¸å­˜åœ¨: {src_path}")
                return False
            if self.fs.exists(dst_path):
                logger.info(f"[#file]ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨: {dst_path}")
                return False
            with self.fs.open(src_path, 'rb') as src, self.fs.open(dst_path, 'wb') as dst:
                shutil.copyfileobj(src, dst)
            logger.info(f"[#file]å¤åˆ¶æ–‡ä»¶: {src_path} -> {dst_path}")
            return True
        except Exception as e:
            logger.info(f"[#file]å¤åˆ¶æ–‡ä»¶å¤±è´¥ {src_path} -> {dst_path}: {e}")
            return False

    def get_file_size(self, file_path):
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            return self.fs.info(file_path)['size']
        except Exception as e:
            logger.info(f"[#file]è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
            return 0

    def list_files(self, directory, pattern=None):
        """åˆ—å‡ºç›®å½•ä¸­çš„æ–‡ä»¶"""
        try:
            files = []
            for root, _, filenames in self.fs.walk(directory):
                for filename in filenames:
                    if pattern is None or any(filename.lower().endswith(ext) for ext in pattern):
                        files.append(os.path.join(root, filename))
            return files
        except Exception as e:
            logger.info(f"[#file]åˆ—å‡ºæ–‡ä»¶å¤±è´¥ {directory}: {e}")
            return []

class PathHandler:
    """è·¯å¾„å¤„ç†ç±»"""
    

    @staticmethod
    def ensure_long_path(path):
        """ä¸ºè·¯å¾„æ·»åŠ Windowsé•¿è·¯å¾„å‰ç¼€ï¼Œè¿”å›Pathå¯¹è±¡"""
        try:
            abs_path = Path(path).resolve()
            path_str = str(abs_path)
            if len(path_str) > 260 or any((ord(c) > 127 for c in path_str)):
                if not path_str.startswith('\\\\?\\'):
                    return Path('\\\\?\\' + path_str)
            return abs_path
        except Exception as e:
            logger.info(f"[#file]å¤„ç†é•¿è·¯å¾„æ—¶å‡ºé”™: {e}")
            return Path(path)

    def create_temp_directory(self, file_path):
        """ä¸ºæ¯ä¸ªå‹ç¼©åŒ…åˆ›å»ºå”¯ä¸€çš„ä¸´æ—¶ç›®å½•ï¼Œæ”¯æŒé•¿è·¯å¾„"""
        try:
            fs = fsspec.filesystem('file')
            base_path = Path(file_path).resolve()
            temp_dir = base_path.parent / f'temp_{base_path.stem}_{int(time.time())}'
            safe_temp_dir = PathHandler.ensure_long_path(temp_dir)
            fs.makedirs(str(safe_temp_dir), exist_ok=True)
            logger.info(f"[#file]åˆ›å»ºä¸´æ—¶ç›®å½•: {safe_temp_dir}")  
            return safe_temp_dir
        except Exception as e:
            logger.info(f"[#file]åˆ›å»ºä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
            raise

    def cleanup_temp_files(self, temp_dir, new_zip_path, backup_file_path):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•ï¼Œæ”¯æŒé•¿è·¯å¾„"""
        try:
            fs = fsspec.filesystem('file')
            if temp_dir:
                safe_temp = PathHandler.ensure_long_path(temp_dir)
                if fs.exists(str(safe_temp)):
                    fs.delete(str(safe_temp), recursive=True)
                    logger.info(f"[#file]å·²åˆ é™¤ä¸´æ—¶ç›®å½•: {safe_temp}")  
            if new_zip_path:
                safe_new = PathHandler.ensure_long_path(new_zip_path)
                if fs.exists(str(safe_new)):
                    fs.delete(str(safe_new))
                    logger.info(f"[#file]å·²åˆ é™¤ä¸´æ—¶å‹ç¼©åŒ…: {safe_new}")  
            if backup_file_path:
                safe_backup = PathHandler.ensure_long_path(backup_file_path)
                if fs.exists(str(safe_backup)):
                    fs.delete(str(safe_backup))
                    logger.info(f"[#file]å·²åˆ é™¤å¤‡ä»½æ–‡ä»¶: {safe_backup}")  
        except Exception as e:
            logger.info(f"[#file]æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def wrapper(self, path, *args, **kwargs):
        """è·¯å¾„å¤„ç†åŒ…è£…å™¨"""
        try:
            long_path = self.ensure_long_path(path)
            return self.func(long_path, *args, **kwargs)
        except OSError as e:
            if e.winerror == 3:
                logger.info(f"[#file]è·¯å¾„è¶…é•¿æˆ–æ— æ•ˆ: {path}")
            else:
                raise

class DirectoryHandler:
    """ç›®å½•å¤„ç†ç±»"""

    def flatten_single_subfolder(self, directory, exclude_keywords):
        """å¦‚æœç›®å½•ä¸­åªæœ‰ä¸€ä¸ªå­æ–‡ä»¶å¤¹ï¼Œå°†å…¶å†…å®¹ç§»åŠ¨åˆ°çˆ¶ç›®å½•"""
        try:
            contents = os.listdir(directory)
            if len(contents) != 1:
                return
            subdir = os.path.join(directory, contents[0])
            if not os.path.isdir(subdir):
                return
            if any((keyword in os.path.basename(subdir).lower() for keyword in exclude_keywords)):
                return
            for item in os.listdir(subdir):
                src = os.path.join(subdir, item)
                dst = os.path.join(directory, item)
                shutil.move(src, dst)
            os.rmdir(subdir)
            logger.info(f"[#file]å·²å±•å¹³å­æ–‡ä»¶å¤¹: {subdir}")
        except Exception as e:
            logger.info(f"[#file]å±•å¹³å­æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")

    def remove_empty_directories(self, directory):
        """åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ç©ºæ–‡ä»¶å¤¹"""
        removed_count = 0
        for root, dirs, _ in os.walk(directory, topdown=False):
            for dir_name in dirs:
                dir_path = os.path.join(root, dir_name)
                try:
                    if not os.listdir(dir_path):
                        subprocess.run(['cmd', '/c', 'rd', '/s', '/q', dir_path], check=True)
                        removed_count += 1
                        logger.info(f"[#file]å·²åˆ é™¤ç©ºæ–‡ä»¶å¤¹: {dir_path}")
                except Exception as e:
                    logger.info(f"[#file]åˆ é™¤ç©ºæ–‡ä»¶å¤¹å¤±è´¥ {dir_path}: {e}")
        return removed_count



class Converter:
    """å›¾ç‰‡è½¬æ¢ç±»"""

    def __init__(self):
        self.path_handler = PathHandler()

    def convert_with_cjxl(self, input_path, output_path, is_jpeg=False):
        """ä½¿ç”¨cjxlè¿›è¡Œè½¬æ¢
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ 
            is_jpeg: æ˜¯å¦æ˜¯JPEGæ–‡ä»¶
        """
        try:
            if not CJXL_PATH.exists():
                logger.info(f"[#file]cjxl.exeä¸å­˜åœ¨: {CJXL_PATH}")
                return False
                
            # æ„å»ºå‘½ä»¤
            if is_jpeg:
                # JPEGæ— æŸæ¨¡å¼
                cmd = [
                    str(CJXL_PATH),
                    '-e', '7',  # effort level
                    '--lossless_jpeg=1',  # å¯ç”¨JPEGæ— æŸ
                    str(input_path),
                    str(output_path)
                ]
            else:
                # æ™®é€šæ— æŸæ¨¡å¼
                cmd = [
                    str(CJXL_PATH), 
                    '-e', '7',
                    '-d', '0',  # æ— æŸæ¨¡å¼
                    str(input_path),
                    str(output_path)
                ]
            
            # æ‰§è¡Œè½¬æ¢
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"[#file]cjxlè½¬æ¢æˆåŠŸ: {input_path}")
                return True
            else:
                logger.info(f"[#file]cjxlè½¬æ¢å¤±è´¥: {input_path}\né”™è¯¯: {result.stderr}")
                return False
                
        except Exception as e:
            logger.info(f"[#file]cjxlè½¬æ¢å‡ºé”™: {e}")
            return False

    def process_single_image(self, file_path, params):
        """å¤„ç†å•ä¸ªå›¾ç‰‡æ–‡ä»¶"""
        try:
            fs = fsspec.filesystem('file')
            target_ext = IMAGE_CONVERSION_CONFIG['target_format'].lower()
            
            if file_path.lower().endswith(target_ext):
                return False
                
            base_path = os.path.splitext(file_path)[0]
            new_file_path = base_path + target_ext
            counter = 1
            while fs.exists(new_file_path):
                new_file_path = f'{base_path}_{counter}{target_ext}'
                counter += 1
                
            original_size = fs.info(file_path)['size'] / 1024
            
            # åªåœ¨JXLæ— æŸæ¨¡å¼ä¸‹ä½¿ç”¨cjxl
            if target_ext == '.jxl' and params.get('use_cjxl', False):
                is_jpeg = file_path.lower().endswith(('.jpg', '.jpeg'))
                logger.info(f"[#image]âœ… ä½¿ç”¨cjxlè½¬æ¢: {file_path}")
                if not self.convert_with_cjxl(file_path, new_file_path, is_jpeg):
                    return False
            else:
                # å…¶ä»–æƒ…å†µä½¿ç”¨åŸæœ‰è½¬æ¢æ–¹å¼
                with fs.open(file_path, 'rb') as f:
                    image = pyvips.Image.new_from_buffer(f.read(), '')
                format_config = IMAGE_CONVERSION_CONFIG[f'{target_ext[1:]}_config']
                params = {
                    'Q': format_config['quality'],
                    'strip': format_config.get('strip', True),
                    'lossless': format_config.get('lossless', False)
                }
                safe_new_path = self.path_handler.ensure_long_path(new_file_path)
                image.write_to_file(str(safe_new_path), **params)
                # logger.info(f"[#image]âœ… ä½¿ç”¨libvipè½¬æ¢: {file_path}")
            if fs.exists(new_file_path):
                new_size = fs.info(new_file_path)['size'] / 1024
                size_reduction = original_size - new_size
                compression_ratio = size_reduction / original_size * 100
                # status_message = f'{os.path.basename(file_path)}: {original_size:.0f}KB -> {new_size:.0f}KB (-{size_reduction:.0f}KB, -{compression_ratio:.1f}%)'
                # logger.info(f"[#file]{status_message}")
                try:
                    image = None
                    fs.delete(file_path)
                    return (True, original_size, new_size)
                except Exception as e:
                    error_msg = f'åˆ é™¤åŸæ–‡ä»¶å¤±è´¥ {file_path}: {e}'
                    logger.info(f"[#image]{error_msg}")
                    return False
            return False
        except Exception as e:
            error_msg = f'å¤„ç†å›¾ç‰‡å¤±è´¥ {file_path}: {e}'
            logger.info(f"[#image]{error_msg}")
            return False

    def process_image_in_memory(self, image_data, min_size=640, min_width=0):
        """å¤„ç†å•ä¸ªå›¾ç‰‡æ•°æ®"""
        try:
            with BytesIO(image_data) as bio:
                with Image.open(bio) as img:
                    original_format = img.format.lower()
                    original_size = len(image_data) / 1024
                    original_dimensions = f'{img.width}x{img.height}'
                    width_info = ''
                    if min_width > 0:
                        width_info = f', æœ€å°å®½åº¦è¦æ±‚={min_width}px'
                    logger.info(f"[#image]å¤„ç†å›¾ç‰‡: æ ¼å¼={original_format}, å°ºå¯¸={original_dimensions}{width_info}, å¤§å°={original_size:.2f}KB")
                    if min_width > 0:
                        if img.width < min_width:
                            logger.info(f"[#image][å®½åº¦è¿‡å°] å›¾ç‰‡å®½åº¦ {img.width}px å°äºæŒ‡å®šçš„æœ€å°å®½åº¦ {min_width}pxï¼Œè·³è¿‡å¤„ç†")
                            return (image_data, 'width_too_small')
                        else:
                            logger.info(f"[#image][å®½åº¦ç¬¦åˆ] å›¾ç‰‡å®½åº¦ {img.width}px å¤§äºæŒ‡å®šçš„æœ€å°å®½åº¦ {min_width}pxï¼Œç»§ç»­å¤„ç†")
                    cur_format = img.format.lower()
                    target_format = IMAGE_CONVERSION_CONFIG['target_format'][1:].lower()
                    if cur_format == target_format:
                        logger.info(f"[#image]å›¾ç‰‡å·²ç»æ˜¯ç›®æ ‡æ ¼å¼ {target_format}ï¼Œè·³è¿‡è½¬æ¢")
                        return (image_data, None)
            image = pyvips.Image.new_from_buffer(image_data, '')
            config = IMAGE_CONVERSION_CONFIG[f'{target_format}_config']
            logger.info(f"[#image]è½¬æ¢é…ç½®: ç›®æ ‡æ ¼å¼={target_format}, å‚æ•°={config}")
            if target_format == 'avif':
                params = {'Q': config['quality'], 'speed': config.get('speed', 7), 'strip': config.get('strip', True), 'lossless': config.get('lossless', False)}
            elif target_format == 'webp':
                params = {'Q': config['quality'], 'strip': config.get('strip', True), 'lossless': config.get('lossless', False), 'reduction_effort': config.get('method', 4)}
            elif target_format == 'jxl':
                params = {'Q': config['quality'], 'strip': config.get('strip', True), 'lossless': config.get('lossless', False), 'effort': config.get('effort', 7), 'modular': config.get('modular', False), 'jpeg_recompression': config.get('jpeg_recompression', False), 'jpeg_lossless': config.get('jpeg_lossless', False)}
            elif target_format == 'jpg' or target_format == 'jpeg':
                params = {'Q': config['quality'], 'strip': config.get('strip', True), 'optimize_coding': config.get('optimize', True)}
            else:
                params = {'strip': config.get('strip', True), 'compression': config.get('compress_level', 6)}
            output_buffer = image.write_to_buffer(f'.{target_format}', **params)
            converted_size = len(output_buffer) / 1024
            size_change = original_size - converted_size
            logger.info(f"[#image]è½¬æ¢å®Œæˆ: æ–°å¤§å°={converted_size:.2f}KB, å‡å°‘={size_change:.2f}KB ({size_change / original_size * 100:.1f}%)")
            return (output_buffer, None)
        except Exception as e:
            logger.info(f"[#image]å›¾ç‰‡è½¬æ¢é”™è¯¯: {str(e)}")
            return (None, 'processing_error')

    def has_processed_comment(self, zip_path, comment='Processed'):
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                return zip_ref.comment.decode('utf-8') == comment
        except Exception as e:
            logger.info(f"[#file]Error checking comment in {zip_path}: {e}")
            return False

    def add_processed_comment(self, zip_path, comment='Processed'):
        try:
            with zipfile.ZipFile(zip_path, 'a') as zip_ref:
                zip_ref.comment = comment.encode('utf-8')
            logger.info(f"[#archive]Added comment '{comment}' to {zip_path}")
        except Exception as e:
            logger.info(f"[#archive]Error adding comment to {zip_path}: {e}")

class BatchProcessor:
    """æ‰¹é‡å¤„ç†ç±»"""
    def __init__(self):
        self.converter = Converter()
        self.efficiency_tracker = EfficiencyTracker()

    def _collect_image_files(self, temp_dir):
        """æ”¶é›†ç›®å½•ä¸­çš„å›¾ç‰‡æ–‡ä»¶"""
        image_files = []
        for root, _, files in os.walk(temp_dir):
            for file in files:
                if any((file.lower().endswith(ext) for ext in IMAGE_CONVERSION_CONFIG['source_formats'])):
                    file_path = os.path.join(root, file)
                    image_files.append(file_path)
        image_files.sort()
        return image_files

    def _write_log_header(self, log_file, initial_count, archive_path):
        """å†™å…¥æ—¥å¿—æ–‡ä»¶å¤´éƒ¨"""
        log_file.write('# å›¾ç‰‡è½¬æ¢æ—¥å¿—\n\n')
        log_file.write(f'## åŸºæœ¬ä¿¡æ¯\n\n')
        log_file.write(f"- **è½¬æ¢æ—¶é—´**: `{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}`\n")
        if archive_path:
            log_file.write(f'- **å‹ç¼©åŒ…è·¯å¾„**: `{archive_path}`\n')
            log_file.write(f'- **å‹ç¼©åŒ…åç§°**: `{os.path.basename(archive_path)}`\n')
        log_file.write(f'- **å›¾ç‰‡æ€»æ•°**: `{initial_count}`\n\n')
        
        # åŒæ—¶æ›´æ–°åˆ°é¢æ¿
        logger.info(f"[#archive]ğŸ“ å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {os.path.basename(archive_path) if archive_path else 'æœªçŸ¥'}")
    def _write_conversion_params(self, log_file):
        """å†™å…¥è½¬æ¢å‚æ•°"""
        log_file.write('## è½¬æ¢å‚æ•°\n\n')
        log_file.write(f"- **ç›®æ ‡æ ¼å¼**: `{IMAGE_CONVERSION_CONFIG['target_format']}`\n")
        format_config = IMAGE_CONVERSION_CONFIG[f"{IMAGE_CONVERSION_CONFIG['target_format'][1:]}_config"]
        log_file.write(f"- **è´¨é‡è®¾ç½®**: `{format_config['quality']}`\n")
        
        # åŒæ—¶æ›´æ–°åˆ°é¢æ¿
        params_text = f"ğŸ¯ ç›®æ ‡æ ¼å¼: {IMAGE_CONVERSION_CONFIG['target_format']}, è´¨é‡: {format_config['quality']}"
        
        if IMAGE_CONVERSION_CONFIG['target_format'] == '.jxl':
            log_file.write(f"- **ç¼–ç æ•ˆæœ(effort)**: `{format_config.get('effort', 7)}`\n")
            log_file.write(f"- **æ— æŸæ¨¡å¼**: `{format_config.get('lossless', False)}`\n")
            log_file.write(f"- **JPEGé‡å‹ç¼©**: `{format_config.get('jpeg_recompression', False)}`\n")
            log_file.write(f"- **JPEGæ— æŸ**: `{format_config.get('jpeg_lossless', False)}`\n")
            params_text += f", effort: {format_config.get('effort', 7)}"
            if format_config.get('lossless', False):
                params_text += ", æ— æŸæ¨¡å¼"
        elif IMAGE_CONVERSION_CONFIG['target_format'] == '.avif':
            log_file.write(f"- **é€Ÿåº¦è®¾ç½®**: `{format_config.get('speed', 7)}`\n")
            log_file.write(f"- **è‰²åº¦è´¨é‡**: `{format_config.get('chroma_quality', 100)}`\n")
            params_text += f", é€Ÿåº¦: {format_config.get('speed', 7)}"
        elif IMAGE_CONVERSION_CONFIG['target_format'] == '.webp':
            log_file.write(f"- **å‹ç¼©æ–¹æ³•**: `{format_config.get('method', 4)}`\n")
            params_text += f", æ–¹æ³•: {format_config.get('method', 4)}"
            
        logger.info(f"[#image]{params_text}")

    def _write_log_summary(self, log_file, processed_files, total_time, total_original_size, total_converted_size):
        """å†™å…¥æ—¥å¿—æ€»ç»“"""
        if not processed_files:
            return
            
        avg_time = total_time / len(processed_files) if processed_files else 0
        total_compression_ratio = ((total_original_size - total_converted_size) / total_original_size * 100) if total_original_size > 0 else 0
        
        # å†™å…¥æ–‡ä»¶
        log_file.write(f"\n## æ€»ç»“\n\n")
        log_file.write(f"- **æ€»å¤„ç†æ–‡ä»¶æ•°**: `{len(processed_files)}`\n")
        log_file.write(f"- **æ€»å¤„ç†è€—æ—¶**: `{total_time:.1f}ç§’`\n")
        log_file.write(f"- **å¹³å‡å•å¼ è€—æ—¶**: `{avg_time:.1f}ç§’`\n")
        log_file.write(f"- **æ€»åŸå§‹å¤§å°**: `{total_original_size/1024:.1f}MB`\n")
        log_file.write(f"- **æ€»è½¬æ¢åå¤§å°**: `{total_converted_size/1024:.1f}MB`\n")
        log_file.write(f"- **æ€»ä½“å‹ç¼©ç‡**: `{total_compression_ratio:.1f}%`\n")
        log_file.write(f"- **å¤„ç†å®Œæˆæ—¶é—´**: `{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}`\n")
        
        # æ›´æ–°åˆ°é¢æ¿
        summary_text = (
            f"âœ¨ å¤„ç†å®Œæˆ ğŸ“Š æ€»æ–‡ä»¶æ•°: {len(processed_files)} â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’ (å¹³å‡ {avg_time:.1f}ç§’/å¼ ) ğŸ“¦ æ€»å¤§å°: {total_original_size/1024:.1f}MB -> {total_converted_size/1024:.1f}MB ğŸ“ˆ å‹ç¼©ç‡: {total_compression_ratio:.1f}"
        )
        logger.info(f"[#archive]{summary_text}")

    def _process_image_batch(self, batch, params, processed_files, log_file_path, temp_dir, total_status):
        """å¤„ç†ä¸€æ‰¹å›¾ç‰‡æ–‡ä»¶"""
        futures = []
        current_threads = get_thread_count()
        batch_size = get_batch_size()
        logger.info(f"[#performance]å½“å‰çº¿ç¨‹æ•°: {current_threads}, å½“å‰æ‰¹å¤„ç†å¤§å°: {batch_size}")
     
        with ThreadPoolExecutor(max_workers=current_threads) as executor:
            for file_path in batch:
                future = executor.submit(self.converter.process_single_image, file_path, params)
                futures.append((future, file_path))
                
            for future, file_path in futures:
                try:
                    result = future.result()
                    if isinstance(result, tuple) and result[0]:
                        processed_files.add(file_path)
                        original_size, new_size = result[1], result[2]
                        total_status['original_size'] += original_size
                        total_status['converted_size'] += new_size
                        size_reduction = original_size - new_size
                        compression_ratio = size_reduction / original_size * 100
                        
                        message = f"{os.path.relpath(file_path, temp_dir)} ({original_size:.0f}KB -> {new_size:.0f}KB, å‡å°‘{size_reduction:.0f}KB, å‹ç¼©ç‡{compression_ratio:.1f})"
                        logger.info(f"[#image]âœ… {message}")
                        archive_status=len(processed_files)/total_status['initial_count']*100
                        archive_ratio= str(len(processed_files))+'/'+str(total_status['initial_count'])
                        logger.info(f"[@progress] å½“å‰è¿›åº¦: {archive_ratio} {archive_status:.1f}%")
                        
                        with open(log_file_path, 'a', encoding='utf-8') as f:
                            f.write(f"| `{os.path.relpath(file_path, temp_dir)}` | {original_size:.0f}KB | {new_size:.0f}KB | {size_reduction:.0f}KB | {compression_ratio:.1f}% |\n")
                except Exception as e:
                    logger.info(f"[#file]âŒ å¤„ç†å›¾ç‰‡å¤±è´¥ {os.path.relpath(file_path, temp_dir)}: {e}")
                    with open(log_file_path, 'a', encoding='utf-8') as f:
                        f.write(f"\n> âš ï¸ å¤„ç†å¤±è´¥: `{os.path.relpath(file_path, temp_dir)}` - {str(e)}\n")

    def process_images_in_directory(self, temp_dir, params, archive_path=None):
        """å¤„ç†ç›®å½•ä¸­çš„å›¾ç‰‡"""
        try:
            start_time = time.time()
            total_status = {
                'original_size': 0,
                'converted_size': 0
            }
            
            # æ”¶é›†å›¾ç‰‡æ–‡ä»¶
            image_files = self._collect_image_files(temp_dir)
            total_status['initial_count'] = len(image_files)
            
            if not image_files:
                logger.info(f"[#file]æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶åœ¨ç›®å½•: {temp_dir}")
                return set()
                
            # åˆ›å»ºå¹¶åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
            log_file_path = os.path.join(temp_dir, 'conversion.md')
            with open(log_file_path, 'w', encoding='utf-8') as f:
                self._write_log_header(f, total_status['initial_count'], archive_path)
                self._write_conversion_params(f)
                f.write('\n## è½¬æ¢è¯¦æƒ…\n\n')
                f.write('| æ–‡ä»¶å | åŸå§‹å¤§å° | è½¬æ¢åå¤§å° | å‡å°‘å¤§å° | å‹ç¼©ç‡ |\n')
                f.write('|--------|----------|------------|----------|--------|\n')
            
            # å¤„ç†å›¾ç‰‡æ–‡ä»¶
            processed_files = set()
            batch_size = get_batch_size()
            for i in range(0, len(image_files), batch_size):
                batch = image_files[i:i + batch_size]
                self._process_image_batch(batch, params, processed_files, 
                                       log_file_path, temp_dir, total_status)
            
            # å†™å…¥æ€»ç»“
            with open(log_file_path, 'a', encoding='utf-8') as f:
                self._write_log_summary(f, processed_files, time.time() - start_time,
                                      total_status['original_size'], total_status['converted_size'])
            
            return processed_files
            
        except Exception as e:
            logger.info(f"[#file]å¤„ç†ç›®å½•çš„å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return set()

class ArchiveHandler:
    """å¤„ç†å‹ç¼©åŒ…çš„ç±»"""
    def __init__(self):
        self.path_handler = PathHandler()

    def _validate_archive(self, file_path: Path, params: dict) -> tuple[bool, int]:
        """éªŒè¯å‹ç¼©åŒ…æ˜¯å¦éœ€è¦å¤„ç†"""
        if not file_path.exists():
            logger.info(f"[#file]æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False, 0
            
        if not self.should_process_file(file_path, params):
            logger.info(f"[#archive]æ ¹æ®è¿‡æ»¤æ¡ä»¶è·³è¿‡æ–‡ä»¶: {file_path}")
            logger.info(f"[#archive]è·³è¿‡: {file_path.name} - ä¸ç¬¦åˆå…³é”®è¯è¦æ±‚")
            return False, 0
            
        logger.info(f"[#archive]ğŸ”„ æ­£åœ¨å¤„ç†: {file_path.name}")
            
        needs_processing, image_count = self.check_archive_contents(str(file_path), params.get('min_width', 0))
        
        if needs_processing is None:
            logger.info(f"[#archive]æ–‡ä»¶è¢«å ç”¨ï¼Œå°†æ·»åŠ åˆ°é‡è¯•é˜Ÿåˆ—: {file_path}")
            return False, 0
        elif needs_processing is False:
            logger.info(f"[#archive]å‹ç¼©åŒ… {file_path} æ— éœ€å¤„ç†")
            return False, 0
        elif image_count == 0:
            logger.info(f"[#archive]å‹ç¼©åŒ… {file_path} ä¸åŒ…å«å›¾ç‰‡æ–‡ä»¶")
            return False, 0
            
        return True, image_count

    def _prepare_paths(self, file_path: Path) -> tuple[Path, Path, Path]:
        """å‡†å¤‡å¤„ç†æ‰€éœ€çš„ä¸´æ—¶è·¯å¾„"""
        temp_dir = self.path_handler.create_temp_directory(file_path)
        new_zip_path = file_path.parent / f'{file_path.name}.{int(time.time())}.new'
        backup_file_path = file_path.parent / f'{file_path.name}.{int(time.time())}.bak'
        
        # åˆ›å»ºå¤‡ä»½
        shutil.copy2(file_path, backup_file_path)
        logger.info(f"[#file]åˆ›å»ºå¤‡ä»½: {backup_file_path}")
        
        return temp_dir, new_zip_path, backup_file_path

    def _process_archive_contents(self, file_path: Path, temp_dir: Path, params: dict, 
                                image_count: int) -> tuple[set, dict]:
        """å¤„ç†å‹ç¼©åŒ…å†…å®¹"""
        if not self.extract_archive(file_path, temp_dir):
            logger.info(f"[#file]è§£å‹å¤±è´¥: {file_path}")
            return set(), {}
            
        logger.info(f"[#image]æ­£åœ¨å¤„ç†å›¾ç‰‡: {file_path.name}")
        processed_files = BatchProcessor().process_images_in_directory(
            temp_dir, params, archive_path=file_path
        )
            
        return processed_files, {}

    def _finalize_archive(self, file_path: Path, temp_dir: Path, new_zip_path: Path,
                         backup_file_path: Path, processed_files: set, skipped_files: dict,
                         image_count: int) -> list:
        """å®Œæˆå‹ç¼©åŒ…å¤„ç†"""
        processed_archives = []
        
        if any((reason == 'è¿ç»­ä½æ•ˆç‡è½¬æ¢' for reason in skipped_files.values())):
            logger.info(f"[#archive]å‹ç¼©åŒ… {file_path} å› è¿ç»­ä½æ•ˆç‡è½¬æ¢è¢«è·³è¿‡")
            logger.info(f"[#archive]è·³è¿‡: {file_path.name} - è¿ç»­ä½æ•ˆç‡è½¬æ¢")
            return []
            
        if not processed_files:
            logger.info(f"[#archive]æ²¡æœ‰éœ€è¦å¤„ç†çš„å›¾ç‰‡: {file_path}")
            return []
            
        logger.info(f"[#archive]æ­£åœ¨åˆ›å»ºæ–°å‹ç¼©åŒ…: {file_path.name}")
            
        if not ArchiveContent().cleanup_and_compress(temp_dir, processed_files, skipped_files, new_zip_path):
            logger.info(f"[#archive]æ¸…ç†å’Œå‹ç¼©å¤±è´¥: {file_path}")
            logger.info(f"[#archive]é”™è¯¯: {file_path.name} - æ¸…ç†å’Œå‹ç¼©å¤±è´¥")
            return []
            
        success, size_change = self.handle_size_comparison(file_path, new_zip_path, backup_file_path)
        if success:
            result = {
                'file_path': str(file_path),
                'processed_images': len(processed_files),
                'skipped_images': len(skipped_files),
                'size_reduction_mb': size_change
            }
            processed_archives.append(result)
            logger.info(f"[#archive]å®Œæˆ: {file_path.name} - å‡å°‘äº† {size_change:.2f}MB")
        else:
            logger.info(f"[#archive]è·³è¿‡: {file_path.name} - æ–°æ–‡ä»¶å¤§å°æœªå‡å°")

                
        return processed_archives

    def process_single_archive(self, file_path, params):
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…æ–‡ä»¶"""
        try:
            file_path = Path(file_path)
            logger.info(f"[#file]å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            
            # éªŒè¯å‹ç¼©åŒ…
            is_valid, image_count = self._validate_archive(file_path, params)
            if not is_valid:
                return []
                
            # å‡†å¤‡è·¯å¾„
            temp_dir = None
            new_zip_path = None
            backup_file_path = None
            try:
                temp_dir, new_zip_path, backup_file_path = self._prepare_paths(file_path)
                
                # è®¾ç½®rename_cbrå±æ€§
                self.rename_cbr = params.get('rename_cbr', False)
                
                # å¤„ç†å†…å®¹
                processed_files, skipped_files = self._process_archive_contents(
                    file_path, temp_dir, params, image_count
                )
                
                # å®Œæˆå¤„ç†
                return self._finalize_archive(
                    file_path, temp_dir, new_zip_path, backup_file_path,
                    processed_files, skipped_files, image_count
                )
            finally:
                self.path_handler.cleanup_temp_files(temp_dir, new_zip_path, backup_file_path)
                
        except Exception as e:
            logger.info(f"[#archive]å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™ {file_path}: {e}")
            logger.info(f"[#archive]é”™è¯¯: {file_path.name} - {str(e)}")
            return []

    def prepare_archive(self, file_path):
        """å‡†å¤‡å‹ç¼©åŒ…å¤„ç†ç¯å¢ƒ"""
        temp_dir = PathHandler.create_temp_directory(file_path)
        backup_file_path = file_path + '.bak'
        new_zip_path = file_path + '.new'
        try:
            shutil.copy(file_path, backup_file_path)
            logger.info(f"[#file]åˆ›å»ºå¤‡ä»½: {backup_file_path}")
            try:
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(temp_dir)
                    logger.info(f"[#file]æˆåŠŸè§£å‹æ–‡ä»¶åˆ°: {temp_dir}")
                    return (temp_dir, backup_file_path, new_zip_path, file_path)
            except zipfile.BadZipFile:
                logger.info(f"[#file]æ— æ•ˆçš„å‹ç¼©åŒ…æ ¼å¼: {file_path}")
                PathHandler.cleanup_temp_files(temp_dir, new_zip_path, backup_file_path)
                return (None, None, None, None)
        except Exception as e:
            logger.info(f"[#file]å‡†å¤‡ç¯å¢ƒå¤±è´¥ {file_path}: {e}")
            PathHandler.cleanup_temp_files(temp_dir, new_zip_path, backup_file_path)
            return (None, None, None, None)

    def run_7z_command(self, command, zip_path, operation='', additional_args=None):
        """
        æ‰§è¡Œ7zå‘½ä»¤çš„é€šç”¨å‡½æ•°
        
        Args:
            command: ä¸»å‘½ä»¤ (å¦‚ 'a', 'x', 'l' ç­‰)
            zip_path: å‹ç¼©åŒ…è·¯å¾„
            operation: æ“ä½œæè¿°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            additional_args: é¢å¤–çš„å‘½ä»¤è¡Œå‚æ•°
        """
        try:
            cmd = ['7z', command, zip_path]
            if additional_args:
                cmd.extend(additional_args)
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"[#file]æˆåŠŸæ‰§è¡Œ7z {operation}: {zip_path}")
                return (True, result.stdout)
            else:
                logger.info(f"[#file]7z {operation}å¤±è´¥: {zip_path}\né”™è¯¯: {result.stderr}")
                return (False, result.stderr)
        except Exception as e:
            logger.info(f"[#file]æ‰§è¡Œ7zå‘½ä»¤å‡ºé”™: {e}")
            return (False, str(e))

    def create_new_archive(self, temp_dir, new_zip_path):
        """åˆ›å»ºæ–°çš„å‹ç¼©åŒ…ï¼Œæ”¯æŒé•¿è·¯å¾„"""
        try:
            safe_temp = PathHandler.ensure_long_path(temp_dir)
            safe_zip = PathHandler.ensure_long_path(new_zip_path)
            cmd = ['7z', 'a', '-tzip', str(safe_zip), os.path.join(str(safe_temp), '*')]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.info(f"[#file]åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥: {safe_zip}\né”™è¯¯: {result.stderr}")
                return False
            fs = fsspec.filesystem('file')
            if not fs.exists(str(safe_zip)):
                logger.info(f"[#file]å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {safe_zip}")
                return False
            logger.info(f"[#file]æˆåŠŸåˆ›å»ºæ–°å‹ç¼©åŒ…: {safe_zip}")
            return True
        except Exception as e:
            logger.info(f"[#file]åˆ›å»ºå‹ç¼©åŒ…æ—¶å‡ºé”™: {e}")
            return False

    def check_archive_contents(self, file_path, min_width=0):
        """
        ä½¿ç”¨zipfileæ£€æŸ¥å‹ç¼©åŒ…å†…å®¹
        
        Returns:
            (needs_processing, image_count): (æ˜¯å¦éœ€è¦å¤„ç†, å›¾ç‰‡æ–‡ä»¶æ•°)
            å¦‚æœè¿”å› (False, 0)ï¼Œè¡¨ç¤ºå‹ç¼©åŒ…ä¸éœ€è¦å¤„ç†ï¼ˆå¯èƒ½åŒ…å«è§†é¢‘/éŸ³é¢‘/æ’é™¤æ ¼å¼æˆ–ä¸ºç©ºï¼‰
            å¦‚æœè¿”å› (None, 0)ï¼Œè¡¨ç¤ºæ–‡ä»¶è¢«å ç”¨
        """
        try:
            try:
                with open(file_path, 'rb') as f:
                    f.read(1)
            except (IOError, PermissionError):
                logger.info(f"[#file]æ–‡ä»¶æ­£åœ¨è¢«å ç”¨ï¼Œç¨åé‡è¯•: {file_path}")
                return (None, 0)
            with zipfile.ZipFile(file_path, 'r') as zip_ref:
                file_list = zip_ref.namelist()
                if not file_list:
                    logger.info(f"[#file]å‹ç¼©åŒ…ä¸ºç©º: {file_path}")
                    return (False, 0)
                target_ext = IMAGE_CONVERSION_CONFIG['target_format'].lower()
                image_count = 0
                needs_processing = False
                has_video = False
                has_audio = False
                has_excluded_format = False
                temp_dir = None
                if min_width > 0:
                    temp_dir = tempfile.mkdtemp()
                try:
                    for file_name in file_list:
                        if file_name.endswith('/'):
                            continue
                        file_ext = os.path.splitext(file_name.lower())[1]
                        if file_ext in VIDEO_FORMATS:
                            has_video = True
                            logger.info(f"[#file]å‘ç°è§†é¢‘æ–‡ä»¶: {file_name}")
                            return (False, 0)
                        elif file_ext in AUDIO_FORMATS:
                            has_audio = True
                            logger.info(f"[#file]å‘ç°éŸ³é¢‘æ–‡ä»¶: {file_name}")
                            return (False, 0)
                        elif file_ext in EXCLUDED_IMAGE_FORMATS:
                            has_excluded_format = True
                            logger.info(f"[#file]å‘ç°æ’é™¤æ ¼å¼å›¾ç‰‡: {file_name}")
                            return (False, 0)
                        if any((file_ext == ext for ext in IMAGE_CONVERSION_CONFIG['source_formats'])):
                            image_count += 1
                            if file_ext != target_ext:
                                needs_processing = True
                                if min_width > 0 and temp_dir:
                                    try:
                                        zip_ref.extract(file_name, temp_dir)
                                        img_path = os.path.join(temp_dir, file_name)
                                        with Image.open(img_path) as img:
                                            if img.width < min_width:
                                                logger.info(f"[#image]å‘ç°å®½åº¦ä¸è¶³çš„å›¾ç‰‡: {file_name} (å®½åº¦: {img.width}px < {min_width}px)")
                                                return (False, 0)
                                    except Exception as e:
                                        logger.info(f"[#file]æ£€æŸ¥å›¾ç‰‡å®½åº¦æ—¶å‡ºé”™ {file_name}: {e}")
                                        continue
                                    finally:
                                        try:
                                            if os.path.exists(img_path):
                                                os.remove(img_path)
                                        except:
                                            pass
                finally:
                    if temp_dir and os.path.exists(temp_dir):
                        try:
                            shutil.rmtree(temp_dir)
                        except:
                            pass
                if needs_processing:
                    logger.info(f"[#file]å‹ç¼©åŒ… {file_path} åŒ…å« {image_count} ä¸ªå›¾ç‰‡æ–‡ä»¶ï¼Œéœ€è¦å¤„ç†")
                return (needs_processing, image_count)
        except zipfile.BadZipFile:
            logger.info(f"[#file]æ— æ•ˆçš„å‹ç¼©åŒ…æ ¼å¼: {file_path}")
            return (False, 0)
        except Exception as e:
            logger.info(f"[#file]æ£€æŸ¥å‹ç¼©åŒ…å†…å®¹æ—¶å‡ºé”™ {file_path}: {e}")
            if 'å¦ä¸€ä¸ªç¨‹åºæ­£åœ¨ä½¿ç”¨æ­¤æ–‡ä»¶' in str(e) or 'being used by another process' in str(e):
                return (None, 0)
            return (False, 0)

    def should_process_file(self, file_path, params):
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦éœ€è¦å¤„ç†"""
        if params.get('exclude_paths'):
            is_excluded = any((exclude_path in str(file_path) for exclude_path in params['exclude_paths']))
            if is_excluded:
                logger.info(f"[#file]æ–‡ä»¶åœ¨æ’é™¤è·¯å¾„ä¸­ï¼Œè·³è¿‡: {file_path}")
                return False
        if params.get('keywords'):
            file_name = os.path.basename(str(file_path)).lower()
            if params['keywords'] == 'internal':
                has_keyword = any((keyword.lower() in file_name for keyword in INCLUDED_KEYWORDS))
                if not has_keyword:
                    logger.info(f"[#file]æ–‡ä»¶åä¸åŒ…å«å†…ç½®å…³é”®è¯ï¼Œè·³è¿‡: {file_path}")
                    return False
                logger.info(f"[#file]æ–‡ä»¶ååŒ…å«å†…ç½®å…³é”®è¯ï¼Œç»§ç»­å¤„ç†: {file_path}")
            elif isinstance(params['keywords'], list):
                has_keyword = any((keyword.lower() in file_name for keyword in params['keywords']))
                if not has_keyword:
                    logger.info(f"[#file]æ–‡ä»¶åä¸åŒ…å«æŒ‡å®šå…³é”®è¯ï¼Œè·³è¿‡: {file_path}")
                    return False
                logger.info(f"[#file]æ–‡ä»¶ååŒ…å«æŒ‡å®šå…³é”®è¯ï¼Œç»§ç»­å¤„ç†: {file_path}")
        is_art = self.is_artbook(str(file_path), params['artbook_keywords'])
        if params['handle_artbooks']:
            return is_art
        else:
            return not is_art

    def extract_archive(self, file_path, temp_dir):
        """
        è§£å‹æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§è§£å‹æ–¹æ¡ˆ
        
        Args:
            file_path: å‹ç¼©åŒ…è·¯å¾„
            temp_dir: è§£å‹ç›®æ ‡ç›®å½•
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè§£å‹
        """
        try:
            safe_src = PathHandler.ensure_long_path(file_path)
            safe_dest = PathHandler.ensure_long_path(temp_dir)
            logger.info(f"[#file]å¼€å§‹è§£å‹: {safe_src} ")
            try:
                # ä½¿ç”¨ 7z x å‘½ä»¤ï¼Œä¿æŒç›®å½•ç»“æ„
                cmd = ['7z', 'x', str(safe_src), f'-o{str(safe_dest)}', '-y']
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    logger.info(f"[#file]ä½¿ç”¨7zæˆåŠŸè§£å‹: {safe_src}")
                    return True
                else:
                    logger.info(f"[#file]7zè§£å‹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ: {result.stderr}")
                    
                # å¦‚æœ x å‘½ä»¤å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ e å‘½ä»¤ï¼ˆä¸ä¿æŒç›®å½•ç»“æ„ï¼‰
                # cmd = ['7z', 'e', str(safe_src), f'-o{str(safe_dest)}', '-y']
                # result = subprocess.run(cmd, capture_output=True, text=True)
                # if result.returncode == 0:
                #     logger.info(f"[#file]ä½¿ç”¨7z (e)æˆåŠŸè§£å‹: {safe_src}")
                #     return True
                # else:
                #     logger.info(f"[#file]7z (e)è§£å‹ä¹Ÿå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ¡ˆ: {result.stderr}")
            except Exception as e:
                logger.info(f"[#file]7zè§£å‹å‡ºé”™ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ: {e}")
                
            # å°è¯•ä½¿ç”¨ zipfile
            try:
                with zipfile.ZipFile(str(safe_src), 'r') as zip_ref:
                    file_list = zip_ref.namelist()
                    if not file_list:
                        logger.info(f"[#file]å‹ç¼©åŒ…ä¸ºç©º: {safe_src}")
                        return False
                    for file_name in file_list:
                        decoded_name = file_name
                        try:
                            decoded_name = file_name.encode('cp437').decode('gbk', errors='ignore')
                        except UnicodeError:
                            pass
                        target_path = safe_dest / decoded_name
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        if not file_name.endswith('/'):
                            with zip_ref.open(file_name) as source, open(str(target_path), 'wb') as target:
                                shutil.copyfileobj(source, target)
                    logger.info(f"[#file]ä½¿ç”¨zipfileæˆåŠŸè§£å‹: {safe_src}")
                    return True
            except zipfile.BadZipFile:
                logger.info(f"[#file]zipfileè§£å‹å¤±è´¥: {safe_src}")
            except Exception as e:
                logger.info(f"[#file]zipfileè§£å‹å‡ºé”™: {e}")
                
            logger.info(f"[#file]æ‰€æœ‰è§£å‹æ–¹æ¡ˆéƒ½å¤±è´¥: {safe_src}")
            return False
        except Exception as e:
            logger.info(f"[#file]è§£å‹æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False

    def handle_size_comparison(self, file_path, new_zip_path, backup_file_path):
        """æ¯”è¾ƒæ–°æ—§æ–‡ä»¶å¤§å°å¹¶å¤„ç†æ›¿æ¢ï¼Œæ”¯æŒé•¿è·¯å¾„"""
        try:
            fs = fsspec.filesystem('file')
            safe_file = PathHandler.ensure_long_path(file_path)
            safe_new = PathHandler.ensure_long_path(new_zip_path)
            safe_backup = PathHandler.ensure_long_path(backup_file_path)
            if not fs.exists(str(safe_new)):
                logger.info(f"[#file]æ–°å‹ç¼©åŒ…ä¸å­˜åœ¨: {safe_new}")
                return (False, 0)
            original_size = fs.info(str(safe_file))['size']
            new_size = fs.info(str(safe_new))['size']
            # å¦‚æœæ–°æ–‡ä»¶å¤§å°è¶…è¿‡åŸæ–‡ä»¶çš„80%ï¼Œè®¤ä¸ºå‹ç¼©æ•ˆæœä¸ç†æƒ³
            SIZE_THRESHOLD_RATIO = 0.8  # 80%
            if new_size >= original_size * SIZE_THRESHOLD_RATIO:
                logger.info(f"[#file]æ–°å‹ç¼©åŒ… ({new_size / 1024 / 1024:.2f}MB) å¤§å°è¶…è¿‡åŸå§‹æ–‡ä»¶ ({original_size / 1024 / 1024:.2f}MB) çš„{SIZE_THRESHOLD_RATIO*100}%ï¼Œå‹ç¼©æ•ˆæœä¸ç†æƒ³")
                fs.delete(str(safe_new))
                if fs.exists(str(safe_backup)):
                    fs.move(str(safe_backup), str(safe_file))
                    # åªæœ‰åœ¨å¯ç”¨äº†rename_cbré€‰é¡¹æ—¶æ‰é‡å‘½åä¸ºCBR
                    if hasattr(self, 'rename_cbr') and self.rename_cbr:
                        new_name = safe_file.with_suffix('.cbr')
                        fs.move(str(safe_file), str(new_name))
                        logger.info(f"[#file]å·²å°†æ–‡ä»¶æ”¹ä¸ºCBR: {new_name}")
                return (False, 0)
            try:
                with fs.open(str(safe_file), 'rb') as f:
                    f.read(1)
            except Exception as e:
                logger.info(f"[#file]æ— æ³•è®¿é—®ç›®æ ‡æ–‡ä»¶ï¼Œå¯èƒ½æ­£åœ¨è¢«ä½¿ç”¨: {safe_file}")
                return (False, 0)
            try:
                fs.delete(str(safe_file))
                fs.move(str(safe_new), str(safe_file))
            except Exception as e:
                logger.info(f"[#file]æ›¿æ¢æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                if fs.exists(str(safe_backup)):
                    try:
                        fs.move(str(safe_backup), str(safe_file))
                        logger.info("[#file]å·²è¿˜åŸåŸå§‹æ–‡ä»¶")
                    except Exception as restore_error:
                        logger.info(f"[#file]è¿˜åŸæ–‡ä»¶å¤±è´¥: {restore_error}")
                return (False, 0)
            if fs.exists(str(safe_backup)):
                try:
                    fs.delete(str(safe_backup))
                    logger.info(f"[#file]å·²åˆ é™¤å¤‡ä»½æ–‡ä»¶: {safe_backup}")
                except Exception as e:
                    logger.info(f"[#file]åˆ é™¤å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
            size_change = (original_size - new_size) / (1024 * 1024)
            logger.info(f"[#file]æ›´æ–°å‹ç¼©åŒ…: {safe_file} (å‡å°‘ {size_change:.2f}MB)")
            return (True, size_change)
        except Exception as e:
            logger.info(f"[#file]æ¯”è¾ƒæ–‡ä»¶å¤§å°æ—¶å‡ºé”™: {e}")
            if fs.exists(str(safe_backup)):
                try:
                    fs.move(str(safe_backup), str(safe_file))
                    logger.info("[#file]å·²è¿˜åŸåŸå§‹æ–‡ä»¶")
                except Exception as restore_error:
                    logger.info(f"[#file]è¿˜åŸæ–‡ä»¶å¤±è´¥: {restore_error}")
            if fs.exists(str(safe_new)):
                try:
                    fs.delete(str(safe_new))
                except Exception as remove_error:
                    logger.info(f"[#file]åˆ é™¤æ–°æ–‡ä»¶å¤±è´¥: {remove_error}")
            return (False, 0)

    def is_artbook(self, file_path, artbook_keywords):
        """æ£€æŸ¥æ˜¯å¦ä¸ºç”»é›†"""
        file_path_str = str(file_path)
        file_name = os.path.basename(file_path_str).lower()
        return any((keyword.lower() in file_name or keyword.lower() in file_path_str.lower() for keyword in artbook_keywords))


class ArchiveContent:
    """å‹ç¼©åŒ…å†…å®¹å¤„ç†ç±»"""

    def __init__(self):
        self.directory_handler = DirectoryHandler()

    def cleanup_and_compress(self, temp_dir, processed_files, skipped_files, new_zip_path):
        """æ¸…ç†æ–‡ä»¶å¹¶åˆ›å»ºæ–°å‹ç¼©åŒ…"""
        try:
            logger.info(f"[#file]å¤„ç†äº† {len(processed_files)} å¼ å›¾ç‰‡ï¼Œè·³è¿‡äº† {len(skipped_files)} å¼ å›¾ç‰‡")
            if backup_removed_files_enabled:
                self.backup_removed_files(new_zip_path, processed_files, skipped_files)
            removed_count = 0
            for file_path in processed_files:
                try:
                    if file_path and os.path.exists(file_path):
                        os.remove(file_path)
                        removed_count += 1
                        logger.info(f"[#file]å·²åˆ é™¤æ–‡ä»¶: {file_path}")
                except Exception as e:
                    logger.info(f"[#file]åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue
            logger.info(f"[#file]å·²åˆ é™¤ {removed_count} ä¸ªæ–‡ä»¶")
            empty_dirs_removed = self.directory_handler.remove_empty_directories(temp_dir)
            if empty_dirs_removed != 0:
                logger.info(f"[#file]å·²åˆ é™¤ {empty_dirs_removed} ä¸ªç©ºæ–‡ä»¶å¤¹")
            self.directory_handler.flatten_single_subfolder(temp_dir, [])
            if not os.path.exists(temp_dir):
                logger.info(f"[#file]ä¸´æ—¶ç›®å½•ä¸å­˜åœ¨: {temp_dir}")
                return False
            if not any(os.scandir(temp_dir)):
                logger.info(f"[#file]ä¸´æ—¶ç›®å½•ä¸ºç©º: {temp_dir}")
                return False
            try:
                with zipfile.ZipFile(new_zip_path, 'w', zipfile.ZIP_DEFLATED) as zip_ref:
                    for root, _, files in os.walk(temp_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, temp_dir)
                            zip_ref.write(file_path, arcname)
                if not os.path.exists(new_zip_path):
                    logger.info(f"[#archive]å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥: {new_zip_path}")
                    return False
                logger.info(f"[#archive]æˆåŠŸåˆ›å»ºæ–°å‹ç¼©åŒ…: {new_zip_path}")
                return True
            except Exception as e:
                logger.info(f"[#archive]åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥: {e}")
                return False
        except Exception as e:
            logger.info(f"[#archive]æ¸…ç†å’Œå‹ç¼©æ—¶å‡ºé”™: {e}")
            return False

    def create_new_zip(self, zip_path, temp_dir):
        """ä»ä¸´æ—¶ç›®å½•åˆ›å»ºæ–°çš„å‹ç¼©åŒ…"""
        try:
            if not any(os.scandir(temp_dir)):
                logger.info(f"[#file]ä¸´æ—¶ç›®å½•ä¸ºç©º: {temp_dir}")
                return False
            cmd = ['7z', 'a', '-tzip', zip_path, os.path.join(temp_dir, '*')]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                if not os.path.exists(zip_path):
                    logger.info(f"[#archive]å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥: {zip_path}")
                    return False
                logger.info(f"[#archive]æˆåŠŸåˆ›å»ºæ–°å‹ç¼©åŒ…: {zip_path} ({os.path.getsize(zip_path) / 1024 / 1024:.2f} MB)")
                return True
            else:
                logger.info(f"[#archive]åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            logger.info(f"[#file]åˆ›å»ºå‹ç¼©åŒ…æ—¶å‡ºé”™: {e}")
            return False

    def read_zip_contents(self, zip_path):
        """è¯»å–å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶åˆ—è¡¨"""
        try:
            cmd = ['7z', 'l', '-slt', zip_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.info(f"[#file]è¯»å–å‹ç¼©åŒ…å¤±è´¥: {zip_path}\né”™è¯¯: {result.stderr}")
                return []
            files = []
            cur_file = None
            for line in result.stdout.split('\n'):
                line = line.strip()
                if line.startswith('Path = '):
                    cur_file = line[7:]
                    if cur_file and (not cur_file.endswith('/')):
                        files.append(cur_file)
            logger.info(f"[#file]Found {len(files)} files in archive: {zip_path}")
            return files
        except Exception as e:
            logger.info(f"[#file]è¯»å–å‹ç¼©åŒ…å†…å®¹æ—¶å‡ºé”™ {zip_path}: {e}")
            return []

    def extract_file_from_zip(self, zip_path, file_name, temp_dir):
        """ä»å‹ç¼©åŒ…ä¸­æå–å•ä¸ªæ–‡ä»¶"""
        extract_path = os.path.join(temp_dir, file_name)
        success, _ = ArchiveHandler().run_7z_command('e', zip_path, 'æå–æ–‡ä»¶', [f'-o{temp_dir}', file_name, '-y'])
        if success and os.path.exists(extract_path):
            with open(extract_path, 'rb') as f:
                data = f.read()
            os.remove(extract_path)
            return data
        return None

    def delete_backup_if_successful(self, backup_path):
        """å¦‚æœå¤„ç†æˆåŠŸåˆ™åˆ é™¤å¤‡ä»½æ–‡ä»¶"""
        if os.path.exists(backup_path) and backup_path.endswith('.bak'):
            try:
                logger.info(f"[#file]å°†å¤‡ä»½æ–‡ä»¶ç§»è‡³å›æ”¶ç«™: {backup_path}")
                send2trash(backup_path)
            except Exception as e:
                logger.info(f"[#file]ç§»åŠ¨å¤‡ä»½æ–‡ä»¶åˆ°å›æ”¶ç«™å¤±è´¥: {backup_path} - {e}")

    def backup_removed_files(self, zip_path, removed_files, duplicate_files):
        """å°†åˆ é™¤çš„æ–‡ä»¶å¤‡ä»½åˆ°trashæ–‡ä»¶å¤¹ä¸­ï¼Œä¿æŒåŸå§‹ç›®å½•ç»“æ„"""
        try:
            if not removed_files and (not duplicate_files):
                return
            zip_name = os.path.splitext(os.path.basename(zip_path))[0]
            trash_dir = os.path.join(os.path.dirname(zip_path), f'{zip_name}.trash')
            os.makedirs(trash_dir, exist_ok=True)
            for file_path in removed_files:
                rel_path = os.path.relpath(file_path, os.path.dirname(zip_path))
                dest_path = os.path.join(trash_dir, 'removed', rel_path)
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                shutil.copy2(file_path, dest_path)
            for file_path in duplicate_files:
                rel_path = os.path.relpath(file_path, os.path.dirname(zip_path))
                dest_path = os.path.join(trash_dir, 'duplicates', rel_path)
                os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                shutil.copy2(file_path, dest_path)
            logger.info(f"[#file]å·²å¤‡ä»½åˆ é™¤çš„æ–‡ä»¶åˆ°: {trash_dir}")
        except Exception as e:
            logger.info(f"[#file]å¤‡ä»½åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {e}")


class Performance:
    """
    ç±»æè¿°
    """

    def get_optimal_thread_count(self, image_count):
        """æ ¹æ®å›¾ç‰‡æ•°é‡è·å–æœ€ä¼˜çº¿ç¨‹æ•°"""
        if image_count <= 10:
            return 2
        elif image_count <= 50:
            return min(4, os.cpu_count() or 4)
        else:
            return min(8, os.cpu_count() or 4)

# æ›´æ–° PerformanceConfig ç±»
class PerformanceConfig:
    """æ€§èƒ½é…ç½®ç®¡ç†ç±»"""
    
    def __init__(self, config_path=None):
        self.config_path = config_path or PERFORMANCE_CONFIG_PATH
        self._load_config()

    def _load_config(self):
        """åŠ è½½æ€§èƒ½é…ç½®"""
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("performance_config", self.config_path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            self.get_thread_count = module.get_thread_count
            self.get_batch_size = module.get_batch_size
            return True
        except Exception as e:
            logger.info(f"[#file]åŠ è½½æ€§èƒ½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            self.get_thread_count = self.default_thread_count
            self.get_batch_size = self.default_batch_size
            return False

    @staticmethod
    def default_batch_size():
        """è·å–é»˜è®¤æ‰¹å¤„ç†å¤§å°"""
        return 10

    @staticmethod
    def default_thread_count():
        """è·å–é»˜è®¤çº¿ç¨‹æ•°"""
        return min(4, os.cpu_count() or 4)

    def set_performance_config_path(self, path):
        """è®¾ç½®æ€§èƒ½é…ç½®æ–‡ä»¶è·¯å¾„"""
        if os.path.exists(path):
            self.config_path = path
            success = self._load_config()
            if success:
                logger.info(f"[#file]å·²è®¾ç½®æ€§èƒ½é…ç½®æ–‡ä»¶è·¯å¾„: {path}")
            return success
        else:
            logger.info(f"[#file]æ€§èƒ½é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return False

    def get_optimal_thread_count(self, image_count):
        """æ ¹æ®å›¾ç‰‡æ•°é‡è·å–æœ€ä¼˜çº¿ç¨‹æ•°"""
        if image_count <= 10:
            return 2
        elif image_count <= 50:
            return min(4, os.cpu_count() or 4)
        else:
            return min(8, os.cpu_count() or 4)

class EfficiencyTracker:
    """æ•ˆç‡è·Ÿè¸ªç±»"""

    def __init__(self, config=None):
        self.config = config or EFFICIENCY_CHECK_CONFIG
        self.processed_files = []
        self.inefficient_count = 0

    def add_result(self, original_size, new_size):
        """æ·»åŠ ä¸€ä¸ªè½¬æ¢ç»“æœ"""
        if original_size <= 0:
            return False
        reduction_percent = (original_size - new_size) / original_size * 100
        self.processed_files.append(reduction_percent)
        if len(self.processed_files) >= self.config['min_files_to_check']:
            if reduction_percent < self.config['min_efficiency_threshold']:
                self.inefficient_count += 1
        return self.should_continue()

    def reset(self):
        """é‡ç½®è·Ÿè¸ªå™¨"""
        self.processed_files = []
        self.inefficient_count = 0

    def get_average_efficiency(self):
        """è·å–å¹³å‡æ•ˆç‡"""
        if not self.processed_files:
            return 0
        return sum(self.processed_files) / len(self.processed_files)

    def should_continue(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­å¤„ç†"""
        return self.inefficient_count < self.config['max_inefficient_files']



class Monitor:
    """ç›‘æ§ç±»"""
    def __init__(self):
        """åˆå§‹åŒ–ç›‘æ§ç±»"""
        self.total_files = 0
        self.processed_files = 0
        self.skipped_files = 0  # æ·»åŠ è·³è¿‡æ–‡ä»¶è®¡æ•°
        self.cur_file = ""
        self.start_time = None
        self.last_config = (0, 0)
        self.current_batch_size = get_batch_size()  # åˆå§‹åŒ–æ‰¹å¤„ç†å¤§å°
    def handle_config_update(self, new_threads: int, new_batch: int):
        """ç»Ÿä¸€å¤„ç†é…ç½®æ›´æ–°"""
        if (new_threads, new_batch) == self.last_config:
            return  # æ— å˜åŒ–æ—¶è·³è¿‡
        
        logger.info(f"[#config] åº”ç”¨æ–°é…ç½®: çº¿ç¨‹æ•°={new_threads} æ‰¹å¤„ç†={new_batch}")
        
        # æ›´æ–°çº¿ç¨‹æ± 
        global executor
        executor.shutdown(wait=False)
        executor = ThreadPoolExecutor(max_workers=new_threads)
        
        # æ›´æ–°æ‰¹å¤„ç†å¤§å°
        self.current_batch_size = new_batch
        self.last_config = (new_threads, new_batch)
        logger.info(f"[#runtime] é…ç½®å·²ç”Ÿæ•ˆ | çº¿ç¨‹: {new_threads} | æ‰¹æ¬¡: {new_batch}")
    def _update_executor(self, new_threads: int, new_batch: int):
        """åŠ¨æ€æ›´æ–°çº¿ç¨‹æ± çš„å®ä¾‹æ–¹æ³•"""
        global executor
        executor.shutdown(wait=False)
        executor = ThreadPoolExecutor(max_workers=new_threads)
        # logger.info(f"[#performance] çº¿ç¨‹æ± å·²æ›´æ–°è‡³{new_threads} workers")

    @staticmethod
    def update_performance_info():
        """æ›´æ–°æ€§èƒ½é¢æ¿ä¿¡æ¯ï¼ˆå•æ¬¡æ›´æ–°ï¼‰"""
        thread_count = get_thread_count()
        batch_size = get_batch_size()
        logger.info(f"[#performance]çº¿ç¨‹æ•°: {thread_count} æ‰¹å¤„ç†å¤§å°: {batch_size} ")

    def auto_run_process(self, directories, params, interval_minutes=10, infinite_mode=False):
        """è‡ªåŠ¨è¿è¡Œå¤„ç†è¿‡ç¨‹"""
        try:
            # åˆå§‹åŒ–å¸ƒå±€
            self.start_time = time.time()
            
            self._run_process_loop(directories, params, interval_minutes, infinite_mode)
        except KeyboardInterrupt:
            logger.info("[#file]âš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        except Exception as e:
            logger.info(f"[#file]âŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")

    def _update_status(self):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if self.total_files > 0:
            total_processed = self.processed_files + self.skipped_files  # åŒ…å«å·²å¤„ç†å’Œè·³è¿‡çš„æ–‡ä»¶
            progress = total_processed / self.total_files * 100
            logger.info(f"[#status]æ€»è¿›åº¦: (âœ…{self.processed_files}+â­ï¸{self.skipped_files}/{self.total_files}) {progress:.1f}%")

    def _run_process_loop(self, directories, params, interval_minutes, infinite_mode):
        """å¤„ç†å¾ªç¯çš„å…·ä½“å®ç°"""
        round_count = 0
        processed_files = set()
        skipped_files = {}
        occupied_files = set()
        
        while True:
            try:
                round_count += 1
                logger.info(f"[#file]ğŸ”„ å¼€å§‹ç¬¬ {round_count} è½®å¤„ç†...")
                
                # è·å–è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
                files_to_process = self._get_files_to_process(directories, processed_files, skipped_files, occupied_files)
                
                if not files_to_process:
                    if infinite_mode:
                        logger.info("[#file]â¸ï¸ å½“å‰æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼Œç»§ç»­ç›‘æ§...")
                        self._wait_next_round(interval_minutes)
                        continue
                    else:
                        logger.info("[#file]âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ")
                        break
                
                # æ›´æ–°æ€»æ–‡ä»¶æ•°
                self.total_files = len(files_to_process)
                self._update_status()
                
                # å¤„ç†æ–‡ä»¶
                self._process_files(files_to_process, params, processed_files, skipped_files, occupied_files)
                
                # ç­‰å¾…ä¸‹ä¸€è½®
                if infinite_mode or occupied_files or len(processed_files) > 0 or len(skipped_files) > 0:
                    wait_minutes = min(round_count, 10)
                    logger.info(f"[#file]â¸ï¸ ç­‰å¾… {wait_minutes} åˆ†é’Ÿåå¼€å§‹ä¸‹ä¸€è½®...")
                    self._wait_next_round(wait_minutes)
                    continue
                else:
                    break
                    
            except Exception as e:
                logger.info(f"[#file]âŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")
                if infinite_mode:
                    logger.info(f"[#file]âš ï¸ å¤„ç†å‡ºé”™: {e}ï¼Œç­‰å¾…ä¸‹ä¸€è½®...")
                    self._wait_next_round(interval_minutes)
                    continue
                else:
                    break

    def _wait_next_round(self, minutes):
        """ç­‰å¾…ä¸‹ä¸€è½®å¤„ç†"""
        total_seconds = minutes * 60
        for remaining in range(total_seconds, 0, -1):
            logger.info(f"[#status]â³ ç­‰å¾…ä¸‹ä¸€è½® å‰©ä½™æ—¶é—´: {remaining // 60}åˆ†{remaining % 60}ç§’")
            logger.info(f"[@status] ç­‰å¾…ä¸‹ä¸€è½®{remaining / total_seconds * 100:.1f}%")
            
            time.sleep(1)


    def _process_files(self, files, params, processed_files, skipped_files, occupied_files):
        """å¤„ç†æ–‡ä»¶åˆ—è¡¨"""
        if not files:
            return
            
        for index, file_path in enumerate(files, 1):
            try:
                file_name = os.path.basename(file_path)

                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«å ç”¨
                if self._is_file_locked(file_path):
                    occupied_files.add(file_path)
                    logger.info(f"[#file]âš ï¸ æ–‡ä»¶è¢«å ç”¨: {file_name}")
                    continue
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                result = ArchiveHandler().process_single_archive(file_path, params)
                
                if result:
                    processed_files.add(file_path)
                    self.processed_files += 1
                    self._update_status()
                    logger.info(f"[#file]âœ… å¤„ç†å®Œæˆ: {file_name}")
                else:
                    reason = skipped_files.get(file_path, "æœªçŸ¥åŸå› ")
                    if file_path in skipped_files:
                        logger.info(f"[#file]âš ï¸ è·³è¿‡æ–‡ä»¶: {file_name} - {reason}")
                        self.skipped_files += 1  # å¢åŠ è·³è¿‡æ–‡ä»¶è®¡æ•°
                    else:
                        logger.info(f"[#file]âš ï¸ è·³è¿‡æ–‡ä»¶: {file_name} - å¤„ç†å¤±è´¥æˆ–ä¸éœ€è¦å¤„ç†")
                    
            except Exception as e:
                logger.info(f"[#file]âŒ å¤„ç†æ–‡ä»¶å‡ºé”™ {file_path}: {e}")


    def _get_files_to_process(self, directories, processed_files, skipped_files, occupied_files):
        """è·å–éœ€è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        files_to_process = []
        for directory in directories:
            if os.path.isfile(directory):
                if any(directory.lower().endswith(ext) for ext in SUPPORTED_ARCHIVE_FORMATS):
                    if directory not in processed_files and directory not in skipped_files:
                        files_to_process.append(directory)
            else:
                for root, _, files in os.walk(directory):
                    for file in files:
                        if any(file.lower().endswith(ext) for ext in SUPPORTED_ARCHIVE_FORMATS):
                            file_path = os.path.join(root, file)
                            if file_path not in processed_files and file_path not in skipped_files:
                                files_to_process.append(file_path)
        
        # æ£€æŸ¥å¹¶ç§»é™¤è¢«å ç”¨çš„æ–‡ä»¶
        files_to_process = [f for f in files_to_process if not self._is_file_locked(f)]
        
        if files_to_process:
            logger.info(f"[#file]ğŸ“ æ‰¾åˆ° {len(files_to_process)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        return files_to_process

    def _is_file_locked(self, file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«é”å®š"""
        try:
            with open(file_path, 'rb') as f:
                f.read(1)
            return False
        except (IOError, PermissionError):
            return True

class InputHandler:
    """è¾“å…¥å¤„ç†ç±»"""

    def parse_arguments(self):
        """è§£æå‘½ä»¤è¡Œå‚æ•°"""
        parser = argparse.ArgumentParser(description='å›¾ç‰‡å‹ç¼©åŒ…è½¬æ¢å·¥å…·')
        parser.add_argument('--clipboard', '-c', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
        parser.add_argument('--format', '-f', choices=['avif', 'webp', 'jxl', 'jpg', 'png'], default='avif', help='æŒ‡å®šè½¬æ¢çš„ç›®æ ‡æ ¼å¼ (é»˜è®¤: avif)')
        parser.add_argument('--quality', '-q', type=int, default=90, help='æŒ‡å®šå‹ç¼©è´¨é‡ (1-100, é»˜è®¤: 90)')
        parser.add_argument('--lossless', '-l', action='store_true', help='ä½¿ç”¨æ— æŸå‹ç¼©æ¨¡å¼')
        parser.add_argument('--jxl-jpeg-lossless', '-j', action='store_true', help='ä½¿ç”¨ JXL çš„ JPEG æ— æŸè½¬æ¢æ¨¡å¼ï¼ˆä»…åœ¨æ ¼å¼ä¸º jxl æ—¶æœ‰æ•ˆï¼‰')
        parser.add_argument('--interval', '-i', type=int, default=10, help='è‡ªåŠ¨è¿è¡Œçš„æ—¶é—´é—´éš”ï¼ˆåˆ†é’Ÿï¼Œé»˜è®¤10åˆ†é’Ÿï¼‰')
        parser.add_argument('--min-width', '-w', type=int, default=0, help='åªå¤„ç†å®½åº¦å¤§äºæŒ‡å®šå€¼çš„å›¾ç‰‡ï¼ˆåƒç´ ï¼Œé»˜è®¤0è¡¨ç¤ºä¸é™åˆ¶ï¼‰')
        parser.add_argument('--keywords', '-k', action='store_true', help='ä½¿ç”¨å†…ç½®å…³é”®è¯åˆ—è¡¨è¿‡æ»¤å‹ç¼©åŒ…')
        parser.add_argument('--performance-config', '-p', type=str, help='æŒ‡å®šæ€§èƒ½é…ç½®æ–‡ä»¶çš„è·¯å¾„')
        parser.add_argument('--infinite', '-inf', action='store_true', help='å¯ç”¨æ— é™å¾ªç¯æ¨¡å¼ï¼Œå³ä½¿æ²¡æœ‰å˜åŒ–ä¹Ÿç»§ç»­ç›‘æ§')
        parser.add_argument('--rename-cbr', '-r', action='store_true', help='å¯ç”¨ä½å‹ç¼©ç‡æ–‡ä»¶é‡å‘½åä¸ºCBRåŠŸèƒ½')
        return parser.parse_args()

    def get_paths_from_clipboard(self):
        """ä»å‰ªè´´æ¿è¯»å–å¤šè¡Œè·¯å¾„"""
        try:
            clipboard_content = pyperclip.paste()
            if not clipboard_content:
                return []
            paths = [path.strip().strip('"') for path in clipboard_content.splitlines() if path.strip()]
            valid_paths = [path for path in paths if os.path.exists(path)]
            if valid_paths:
                logger.info(f"[#file]ä»å‰ªè´´æ¿è¯»å–åˆ° {len(valid_paths)} ä¸ªæœ‰æ•ˆè·¯å¾„")
            else:
                logger.info(f"[#file]å‰ªè´´æ¿ä¸­æ²¡æœ‰æœ‰æ•ˆè·¯å¾„")
            return valid_paths
        except ImportError:
            logger.info(f"[#file]æœªå®‰è£… pyperclip æ¨¡å—ï¼Œæ— æ³•è¯»å–å‰ªè´´æ¿")
            return []
        except Exception as e:
            logger.info(f"[#file]è¯»å–å‰ªè´´æ¿æ—¶å‡ºé”™: {e}")
            return []




class ProcessingQueue:
    """æ–‡ä»¶å¤„ç†é˜Ÿåˆ—ç®¡ç†ç±»"""
    
    def __init__(self):
        self.pending_files = set()
        self.processing_files = set()
        self.processing_lock = threading.Lock()
        self.last_check_time = time.time()
        self.check_interval = 10

class FileWatcher:
    """æ–‡ä»¶ç›‘æ§ç±»"""
    
    def __init__(self, processing_queue):
        self.processing_queue = processing_queue
    
    def on_created(self, event):
        """å¤„ç†æ–°æ–‡ä»¶åˆ›å»ºäº‹ä»¶"""
        if event.is_directory:
            return
        file_path = event.src_path
        if not any((file_path.lower().endswith(ext) for ext in SUPPORTED_ARCHIVE_FORMATS)):
            return
        with self.processing_queue.processing_lock:
            self.processing_queue.pending_files.add(file_path)
            logger.info(f"[#file]æ·»åŠ æ–‡ä»¶åˆ°å¾…å¤„ç†åˆ—è¡¨: {file_path}")
            self.processing_queue.last_check_time = 0
            self.check_pending_files()

    def check_pending_files(self):
        """æ£€æŸ¥å¾…å¤„ç†æ–‡ä»¶æ˜¯å¦å¯ä»¥å¤„ç†"""
        cur_time = time.time()
        if cur_time - self.processing_queue.last_check_time < self.processing_queue.check_interval:
            return
            
        self.processing_queue.last_check_time = cur_time
        
        with self.processing_queue.processing_lock:
            files_to_remove = set()
            files_to_process = set()
            
            for file_path in self.processing_queue.pending_files:
                try:
                    if not os.path.exists(file_path):
                        files_to_remove.add(file_path)
                        continue
                        
                    try:
                        with open(file_path, 'rb') as f:
                            f.read(1)
                    except (IOError, PermissionError):
                        logger.info(f"[#file]æ–‡ä»¶è¢«å ç”¨ï¼Œè·³è¿‡: {file_path}")
                        files_to_remove.add(file_path)
                        continue
                        
                    files_to_process.add(file_path)
                    
                except Exception as e:
                    logger.info(f"[#file]æ£€æŸ¥æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
                    files_to_remove.add(file_path)
            
            self.processing_queue.pending_files -= files_to_remove
            self.processing_queue.pending_files -= files_to_process
            
            for file_path in files_to_process:
                if file_path not in self.processing_queue.processing_files:
                    self.processing_queue.processing_files.add(file_path)
                    threading.Thread(
                        target=self._process_file,
                        args=(file_path,)
                    ).start()

    def _process_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶çš„çº¿ç¨‹å‡½æ•°"""
        try:
            ArchiveHandler().process_single_archive(file_path, {})
        except Exception as e:
            logger.info(f"[#file]å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
        finally:
            with self.processing_queue.processing_lock:
                self.processing_queue.processing_files.remove(file_path)

# def parse_arguments():
#     """è§£æå‘½ä»¤è¡Œå‚æ•°"""
#     parser = argparse.ArgumentParser(description='å›¾ç‰‡å‹ç¼©åŒ…è½¬æ¢å·¥å…·')
#     parser.add_argument('--clipboard', '-c', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
#     parser.add_argument('--format', '-f', choices=['avif', 'webp', 'jxl', 'jpg', 'png'], default='avif', help='æŒ‡å®šè½¬æ¢çš„ç›®æ ‡æ ¼å¼ (é»˜è®¤: avif)')
#     parser.add_argument('--quality', '-q', type=int, default=90, help='æŒ‡å®šå‹ç¼©è´¨é‡ (1-100, é»˜è®¤: 90)')
#     parser.add_argument('--lossless', '-l', action='store_true', help='ä½¿ç”¨æ— æŸå‹ç¼©æ¨¡å¼')
#     parser.add_argument('--jxl-jpeg-lossless', '-j', action='store_true', help='ä½¿ç”¨ JXL çš„ JPEG æ— æŸè½¬æ¢æ¨¡å¼ï¼ˆä»…åœ¨æ ¼å¼ä¸º jxl æ—¶æœ‰æ•ˆï¼‰')
#     parser.add_argument('--interval', '-i', type=int, default=10, help='è‡ªåŠ¨è¿è¡Œçš„æ—¶é—´é—´éš”ï¼ˆåˆ†é’Ÿï¼Œé»˜è®¤10åˆ†é’Ÿï¼‰')
#     parser.add_argument('--min-width', '-w', type=int, default=0, help='åªå¤„ç†å®½åº¦å¤§äºæŒ‡å®šå€¼çš„å›¾ç‰‡ï¼ˆåƒç´ ï¼Œé»˜è®¤0è¡¨ç¤ºä¸é™åˆ¶ï¼‰')
#     parser.add_argument('--keywords', '-k', action='store_true', help='ä½¿ç”¨å†…ç½®å…³é”®è¯åˆ—è¡¨è¿‡æ»¤å‹ç¼©åŒ…')
#     parser.add_argument('--performance-config', '-p', type=str, help='æŒ‡å®šæ€§èƒ½é…ç½®æ–‡ä»¶çš„è·¯å¾„')
#     parser.add_argument('--infinite', '-inf', action='store_true', help='å¯ç”¨æ— é™å¾ªç¯æ¨¡å¼ï¼Œå³ä½¿æ²¡æœ‰å˜åŒ–ä¹Ÿç»§ç»­ç›‘æ§')
#     return parser.parse_args()

class ErrorHandler:
    """é”™è¯¯å¤„ç†ç±»"""
    
    @staticmethod
    def handle_file_error(e, file_path, operation):
        """å¤„ç†æ–‡ä»¶æ“ä½œé”™è¯¯"""
        if isinstance(e, PermissionError):
            logger.info(f"[#file]{operation}å¤±è´¥(æƒé™ä¸è¶³): {file_path}")
        elif isinstance(e, FileNotFoundError):
            logger.info(f"[#file]{operation}å¤±è´¥(æ–‡ä»¶ä¸å­˜åœ¨): {file_path}")
        else:
            logger.info(f"[#file]{operation}å¤±è´¥: {file_path} - {str(e)}")

    @staticmethod
    def handle_archive_error(e, archive_path):
        """å¤„ç†å‹ç¼©åŒ…é”™è¯¯"""
        if isinstance(e, zipfile.BadZipFile):
            logger.info(f"[#file]æ— æ•ˆçš„å‹ç¼©åŒ…æ ¼å¼: {archive_path}")
        elif isinstance(e, PermissionError):
            logger.info(f"[#file]å‹ç¼©åŒ…è®¿é—®æƒé™ä¸è¶³: {archive_path}")
        else:
            logger.info(f"[#file]å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™: {archive_path} - {str(e)}")

    @staticmethod
    def handle_image_error(e, image_path):
        """å¤„ç†å›¾ç‰‡é”™è¯¯"""
        if isinstance(e, Image.DecompressionBombError):
            logger.info(f"[#file]å›¾ç‰‡è¿‡å¤§: {image_path}")
        elif isinstance(e, Image.UnidentifiedImageError):
            logger.info(f"[#file]æ— æ³•è¯†åˆ«çš„å›¾ç‰‡æ ¼å¼: {image_path}")
        else:
            logger.info(f"[#file]å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {image_path} - {str(e)}")

class ConfigManager:
    """é…ç½®ç®¡ç†ç±»"""
    
    def __init__(self):
        self.config = {}
        self.config_file = None
        
    def load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
                self.config_file = config_file
            return True
        except Exception as e:
            logger.info(f"[#file]åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        if not self.config_file:
            logger.info(f"[#file]æœªæŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„")
            return False
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                yaml.safe_dump(self.config, f, allow_unicode=True)
            return True
        except Exception as e:
            logger.info(f"[#file]ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def get_value(self, key, default=None):
        """è·å–é…ç½®å€¼"""
        return self.config.get(key, default)

    def set_value(self, key, value):
        """è®¾ç½®é…ç½®å€¼"""
        self.config[key] = value

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç±»"""
    
    def __init__(self):
        self.start_time = time.time()
        self.metrics = {}
        self.checkpoints = {}
        
    def record_metric(self, name, value):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        if name not in self.metrics:
            self.metrics[name] = []
        self.metrics[name].append(value)

    def start_checkpoint(self, name):
        """å¼€å§‹æ£€æŸ¥ç‚¹è®¡æ—¶"""
        self.checkpoints[name] = time.time()

    def end_checkpoint(self, name):
        """ç»“æŸæ£€æŸ¥ç‚¹è®¡æ—¶å¹¶è®°å½•è€—æ—¶"""
        if name in self.checkpoints:
            duration = time.time() - self.checkpoints[name]
            self.record_metric(f'{name}_duration', duration)
            del self.checkpoints[name]
            return duration
        return None

    def get_average_metric(self, name):
        """è·å–æŒ‡æ ‡å¹³å‡å€¼"""
        if name in self.metrics and self.metrics[name]:
            return sum(self.metrics[name]) / len(self.metrics[name])
        return None

    def get_summary(self):
        """è·å–æ€§èƒ½ç›‘æ§æ‘˜è¦"""
        summary = {
            'total_duration': time.time() - self.start_time,
            'metrics': {}
        }
        for name, values in self.metrics.items():
            summary['metrics'][name] = {
                'average': sum(values) / len(values),
                'min': min(values),
                'max': max(values),
                'count': len(values)
            }
        return summary

def init_performance_config(args):
    """åˆå§‹åŒ–æ€§èƒ½é…ç½®"""
    if args.performance_config:
        performance_config = PerformanceConfig(args.performance_config)
        if not performance_config.load_config():
            logger.info(f"[#file]ä½¿ç”¨é»˜è®¤æ€§èƒ½é…ç½®")

def init_keywords(args):
    """åˆå§‹åŒ–å…³é”®è¯è®¾ç½®"""
    if args.keywords:
        keywords = 'internal'
        logger.info(f"[#file]å°†ä½¿ç”¨å†…ç½®å…³é”®è¯åˆ—è¡¨: {INCLUDED_KEYWORDS}")
    else:
        keywords = None
        logger.info("[#file]æœªå¯ç”¨å…³é”®è¯è¿‡æ»¤")
    return keywords

def configure_image_conversion(args):
    """é…ç½®å›¾åƒè½¬æ¢å‚æ•°"""
    target_format = f'.{args.format}'
    IMAGE_CONVERSION_CONFIG['target_format'] = target_format
    quality = args.quality
    
    # åˆ›å»ºå‚æ•°å­—å…¸
    params = {}
    
    # æ›´æ–°å„æ ¼å¼çš„è´¨é‡è®¾ç½®
    for format_config in ['webp_config', 'avif_config', 'jxl_config', 'jpeg_config']:
        IMAGE_CONVERSION_CONFIG[format_config]['quality'] = quality
    
    # å¤„ç†ç‰¹æ®Šæ ¼å¼è®¾ç½®
    if args.format == 'jxl' and args.jxl_jpeg_lossless:
        params['use_cjxl'] = True  # æ·»åŠ æ ‡å¿—ä»¥å¯ç”¨CJXL
        logger.info("[#file]å·²å¯ç”¨ CJXL çš„ JPEG æ— æŸè½¬æ¢æ¨¡å¼")
    elif args.lossless:
        configure_lossless_mode()
        
    return params  # è¿”å›å‚æ•°å­—å…¸

def configure_lossless_mode():
    """é…ç½®æ— æŸæ¨¡å¼å‚æ•°"""
    IMAGE_CONVERSION_CONFIG['webp_config'].update({'lossless': True, 'quality': 100})
    IMAGE_CONVERSION_CONFIG['avif_config'].update({'lossless': True, 'quality': 100, 'speed': 6})
    IMAGE_CONVERSION_CONFIG['jxl_config'].update({
        'lossless': True,
        'quality': 100,
        'effort': 7,
        'jpeg_recompression': False,
        'jpeg_lossless': False
    })
    IMAGE_CONVERSION_CONFIG['jpeg_config']['quality'] = 100
    IMAGE_CONVERSION_CONFIG['png_config'].update({'optimize': True, 'compress_level': 9})
    logger.info("[#file]å·²å¯ç”¨æ™®é€šæ— æŸå‹ç¼©æ¨¡å¼")

def process_directories(use_clipboard, input_handler):
    """å¤„ç†ç›®å½•è¾“å…¥"""
    directories = []
    if use_clipboard:
        directories = input_handler.get_paths_from_clipboard()
    if not directories:
        # ä½¿ç”¨å¯Œæ–‡æœ¬è¾“å…¥ç•Œé¢è·å–è·¯å¾„
        print("è¯·è¾“å…¥è¦å¤„ç†çš„æ–‡ä»¶å¤¹æˆ–å‹ç¼©åŒ…è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")

        
        for i, line in enumerate(directories):
            directory = line.strip().strip('"').strip("'")
            if os.path.exists(directory):
                directories.append(directory)
                progress = (i+1)/len(directories)*100
                logger.info(f"[#file]âœ… å·²æ·»åŠ è·¯å¾„: {directory}")
            else:
                logger.info(f"[#file]è·¯å¾„ä¸å­˜åœ¨: {directory}")
    return directories

def run_with_args(args):
    """ä¾›TUIç•Œé¢è°ƒç”¨çš„å‡½æ•°"""
    # åˆå§‹åŒ–é…ç½®
    init_layout()

    init_performance_config(args)
    keywords = init_keywords(args)
    params = configure_image_conversion(args)

    directories = process_directories(args.clipboard, InputHandler())
    if directories:
        params.update({
            'min_size': min_size,
            'white_threshold': white_threshold,
            'white_score_threshold': white_score_threshold,
            'threshold': threshold,
            'filter_height_enabled': filter_height_enabled,
            'filter_white_enabled': filter_white_enabled,
            'max_workers': get_thread_count(),
            'handle_artbooks': handle_artbooks,
            'artbook_keywords': artbook_keywords,
            'exclude_paths': exclude_paths,
            'ignore_processed_log': ignore_processed_log,
            'ignore_yaml_log': ignore_yaml_log,
            'min_width': args.min_width,
            'keywords': keywords,
            'rename_cbr': args.rename_cbr,
            'batch_size': get_batch_size()
        })
        
        # åˆå§‹åŒ–é¢æ¿å¸ƒå±€

        # å¯åŠ¨æ€§èƒ½é…ç½®GUI
        config_gui_thread = threading.Thread(target=lambda: ConfigGUI().run(), daemon=True)
        config_gui_thread.start()
        logger.info("[#file]ğŸ”§ å·²å¯åŠ¨æ€§èƒ½é…ç½®è°ƒæ•´å™¨")
        
        logger.info(f"[#file]ğŸš€ å¯åŠ¨{('æ— é™å¾ªç¯' if args.infinite else 'è‡ªåŠ¨è¿è¡Œ')}æ¨¡å¼ï¼Œæ¯ {args.interval} åˆ†é’Ÿè¿è¡Œä¸€æ¬¡...")
        monitor = Monitor()
        monitor.auto_run_process(directories, params, args.interval, args.infinite)


def main():
    """ä¸»å‡½æ•°"""

    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        input_handler = InputHandler()
        args = input_handler.parse_arguments()
        run_with_args(args)
    else:
        # æ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°æ—¶å¯åŠ¨TUIç•Œé¢


        # å®šä¹‰å¤é€‰æ¡†é€‰é¡¹
        checkbox_options = [
            ("ä»å‰ªè´´æ¿è¯»å–è·¯å¾„", "clipboard", "--clipboard", True),
            ("å†…ç½®å…³é”®è¯è¿‡æ»¤", "keywords", "--keywords", False),
            ("æ— é™å¾ªç¯inf", "infinite", "--infinite", False),
            ("JXLçš„JPEGæ— æŸè½¬æ¢", "jxl_jpeg_lossless", "--jxl-jpeg-lossless", False),
            ("æ— æŸå‹ç¼©", "lossless", "--lossless", False),
            ("ä½å‹ç¼©ç‡é‡å‘½åCBR", "rename_cbr", "--rename-cbr", False),
        ]

        # å®šä¹‰è¾“å…¥æ¡†é€‰é¡¹
        input_options = [
            ("ç›®æ ‡æ ¼å¼", "format", "--format", "avif", "avif/webp/jxl/jpg/png"),
            ("å‹ç¼©è´¨é‡", "quality", "--quality", "90", "1-100"),
            ("ç›‘æ§é—´éš”(åˆ†é’Ÿ)", "interval", "--interval", "10", "åˆ†é’Ÿ"),
            ("æœ€å°å®½åº¦(åƒç´ )", "min_width", "--min-width", "0", "åƒç´ "),
            ("æ€§èƒ½é…ç½®æ–‡ä»¶", "performance_config", "--performance-config", "", "é…ç½®æ–‡ä»¶è·¯å¾„"),
            ("å¾…å¤„ç†è·¯å¾„", "path", "-p", "", "è¾“å…¥å¾…å¤„ç†æ–‡ä»¶å¤¹è·¯å¾„"),
        ]

        # é¢„è®¾é…ç½®
        preset_configs = {
            "AVIF-90-inf": {
                "description": "AVIFæ ¼å¼ 90è´¨é‡ æ— é™æ¨¡å¼",
                "checkbox_options": ["infinite","clipboard","rename_cbr"],
                "input_values": {
                    "format": "avif",
                    "quality": "90",
                    "interval": "10",
                    "min_width": "0"
                }
            },
            "JXL-CJXL": {  # æ·»åŠ æ–°çš„é¢„è®¾
                "description": "JXLæ ¼å¼ CJXLæ— æŸè½¬æ¢",
                "checkbox_options": ["clipboard", "jxl_jpeg_lossless"],  # å¯ç”¨JPEGæ— æŸ
                "input_values": {
                    "format": "jxl",
                    "quality": "100",  # æ— æŸæ¨¡å¼
                    "interval": "10",
                    "min_width": "0"
                }
            },
            "JXL-90": {
                "description": "JXLæ ¼å¼ 90è´¨é‡",
                "checkbox_options": ["clipboard"],
                "input_values": {
                    "format": "jxl",
                    "quality": "90",
                    "interval": "10",
                    "min_width": "0"
                }
            },
            "JXL-75": {
                "description": "JXLæ ¼å¼ 75è´¨é‡",
                "checkbox_options": ["clipboard"],
                "input_values": {
                    "format": "jxl",
                    "quality": "75",
                    "interval": "10",
                    "min_width": "0"
                }
            },
            "AVIF-90-1800": {
                "description": "AVIFæ ¼å¼ 90è´¨é‡ 1800å®½åº¦è¿‡æ»¤",
                "checkbox_options": ["clipboard","rename_cbr"],
                "input_values": {
                    "format": "avif",
                    "quality": "90",
                    "interval": "10",
                    "min_width": "1800"
                }
            },
            "AVIF-90-1800-kw": {
                "description": "AVIFæ ¼å¼ 90è´¨é‡ 1800å®½åº¦ å…³é”®è¯è¿‡æ»¤",
                "checkbox_options": ["keywords","clipboard","rename_cbr"],
                "input_values": {
                    "format": "avif",
                    "quality": "90",
                    "interval": "10",
                    "min_width": "1800"
                }
            }
        }

        # åˆ›å»ºé…ç½®ç•Œé¢
        app = create_config_app(
            program=__file__,
            checkbox_options=checkbox_options,
            input_options=input_options,
            title="å›¾ç‰‡å‹ç¼©é…ç½®",
            preset_configs=preset_configs
        )
        
        app.run()



if __name__ == '__main__':
    main()



