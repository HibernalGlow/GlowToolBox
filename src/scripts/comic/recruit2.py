import os
import zipfile
import shutil
from PIL import Image
import imagehash
import io
import logging
import json
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import sys
import warnings
from tqdm import tqdm
import yaml
import subprocess
import pillow_jxl
import pillow_avif
from PIL import Image, ExifTags
import re  # ç”¨äºåŒ¹é…å“ˆå¸Œå€¼çš„æ­£åˆ™è¡¨è¾¾å¼
import datetime
from logging.handlers import RotatingFileHandler
import argparse
import pyperclip
from textual.app import App, ComposeResult
from textual.containers import Container, Horizontal, Vertical, ScrollableContainer
from textual.widgets import Button, Header, Footer, RadioSet, RadioButton, Static, Label
from textual.screen import Screen
from textual import events
from textual.binding import Binding
from textual.widgets._radio_button import RadioButton
from rich.text import Text
from textual.widgets import RichLog
from textual.coordinate import Coordinate
from textual.scroll_view import ScrollView
from textual.reactive import reactive
from textual.message import Message
from textual.worker import Worker, get_current_worker
from textual.widgets import DataTable
from textual.design import ColorSystem
from nodes.record.logger_config import setup_logger
config = {
    'script_name': 'recruit_remove',
}
logger, config_info = setup_logger(config)

def load_processed_zips_uuid(processed_zips_file):
    """ä» YAML æ–‡ä»¶ä¸­åŠ è½½å·²å¤„ç†çš„å‹ç¼©åŒ… UUID é›†åˆã€‚"""
    if os.path.exists(processed_zips_file):
        with open(processed_zips_file, 'r', encoding='utf-8') as file:
            try:
                # åªåŠ è½½ UUIDï¼Œä¸åŠ è½½æ–‡ä»¶å
                return set(yaml.safe_load(file) or [])
            except yaml.YAMLError as e:
                logger.error(f"Error reading processed UUIDs from {processed_zips_file}: {e}")
    return set()

def save_processed_zips_uuid(processed_zips_file, processed_zips_set):
    """å°†å¤„ç†è¿‡çš„å‹ç¼©åŒ… UUID é›†åˆä¿å­˜åˆ° YAML æ–‡ä»¶ä¸­ã€‚"""
    with open(processed_zips_file, 'w', encoding='utf-8') as file:
        try:
            # åªä¿å­˜ UUID é›†åˆ
            yaml.safe_dump(list(processed_zips_set), file)
        except yaml.YAMLError as e:
            logger.error(f"Error saving processed UUIDs to {processed_zips_file}: {e}")

def load_hashes(hash_file):
    hash_pattern = re.compile(r"\[hash-(?P<hash>[0-9a-fA-F]+)\]")
    compare_images_hashes = set()
    files_to_rename = []  # å­˜å‚¨éœ€è¦é‡å‘½åçš„æ–‡ä»¶ä¿¡æ¯
    
    # ä» hash_file åŠ è½½å·²ä¿å­˜çš„å“ˆå¸Œå€¼
    if os.path.exists(hash_file):
        try:
            with open(hash_file, 'r') as f:
                data = json.load(f)
                if data:
                    compare_images_hashes.update(imagehash.hex_to_hash(h) for h in data)
        except json.JSONDecodeError:
            pass
    
    # ä»æ–‡ä»¶åä¸­æå–å“ˆå¸Œå€¼æˆ–è®¡ç®—æ–°çš„å“ˆå¸Œå€¼
    compare_folder = 'E:\\1EHV\\[00å»å›¾]'
    for file_name in os.listdir(compare_folder):
        if file_name.lower().endswith(('.png', '.jpg', '.webp', '.jpeg','.avif', '.jxl')):
            # æ£€æŸ¥æ–‡ä»¶åä¸­æ˜¯å¦å·²åŒ…å«å“ˆå¸Œå€¼
            match = hash_pattern.search(file_name)
            if match:
                # ç›´æ¥ä»æ–‡ä»¶åä¸­æå–å“ˆå¸Œå€¼
                img_hash = imagehash.hex_to_hash(match.group('hash'))
                compare_images_hashes.add(img_hash)
            else:
                # è®¡ç®—æ–°çš„å“ˆå¸Œå€¼
                file_path = os.path.join(compare_folder, file_name)
                try:
                    with open(file_path, 'rb') as f:
                        img_bytes = f.read()
                        img_hash = get_image_hash(img_bytes)
                        if img_hash:
                            compare_images_hashes.add(img_hash)
                            # å°†éœ€è¦é‡å‘½åçš„æ–‡ä»¶ä¿¡æ¯å­˜å‚¨èµ·æ¥
                            name, ext = os.path.splitext(file_name)
                            new_name = f"{name}[hash-{img_hash}]{ext}"
                            files_to_rename.append((file_path, os.path.join(compare_folder, new_name)))
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                    continue
    
    # ä¿å­˜éœ€è¦é‡å‘½åçš„æ–‡ä»¶ä¿¡æ¯
    rename_info_file = os.path.join(compare_folder, 'files_to_rename.json')
    try:
        with open(rename_info_file, 'w', encoding='utf-8') as f:
            json.dump(files_to_rename, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜é‡å‘½åä¿¡æ¯æ—¶å‡ºé”™: {e}")
    
    return compare_images_hashes

def save_hashes(hash_file, hashes):
    with open(hash_file, 'w') as f:
        json.dump([str(h) for h in hashes], f)

def load_yaml_uuid_from_archive(archive_path):
    """å°è¯•ä»å‹ç¼©åŒ…å†…åŠ è½½ YAML æ–‡ä»¶ä»¥è·å– UUIDï¼ˆæ–‡ä»¶åï¼‰ã€‚"""
    try:
        command = ['7z', 'l', archive_path]
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        for line in result.stdout.splitlines():
            line = line.strip()
            if line.endswith('.yaml'):
                parts = line.split()
                yaml_filename = parts[-1]
                yaml_uuid = os.path.splitext(yaml_filename)[0]
                return yaml_uuid
    except Exception as e:
        print(f"æ— æ³•åŠ è½½å‹ç¼©åŒ…ä¸­çš„ YAML æ–‡ä»¶: {e}")
    return None

def get_image_hash(img_bytes):
    try:
        img = Image.open(io.BytesIO(img_bytes))
        return imagehash.phash(img)
    except Exception as e:
        # æ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•é”™è¯¯ï¼Œç„¶åè·³è¿‡è¯¥å›¾ç‰‡
        # logger.error(f"Error processing image: {e}")
        return None

# åˆ¤æ–­ä¸¤å¼ å›¾ç‰‡æ˜¯å¦ç›¸ä¼¼
def are_images_similar(hash1, hash2, threshold):
    return abs(hash1 - hash2) <= threshold

# ä½¿ç”¨7zå‘½ä»¤è¡Œå·¥å…·åˆ—å‡ºå‹ç¼©å†…å®¹ï¼ˆéšè—æ—¥å¿—ï¼‰
import subprocess
import locale
locale.setlocale(locale.LC_COLLATE, 'zh_CN.UTF-8')  # æ ¹æ®ä½ çš„æ“ä½œç³»ç»Ÿè®¾ç½®åˆé€‚çš„locale

def natural_sort_key(s):
    """å°†å­—ç¬¦ä¸²sè½¬æ¢æˆä¸€ä¸ªç”¨äºè‡ªç„¶æ’åºçš„é”®"""
    return [int(text) if text.isdigit() else locale.strxfrm(text) for text in re.split('([0-9]+)', s)]

def list_zip_contents(zip_path):
    """ä½¿ç”¨7zåˆ—å‡ºå‹ç¼©åŒ…å†…çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶æŒ‰ç…§è‡ªç„¶é¡ºåºæ’åº"""
    try:
        result = subprocess.run(['7z', 'l', zip_path], capture_output=True, text=True, check=True)
        all_files = []
        image_files = []
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼å’Œå…³é”®è¯æ–‡ä»¶
        image_extensions = ('.png', '.jpg', '.jpeg', '.webp', '.avif', '.jxl')
        
        for line in result.stdout.splitlines():
            parts = line.split()
            if len(parts) >= 1:
                file_name = parts[-1]
                # è®°å½•æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬æ–‡ä»¶å¤¹è·¯å¾„ï¼‰
                all_files.append(file_name)
                # è®°å½•å›¾ç‰‡æ–‡ä»¶
                if any(file_name.lower().endswith(ext) for ext in image_extensions):
                    image_files.append(file_name)

        # ä½¿ç”¨è‡ªç„¶æ’åºè¿›è¡Œæ’åº
        sorted_image_files = sorted(image_files, key=natural_sort_key)
        return all_files, sorted_image_files

    except subprocess.CalledProcessError as e:
        logger.error(f"æ— æ³•å¤„ç†å‹ç¼©åŒ… {zip_path}: {e}")
        return [], []
    except Exception as e:
        logger.error(f"å¤„ç†å‹ç¼©åŒ…æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ {zip_path}: {e}")
        return [], []

# ç¤ºä¾‹è°ƒç”¨
# sorted_files = list_zip_contents('example.zip')
# print(sorted_files)
# è®¾ç½®localeä¸ºä¸­æ–‡ï¼Œé€šå¸¸åœ¨ç¨‹åºå¼€å§‹æ—¶è¿›è¡Œè®¾ç½®

# ä½¿ç”¨7zæå–éƒ¨åˆ†æ–‡ä»¶ï¼ˆéšè—æ—¥å¿—ã€å¼‚æ­¥å¤„ç†ï¼‰
def extract_files(zip_path, files_to_extract, output_dir):
    """ä½¿ç”¨7zæå–éƒ¨åˆ†æ–‡ä»¶"""
    try:
        os.makedirs(output_dir, exist_ok=True)
        subprocess.run(['7z', 'e', zip_path, '-o' + output_dir] + files_to_extract, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        # print(f"Extracted files: {files_to_extract}")
    except subprocess.CalledProcessError as e:
        logger.error(f"Error extracting files: {e}")

# ä½¿ç”¨7zç»Ÿä¸€åˆ é™¤å’Œæ›´æ–°å‹ç¼©åŒ…æ–‡ä»¶ï¼ˆéšè—æ—¥å¿—ï¼‰
def update_zip(zip_path, files_to_delete, files_to_add):
    """ä½¿ç”¨7zåˆ é™¤æ—§æ–‡ä»¶å¹¶æ›´æ–°æ–°æ–‡ä»¶"""
    try:
        # åˆ é™¤å‹ç¼©åŒ…ä¸­çš„åŸæ–‡ä»¶
        if files_to_delete:
            subprocess.run(['7z', 'd', zip_path] + files_to_delete, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            # print(f"Deleted from {zip_path}")

        # æ·»åŠ æ–°æ–‡ä»¶åˆ°å‹ç¼©åŒ…
        if files_to_add:
            subprocess.run(['7z', 'u', zip_path] + files_to_add, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            # print(f"Updated {zip_path}")

    except subprocess.CalledProcessError as e:
        logger.error(f"Error updating zip: {e}")
        pass


# def generate_unique_filename(file_name,uuid):
#     """ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å"""
#     name, ext = os.path.splitext(file_name)
#     return f"{name}_{uuid}{ext}"

def init_folder_stats():
    return {
        'total_files': 0,
        'processed_files': 0,
        'modified_files': 0,
        'skipped_files': 0,
        'errors': [],
        'start_time': datetime.datetime.now()
    }

def process_zip(zip_path, compare_images_hashes, processed_zips_set, processed_zips_file, 
               threshold, folder_stats, num_start=3, num_end=3, ignore_processed_zips=False, 
               use_tdel=True, use_trash=True):
    try:
        folder_stats['total_files'] += 1
        output_dir = os.path.join(os.path.dirname(zip_path), os.path.basename(zip_path) + '_temp')
        
        try:
            original_stat = os.stat(zip_path)
        except Exception as e:
            logger.error(f"æ— æ³•è®¿é—®å‹ç¼©åŒ… {zip_path}: {e}")
            folder_stats['skipped_files'] += 1
            folder_stats['processed_files'] += 1
            return

        logger.info(f"å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {zip_path}")
        
        # è·å–å‹ç¼©åŒ…å†…æ‰€æœ‰æ–‡ä»¶å’Œå›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
        all_files, img_files = list_zip_contents(zip_path)
        logger.info(f"å‹ç¼©åŒ…å†…æ–‡ä»¶æ€»æ•°: {len(all_files)}, å›¾ç‰‡æ–‡ä»¶æ•°: {len(img_files)}")
        
        # å®šä¹‰éœ€è¦åˆ é™¤çš„å…³é”®è¯
        keywords_to_delete = ['ç»…å£«çš„å¿«ä¹', 'æ‹›å‹Ÿ','æ±‰åŒ–ç»„']
        files_to_delete = []
        needs_modification = False
        save_uuid_needed = False
        
        # æ£€æŸ¥æ–‡ä»¶åä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        for file_path in all_files:
            for keyword in keywords_to_delete:
                if keyword in file_path:
                    files_to_delete.append(file_path)
                    logger.info(f"å‘ç°åŒ…å«å…³é”®è¯'{keyword}'çš„æ–‡ä»¶: {file_path}")
                    break
        
        # æ£€æŸ¥.tdelæ–‡ä»¶
        tdel_files = [f for f in all_files if f.endswith('.tdel')]
        if not use_tdel and tdel_files:
            files_to_delete.extend(tdel_files)
            logger.info(f"å‘ç°.tdelæ–‡ä»¶: {', '.join(tdel_files)}")

        # å¦‚æœæœ‰æ–‡ä»¶éœ€è¦åˆ é™¤ï¼Œæ‰§è¡Œåˆ é™¤æ“ä½œ
        if files_to_delete:
            try:
                logger.info(f"å‡†å¤‡åˆ é™¤ä»¥ä¸‹æ–‡ä»¶: {', '.join(files_to_delete)}")
                delete_command = ['7z', 'd', zip_path] + files_to_delete
                logger.debug(f"æ‰§è¡Œå‘½ä»¤: {' '.join(delete_command)}")
                
                result = subprocess.run(delete_command, 
                                      capture_output=True,
                                      text=True,
                                      check=True)
                
                logger.info(f"åˆ é™¤å‘½ä»¤è¾“å‡º: {result.stdout}")
                if result.stderr:
                    logger.warning(f"åˆ é™¤å‘½ä»¤é”™è¯¯è¾“å‡º: {result.stderr}")
                
                logger.info(f"å·²ä»å‹ç¼©åŒ…åˆ é™¤ {len(files_to_delete)} ä¸ªæ–‡ä»¶")
                needs_modification = True
                save_uuid_needed = True
            except subprocess.CalledProcessError as e:
                logger.error(f"åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                logger.error(f"é”™è¯¯è¾“å‡º: {e.stderr if hasattr(e, 'stderr') else 'æ— é”™è¯¯è¾“å‡º'}")

        # åªåœ¨éœ€è¦æ£€æŸ¥å·²å¤„ç†æ–‡ä»¶æ—¶æ‰åŠ è½½ UUID
        uuid = None
        if not ignore_processed_zips:
            uuid = load_yaml_uuid_from_archive(zip_path)
            if uuid and uuid in processed_zips_set:
                return

        # è·å–å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨ï¼ˆç°åœ¨ä½¿ç”¨ä¹‹å‰è·å–çš„ img_filesï¼‰
        tdel_files = [f for f in img_files if f.endswith('.tdel')]
        
        # å¦‚æœå‘ç° .tdel æ–‡ä»¶ä¸” use_tdel ä¸º Falseï¼Œåˆ™åˆ é™¤è¿™äº›æ–‡ä»¶
        if not use_tdel and tdel_files:
            logger.info(f"å‘ç° .tdel æ–‡ä»¶åœ¨å‹ç¼©åŒ…ä¸­: {zip_path}")
            needs_modification = True
            files_to_delete.extend(tdel_files)
            save_uuid_needed = True

        if not img_files:
            logger.warning(f"å‹ç¼©åŒ…ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {zip_path}")
            folder_stats['skipped_files'] += 1
            folder_stats['processed_files'] += 1
            return

        # ä¿®æ”¹æ–‡ä»¶é€‰æ‹©é€»è¾‘
        files_to_process = []
        total_images = len(img_files)
        
        if total_images <= (num_start + num_end):
            # å¦‚æœæ€»æ•°å°äºæˆ–ç­‰äºè¦å¤„ç†çš„æ€»æ•°é‡ï¼Œå¤„ç†æ‰€æœ‰å›¾ç‰‡
            files_to_process = img_files
            # logger.debug(f"å›¾ç‰‡æ€»æ•°({total_images})å°äºæˆ–ç­‰äºæŒ‡å®šå¤„ç†æ•°é‡({num_start}+{num_end})ï¼Œå¤„ç†æ‰€æœ‰å›¾ç‰‡")
        else:
            # åˆ†åˆ«è·å–å‰num_startå’Œånum_endå¼ å›¾ç‰‡
            files_to_process = img_files[:num_start]  # å‰é¢çš„å›¾ç‰‡
            files_to_process.extend(img_files[-num_end:])  # åé¢çš„å›¾ç‰‡
            # logger.debug(f"å¤„ç†å‰{num_start}å¼ å’Œå{num_end}å¼ å›¾ç‰‡ï¼Œæ€»å…±{len(files_to_process)}/{total_images}å¼ ")

        # æå–é€‰å®šçš„æ–‡ä»¶
        extract_files(zip_path, files_to_process, output_dir)

        files_to_delete = []
        files_to_add = []
        hash_pattern = re.compile(r"\[hash-(?P<hash>[0-9a-fA-F]+)\]")
        save_uuid_needed = False
        needs_modification = False  # æ–°å¢æ ‡å¿—ä½,ç”¨äºæ ‡è®°æ˜¯å¦éœ€è¦ä¿®æ”¹å‹ç¼©åŒ…

        # å¤„ç†æå–çš„æ–‡ä»¶
        for file_name in files_to_process:
            file_path = os.path.join(output_dir, file_name)
            
            if not os.path.exists(file_path):
                continue

            # æ·»åŠ å¤„ç†å¼€å§‹çš„æ—¥å¿—
            # logger.info(f"æ­£åœ¨å¤„ç†: {os.path.basename(zip_path)} -> {file_name}")

            hash_file_name_changed = False

            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦å·²åŒ…å«å“ˆå¸Œ
            match = hash_pattern.search(file_name)
            if match:
                img_hash = imagehash.hex_to_hash(match.group('hash'))
            else:
                with open(file_path, 'rb') as f:
                    img_bytes = f.read()
                img_hash = get_image_hash(img_bytes)

                # å¦‚æœå“ˆå¸Œè®¡ç®—å¤±è´¥ï¼Œåˆ™è·³è¿‡è¯¥å›¾ç‰‡
                if img_hash is None:
                    continue

                # æ›´æ–°æ–‡ä»¶åï¼Œæ·»åŠ å“ˆå¸Œ
                if file_name not in files_to_delete:
                    files_to_delete.append(file_name)
                name, ext = os.path.splitext(file_name)
                hash_file_name = f"{name}[hash-{img_hash}]{ext}"
                hash_file_path = os.path.join(output_dir, hash_file_name)
                os.rename(file_path, hash_file_path)
                file_name = hash_file_name
                file_path = hash_file_path
                hash_file_name_changed = True
            
            # æ£€æŸ¥å“ˆå¸Œæ˜¯å¦ä¸ºå…¨ç™½æˆ–å…¨é»‘
            if is_hash_all_white_or_black(img_hash):
                logger.info(f"æ£€æµ‹åˆ°å…¨ç™½/å…¨é»‘å›¾ç‰‡: {file_path}")
                needs_modification = True
                if use_tdel:
                    new_file_name = file_name + ".tdel"
                    logger.info(f"æ·»åŠ .tdelåç¼€: {file_path} -> {new_file_name}")
                    new_file_path = os.path.join(output_dir, new_file_name)
                    os.rename(file_path, new_file_path)
                    if file_name not in files_to_delete:
                        files_to_delete.append(file_name)
                    if new_file_path not in files_to_add:
                        files_to_add.append(new_file_path)
                else:
                    logger.info(f"å°†åˆ é™¤æ–‡ä»¶: {file_path}")
                    if file_name not in files_to_delete:
                        files_to_delete.append(file_name)
                save_uuid_needed = True
            
            elif any(are_images_similar(img_hash, compare_hash, threshold) for compare_hash in compare_images_hashes):
                logger.info(f"æ£€æµ‹åˆ°ç›¸ä¼¼å›¾ç‰‡: {file_path}")
                needs_modification = True
                if use_tdel:
                    new_file_name = file_name + ".tdel"
                    logger.info(f"æ·»åŠ .tdelåç¼€: {file_path} -> {new_file_name}")
                    new_file_path = os.path.join(output_dir, new_file_name)
                    os.rename(file_path, new_file_path)
                    if file_name not in files_to_delete:
                        files_to_delete.append(file_name)
                    if new_file_path not in files_to_add:
                        files_to_add.append(new_file_path)
                else:
                    logger.info(f"å°†åˆ é™¤æ–‡ä»¶: {file_path}")
                    if file_name not in files_to_delete:
                        files_to_delete.append(file_name)
                save_uuid_needed = True
            
            elif hash_file_name_changed:
                logger.info(f"æ›´æ–°æ–‡ä»¶å: {file_path} in {os.path.basename(zip_path)}")
                needs_modification = True  # æ–‡ä»¶åæ”¹å˜ä¹Ÿéœ€è¦æ›´æ–°å‹ç¼©åŒ…
                files_to_add.append(file_path)

        # åªæœ‰å½“éœ€è¦ä¿®æ”¹æ—¶æ‰æ›´æ–°å‹ç¼©åŒ…
        if needs_modification and (files_to_delete or files_to_add):
            update_zip(zip_path, files_to_delete, files_to_add)
            
            if save_uuid_needed:
                processed_zips_set.add(uuid)
                save_processed_zips_uuid(processed_zips_file, processed_zips_set)
                
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            folder_stats['modified_files'] += 1

        # åœ¨å®Œæˆå‹ç¼©åŒ…å¤„ç†æ—¶æ·»åŠ æ€»ç»“æ—¥å¿—
        if needs_modification:
            logger.info(f"å®Œæˆå‹ç¼©åŒ…å¤„ç†: {os.path.basename(zip_path)}")
            if files_to_delete:
                logger.info(f"åˆ é™¤çš„æ–‡ä»¶: {', '.join(files_to_delete)}")
            if files_to_add:
                logger.info(f"æ·»åŠ çš„æ–‡ä»¶: {', '.join([os.path.basename(f) for f in files_to_add])}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(output_dir)
        os.utime(zip_path, (original_stat.st_atime, original_stat.st_mtime))

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        folder_stats['processed_files'] += 1
        if files_to_delete or files_to_add:
            folder_stats['modified_files'] += 1
        if uuid and uuid in processed_zips_set:
            folder_stats['skipped_files'] += 1

    except Exception as e:
        error_msg = f"å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™ {zip_path}: {e}"
        logger.error(error_msg)
        folder_stats['errors'].append(error_msg)
        folder_stats['skipped_files'] += 1
        folder_stats['processed_files'] += 1

def print_folder_report(folder_path, stats):
    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¿®æ”¹æ“ä½œ
    if stats['modified_files'] == 0:
        return

    end_time = datetime.datetime.now()
    duration = end_time - stats['start_time']
    
    print(f"\n{'='*50}")
    print(f"æ–‡ä»¶å¤¹å¤„ç†æŠ¥å‘Š: {folder_path}")
    print(f"{'='*50}")
    print(f"å¼€å§‹æ—¶é—´: {stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"å¤„ç†æ—¶é•¿: {duration}")
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"- æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"- å·²å¤„ç†: {stats['processed_files']}")
    print(f"- å·²ä¿®æ”¹: {stats['modified_files']}")
    print(f"- å·²è·³è¿‡: {stats['skipped_files']}")
    print(f"- é”™è¯¯æ•°: {len(stats['errors'])}")
    
    if stats['errors']:
        print("\né”™è¯¯æ—¥å¿—:")
        for error in stats['errors']:
            print(f"- {error}")
    print(f"{'='*50}\n")

def process_all_zips(root_folder, compare_images_hashes, processed_zips_set, processed_zips_file, 
                     threshold, enable_processed_zips, exclude_keywords, max_workers=8, 
                     num_start=3, num_end=3, ignore_processed_zips=False, use_tdel=True, use_trash=True):
    folder_stats_dict = {}
    
    all_zip_files = []
    for foldername, _, filenames in os.walk(root_folder):
        if any(keyword in foldername for keyword in exclude_keywords):
            continue
        zip_files = [os.path.join(foldername, f) for f in filenames 
                    if f.lower().endswith(('.zip', '.cbz'))]
        all_zip_files.extend(zip_files)

    if not all_zip_files:
        logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„ZIPæ–‡ä»¶")
        return

    # åˆ›å»ºæ€»è¿›åº¦æ¡
    total_pbar = tqdm(total=len(all_zip_files), 
                     desc="æ€»è¿›åº¦", 
                     position=0, 
                     leave=True,
                     ncols=100,
                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for zip_path in all_zip_files:
            foldername = os.path.dirname(zip_path)
            if foldername not in folder_stats_dict:
                folder_stats_dict[foldername] = init_folder_stats()
            
            future = executor.submit(
                process_zip, 
                zip_path, 
                compare_images_hashes, 
                processed_zips_set,
                processed_zips_file, 
                threshold, 
                folder_stats_dict[foldername],
                num_start, 
                num_end,
                ignore_processed_zips,
                use_tdel,
                use_trash
            )
            futures[future] = zip_path

        # å½“å‰å¤„ç†æ–‡ä»¶çš„çŠ¶æ€è¡Œ
        current_status = tqdm(total=0, 
                            desc="å½“å‰æ–‡ä»¶", 
                            position=1, 
                            leave=False,
                            bar_format='{desc}: {postfix}')

        for future in as_completed(futures):
            zip_path = futures[future]
            try:
                future.result()
                logger.info(f"å¤„ç†å®Œæˆ: {os.path.basename(zip_path)}")
            except Exception as e:
                logger.error(f"è·³è¿‡å¤„ç†å‡ºé”™çš„å‹ç¼©åŒ… {os.path.basename(zip_path)}: {e}")
            finally:
                total_pbar.update(1)
                current_status.set_postfix_str(os.path.basename(zip_path))

        current_status.close()
    total_pbar.close()

    # æ‰“å°æ‰€æœ‰æ–‡ä»¶å¤¹çš„æŠ¥å‘Š
    for foldername, stats in folder_stats_dict.items():
        print_folder_report(foldername, stats)

def record_folder_timestamps(target_directory):
    """è®°å½•target_directoryä¸‹æ‰€æœ‰æ–‡ä»¶å¤¹çš„æ—¶é—´æˆ³ã€‚"""
    folder_timestamps = {}
    for root, dirs, files in os.walk(target_directory):
        for dir in dirs:
            folder_path = os.path.join(root, dir)
            folder_stat = os.stat(folder_path)
            folder_timestamps[folder_path] = (folder_stat.st_atime, folder_stat.st_mtime)
            
        
        # # è®°å½•æ–‡ä»¶çš„æ—¶é—´æˆ³
        # for file in files:
        #     file_path = os.path.join(root, file)
        #     file_stat = os.stat(file_path)
        #     folder_timestamps[file_path] = (file_stat.st_atime, file_stat.st_mtime)
    
    return folder_timestamps

def restore_folder_timestamps(folder_timestamps):
    """æ¢å¤ä¹‹å‰è®°å½•çš„æ–‡ä»¶å¤¹æ—¶é—´æˆ³ã€‚"""
    for folder_path, (atime, mtime) in folder_timestamps.items():
        if os.path.exists(folder_path):
            os.utime(folder_path, (atime, mtime))

# æ–‡ä»¶æ—¶é—´æˆ³è¦è€ƒè™‘é‡å‘½åæ–‡ä»¶å¯¼è‡´çš„å˜åŒ–
# def restore_folder_and_file_timestamps(timestamps):
#     """æ¢å¤ä¹‹å‰è®°å½•çš„æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶çš„æ—¶é—´æˆ³ã€‚"""
#     for path, (atime, mtime) in timestamps.items():
#         if os.path.exists(path):
#             os.utime(path, (atime, mtime))

def get_filter_date():
    """è·å–ç”¨æˆ·è¾“å…¥çš„è¿‡æ»¤æ—¥æœŸ"""
    date_str = input("è¯·è¾“å…¥æ—¥æœŸ (yyyy-mm-dd): ")
    return datetime.datetime.strptime(date_str, "%Y-%m-%d")

def batch_rename_files():
    """æ‰¹é‡é‡å‘½åæ–‡ä»¶çš„ç‹¬ç«‹å‡½æ•°"""
    rename_info_file = 'E:\\1EHV\\[00å»å›¾]\\files_to_rename.json'
    if not os.path.exists(rename_info_file):
        return
    
    try:
        with open(rename_info_file, 'r', encoding='utf-8') as f:
            files_to_rename = json.load(f)
        
        success_count = 0
        for old_path, new_path in files_to_rename:
            try:
                if os.path.exists(old_path) and not os.path.exists(new_path):
                    os.rename(old_path, new_path)
                    success_count += 1
            except Exception as e:
                logger.error(f"é‡å‘½åæ–‡ä»¶æ—¶å‡ºé”™ {old_path}: {e}")
        
        logger.info(f"æˆåŠŸé‡å‘½å {success_count}/{len(files_to_rename)} ä¸ªæ–‡ä»¶")
        
        # é‡å‘½åå®Œæˆååˆ é™¤è®°å½•æ–‡ä»¶
        if success_count == len(files_to_rename):
            os.remove(rename_info_file)
            
    except Exception as e:
        logger.error(f"è¯»å–é‡å‘½åä¿¡æ¯æ—¶å‡ºé”™: {e}")

def is_hash_all_white_or_black(img_hash):
    """æ£€æŸ¥å“ˆå¸Œæ˜¯å¦å¯¹åº”å…¨ç™½æˆ–å…¨é»‘çš„å›¾ç‰‡"""
    # é¢„å®šä¹‰çš„å…¨ç™½æˆ–å…¨é»‘å“ˆå¸Œå€¼åˆ—è¡¨
    hash_list = ['ffffffffffffffff', '0000000000000000', '0000000000000000', '0000000000000001']
    return str(img_hash) in hash_list

def move_to_trash(original_path, zip_folder, use_trash=True):
    """å°†æ–‡ä»¶ç§»åŠ¨åˆ°ç»Ÿä¸€çš„ .trash æ–‡ä»¶å¤¹ï¼Œä¿æŒåŸæœ‰ç›®å½•ç»“æ„"""
    if not use_trash:
        logger.info(f"use_trashä¸ºFalseï¼Œç›´æ¥åˆ é™¤æ–‡ä»¶: {original_path}")
        os.remove(original_path)
        return

    # åˆ›å»ºç»Ÿä¸€çš„ .trash ç›®å½•
    trash_base = os.path.join(zip_folder, '.trash')
    logger.info(f"åˆ›å»ºå›æ”¶ç«™ç›®å½•: {trash_base}")
    
    try:
        # è·å–ç›¸å¯¹äº zip_folder çš„è·¯å¾„
        rel_path = os.path.relpath(original_path, zip_folder)
        logger.info(f"è®¡ç®—ç›¸å¯¹è·¯å¾„: {rel_path}")
    except ValueError as e:
        # å¦‚æœæ–‡ä»¶ä¸åœ¨ zip_folder ä¸‹ï¼Œä½¿ç”¨å®Œæ•´è·¯å¾„ç»“æ„
        logger.warning(f"è®¡ç®—ç›¸å¯¹è·¯å¾„å¤±è´¥: {e}")
        rel_path = original_path.lstrip(os.path.sep)
        logger.info(f"ä½¿ç”¨å®Œæ•´è·¯å¾„: {rel_path}")
    
    # æ„å»ºç›®æ ‡è·¯å¾„
    trash_path = os.path.join(trash_base, rel_path)
    logger.info(f"æ„å»ºç›®æ ‡è·¯å¾„: {trash_path}")
    
    try:
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(trash_path), exist_ok=True)
        logger.info(f"åˆ›å»ºç›®æ ‡ç›®å½•: {os.path.dirname(trash_path)}")
        
        # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
        if os.path.exists(trash_path):
            name, ext = os.path.splitext(trash_path)
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            trash_path = f"{name}_{timestamp}{ext}"
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³: {trash_path}")
        
        # ç§»åŠ¨æ–‡ä»¶
        shutil.move(original_path, trash_path)
        logger.info(f"æˆåŠŸç§»åŠ¨æ–‡ä»¶åˆ°å›æ”¶ç«™: {trash_path}")
    except Exception as e:
        logger.error(f"ç§»åŠ¨æ–‡ä»¶åˆ°å›æ”¶ç«™å¤±è´¥: {e}")
        raise

def process_single_image(image_path, compare_images_hashes, threshold, folder_stats, zip_folder, use_tdel=True, use_trash=True):
    """å¤„ç†å•ä¸ªå›¾ç‰‡æ–‡ä»¶"""
    try:
        folder_stats['total_files'] += 1
        
        # æ£€æŸ¥æ–‡ä»¶åä¸­æ˜¯å¦å·²åŒ…å«å“ˆå¸Œå€¼
        hash_pattern = re.compile(r"\[hash-(?P<hash>[0-9a-fA-F]+)\]")
        base_name = os.path.basename(image_path)
        match = hash_pattern.search(base_name)
        
        if match:
            img_hash = imagehash.hex_to_hash(match.group('hash'))
        else:
            with open(image_path, 'rb') as f:
                img_bytes = f.read()
            img_hash = get_image_hash(img_bytes)
            
            if img_hash is None:
                logger.error(f"æ— æ³•å¤„ç†å›¾ç‰‡: {image_path}")
                folder_stats['errors'].append(f"æ— æ³•å¤„ç†å›¾ç‰‡: {image_path}")
                folder_stats['skipped_files'] += 1
                return

            # æ›´æ–°æ–‡ä»¶åï¼Œæ·»åŠ å“ˆå¸Œ
            name, ext = os.path.splitext(base_name)
            new_name = f"{name}[hash-{img_hash}]{ext}"
            new_path = os.path.join(os.path.dirname(image_path), new_name)
            
            # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ è®¡æ•°åç¼€
            counter = 1
            while os.path.exists(new_path):
                new_name = f"{name}[hash-{img_hash}]_{counter}{ext}"
                new_path = os.path.join(os.path.dirname(image_path), new_name)
                counter += 1
                
            os.rename(image_path, new_path)
            image_path = new_path
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºå…¨ç™½/å…¨é»‘å›¾ç‰‡
        if is_hash_all_white_or_black(img_hash):
            logger.info(f"æ£€æµ‹åˆ°å…¨ç™½/å…¨é»‘å›¾ç‰‡: {image_path}")
            if use_tdel:
                new_path = image_path + ".tdel"
                os.rename(image_path, new_path)
            else:
                move_to_trash(image_path, zip_folder, use_trash)
            folder_stats['modified_files'] += 1
            
        # æ£€æŸ¥æ˜¯å¦ä¸æ¯”è¾ƒé›†ä¸­çš„å›¾ç‰‡ç›¸ä¼¼
        elif any(are_images_similar(img_hash, compare_hash, threshold) for compare_hash in compare_images_hashes):
            logger.info(f"æ£€æµ‹åˆ°ç›¸ä¼¼å›¾ç‰‡: {image_path}")
            if use_tdel:
                new_path = image_path + ".tdel"
                os.rename(image_path, new_path)
            else:
                move_to_trash(image_path, zip_folder, use_trash)
            folder_stats['modified_files'] += 1
            
        folder_stats['processed_files'] += 1
        
    except Exception as e:
        error_msg = f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™ {image_path}: {e}"
        logger.error(error_msg)
        folder_stats['errors'].append(error_msg)
        folder_stats['skipped_files'] += 1

def process_directory(directory_path, compare_images_hashes, threshold, zip_folder, max_workers=8, use_tdel=True, use_trash=True):
    """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶"""
    folder_stats = init_folder_stats()
    image_extensions = ('.png', '.jpg', '.jpeg', '.webp', '.avif', '.jxl')
    
    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ï¼Œæ’é™¤ .trash ç›®å½•
    image_files = []
    for root, _, files in os.walk(directory_path):
        # è·³è¿‡ .trash ç›®å½•
        if '.trash' in root:
            continue
            
        for file in files:
            if file.lower().endswith(image_extensions):
                image_files.append(os.path.join(root, file))
    
    if not image_files:
        logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„å›¾ç‰‡æ–‡ä»¶")
        return
    
    # åˆ›å»ºè¿›åº¦æ¡
    with tqdm(total=len(image_files), desc="å¤„ç†å›¾ç‰‡", ncols=100) as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for image_path in image_files:
                future = executor.submit(
                    process_single_image,
                    image_path,
                    compare_images_hashes,
                    threshold,
                    folder_stats,
                    zip_folder,
                    use_tdel,
                    use_trash
                )
                futures.append(future)
            
            for future in as_completed(futures):
                try:
                    future.result()
                    pbar.update(1)
                except Exception as e:
                    logger.error(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
    
    print_folder_report(directory_path, folder_stats)

def get_paths_from_clipboard():
    """ä»å‰ªè´´æ¿è·å–è·¯å¾„åˆ—è¡¨"""
    try:
        import pyperclip
        text = pyperclip.paste()
        if text:
            return [path.strip().strip('"') for path in text.splitlines() if path.strip()]
    except:
        return []
    return []

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å›¾ç‰‡å‹ç¼©åŒ…å»é‡å·¥å…·')
    parser.add_argument('--clipboard', '-c', 
                       action='store_true',
                       help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
    return parser.parse_args()

class ProcessTypeScreen(Screen):
    """å¤„ç†ç±»å‹é€‰æ‹©ç•Œé¢"""
    
    BINDINGS = [
        Binding("q", "quit", "é€€å‡º", show=True),
        Binding("enter", "submit", "ç¡®è®¤", show=True),
        Binding("escape", "quit", "å–æ¶ˆ", show=True),
    ]

    def compose(self) -> ComposeResult:
        yield Header()
        with Container(id="dialog-background"):
            with Container(id="dialog"):
                yield Static("EHV å›¾ç‰‡å¤„ç†å·¥å…·", id="title", classes="text")
                yield Static("è¯·é€‰æ‹©å¤„ç†æ¨¡å¼", classes="text")
                yield RadioSet(
                    RadioButton("ğŸ—ƒï¸  å‹ç¼©åŒ…å¤„ç†", value=True),
                    RadioButton("ğŸ–¼ï¸  å›¾ç‰‡æ–‡ä»¶å¤¹å¤„ç†"),
                    RadioButton("ğŸ”„  ä¸¤è€…éƒ½å¤„ç†"),
                    id="process_type"
                )
                with Horizontal(classes="button-container"):
                    yield Button("ç¡®å®š", variant="primary", id="confirm")
                    yield Button("å–æ¶ˆ", variant="error", id="cancel")
        yield Footer()

    def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "confirm":
            self._confirm_selection()
        elif event.button.id == "cancel":
            self.app.exit(None)
            
    def _confirm_selection(self) -> None:
        radio_set = self.query_one("#process_type")
        selected_index = radio_set.pressed_index
        self.app.selected_type = str(selected_index + 1)
        self.app.exit(self.app.selected_type)
            
    def action_submit(self) -> None:
        self._confirm_selection()

class ProcessTypeSelector(App):
    """å¤„ç†ç±»å‹é€‰æ‹©åº”ç”¨"""
    
    THEME = "tokyo-night"
    
    CSS = """
    Screen {
        align: center middle;
    }

    #dialog-background {
        width: 100%;
        height: 100%;
        align: center middle;
    }

    #dialog {
        background: $surface;
        padding: 1 2;
        width: 60;
        height: auto;
        border: tall $primary;
        align: center middle;
    }

    #title {
        text-style: bold;
        margin-bottom: 1;
    }

    .text {
        width: 100%;
        content-align: center middle;
    }

    RadioSet {
        width: 100%;
        margin: 1 0;
    }

    .button-container {
        width: 100%;
        height: auto;
        align: center middle;
        margin-top: 1;
    }

    Button {
        min-width: 16;
        margin: 0 1;
    }

    #confirm {
        border: tall $success;
    }

    #confirm:hover {
        background: $success;
    }

    #cancel {
        border: tall $error;
    }

    #cancel:hover {
        background: $error;
    }
    """
    
    def __init__(self, title: str = ""):
        super().__init__()
        self.title = title
        self.selected_type = None

    def on_mount(self) -> None:
        self.push_screen(ProcessTypeScreen())

def select_process_type(zip_folder):
    """ä½¿ç”¨ TUI ç•Œé¢é€‰æ‹©å¤„ç†ç±»å‹"""
    app = ProcessTypeSelector(f"å¤„ç†ç›®å½•: {zip_folder}")
    process_type = app.run()
    return process_type

def main():
    args = parse_arguments()
    directories = []
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨å‰ªè´´æ¿
    if args.clipboard:
        directories = get_paths_from_clipboard()
        process_type = '3'  # å‰ªè´´æ¿æ¨¡å¼ä¸‹é»˜è®¤é€‰æ‹©3
    
    # å¦‚æœå‰ªè´´æ¿ä¸ºç©ºæˆ–æœªå¯ç”¨å‰ªè´´æ¿ï¼Œåˆ™ä»ç”¨æˆ·è¾“å…¥è¯»å–
    if not directories:
        print("è¯·è¾“å…¥è¦å¤„ç†çš„æ–‡ä»¶å¤¹æˆ–å‹ç¼©åŒ…å®Œæ•´è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªè·¯å¾„ï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ:")
        while True:
            directory = input().strip().strip('"')
            if not directory:
                break
            directories.append(directory)

    if not directories:
        print("æœªæä¾›ä»»ä½•è·¯å¾„ï¼Œç¨‹åºé€€å‡º")
        return

    # å¤„ç†æ¯ä¸ªè¾“å…¥çš„è·¯å¾„
    for zip_folder in directories:
        older_timestamps = record_folder_timestamps(zip_folder)
        compare_folder = 'E:\\1EHV\\[00å»å›¾]'
        processed_zips_file = 'E:\\1EHV\\[00å»å›¾]\\processed_zips_uuid.yaml'
        hash_file = 'E:\\1EHV\\[00å»å›¾]\\image_hashes.json'
        threshold = 12
        enable_processed_zips = True
        exclude_keywords = ["ç¾å°‘å¥³ä¸‡è¯é¡", "00å»å›¾", "å›¾é›†","00å»å›¾","fanbox","02COS","02æ‚"]
        max_workers = 14
        update_hashes = True
        ignore_processed_zips = True
        num_start = 2
        num_end = 3
        use_tdel = False  # ä¸ä½¿ç”¨.tdelåç¼€
        use_trash = True  # ä½¿ç”¨å›æ”¶ç«™åŠŸèƒ½
        
        logger.info(f"å¤„ç†å‚æ•°è®¾ç½®: use_tdel={use_tdel}, use_trash={use_trash}")

        compare_images_hashes = load_hashes(hash_file)
        if update_hashes:
            save_hashes(hash_file, compare_images_hashes)

        batch_rename_files()

        processed_zips_set = load_processed_zips_uuid(processed_zips_file) if enable_processed_zips else {}

        if not args.clipboard:  # éå‰ªè´´æ¿æ¨¡å¼æ‰æ˜¾ç¤º TUI ç•Œé¢
            process_type = select_process_type(zip_folder)
            if process_type is None:  # ç”¨æˆ·å–æ¶ˆ
                continue
        
        if process_type in ('1', '3'):
            process_all_zips(zip_folder, compare_images_hashes, processed_zips_set, processed_zips_file,
                            threshold, enable_processed_zips, exclude_keywords, max_workers,
                            num_start, num_end, ignore_processed_zips, use_tdel, use_trash)
        
        if process_type in ('2', '3'):
            process_directory(zip_folder, compare_images_hashes, threshold, zip_folder, max_workers, use_tdel, use_trash)
        
        restore_folder_timestamps(older_timestamps)

if __name__ == "__main__":
    main()