from pathlib import Path
import sys
import os
import json
import logging
from typing import List, Dict, Set, Tuple
import time
import subprocess
import argparse
import pyperclip
from nodes.config.import_bundles import *
from nodes.record.logger_config import setup_logger
from nodes.tui.mode_manager import create_mode_manager
from nodes.file_ops.backup_handler import BackupHandler
from nodes.file_ops.archive_handler import ArchiveHandler
from nodes.pics.image_filter import ImageFilter
from nodes.io.input_handler import InputHandler
from nodes.io.config_handler import ConfigHandler
from nodes.io.path_handler import PathHandler, ExtractMode
import platform
import stat
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import multiprocessing

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¸¸é‡
SUPPORTED_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.avif', '.heic', '.heif', '.jxl'}

config = {
    'script_name': 'recruit_cover_filter',
    'console_enabled': False,
}
logger, config_info = setup_logger(config)
DEBUG_MODE = False

TEXTUAL_LAYOUT = {
    "cur_stats": {
        "ratio": 1,
        "title": "ðŸ“Š æ€»ä½“è¿›åº¦",
        "style": "lightyellow"
    },
    "cur_progress": {
        "ratio": 1,
        "title": "ðŸ”„ å½“å‰è¿›åº¦",
        "style": "lightcyan"
    },
    "file_ops": {
        "ratio": 2,
        "title": "ðŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightpink"
    },
    "ocr_results": {
        "ratio": 2,
        "title": "ðŸ“ OCRç»“æžœ",
        "style": "lightgreen"
    },
    "update_log": {
        "ratio": 1,
        "title": "ðŸ”§ ç³»ç»Ÿæ¶ˆæ¯",
        "style": "lightwhite"
    }
}

def initialize_textual_logger(layout: dict, log_file: str) -> None:
    """
    åˆå§‹åŒ–æ—¥å¿—å¸ƒå±€
    
    Args:
        layout: å¸ƒå±€é…ç½®å­—å…¸
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    try:
        TextualLoggerManager.set_layout(layout, config_info['log_file'])
        logger.info("[#update_log]âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}") 

class RecruitCoverFilter:
    """å°é¢å›¾ç‰‡è¿‡æ»¤å™¨"""
    
    def __init__(self, hash_file: str = None, hamming_threshold: int = 16, watermark_keywords: List[str] = None, max_workers: int = None):
        """åˆå§‹åŒ–è¿‡æ»¤å™¨"""
        self.image_filter = ImageFilter(hash_file, hamming_threshold)
        self.watermark_keywords = watermark_keywords
        self.max_workers = max_workers or multiprocessing.cpu_count()
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        initialize_textual_logger(TEXTUAL_LAYOUT, config_info['log_file'])
        
    def prepare_hash_file(self, recruit_folder: str, workers: int = 16, force_update: bool = False) -> str:
        """
        å‡†å¤‡å“ˆå¸Œæ–‡ä»¶
        
        Args:
            recruit_folder: æ‹›å‹Ÿæ–‡ä»¶å¤¹è·¯å¾„
            workers: å·¥ä½œçº¿ç¨‹æ•°
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            
        Returns:
            str: å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›žNone
        """
        try:
            from nodes.pics.hash_process_config import process_artist_folder
            hash_file = process_artist_folder(recruit_folder, workers, force_update)
            if hash_file:
                logger.info(f"[#update_log]âœ… æˆåŠŸç”Ÿæˆå“ˆå¸Œæ–‡ä»¶: {hash_file}")
                self.image_filter.hash_file = hash_file
                self.image_filter.hash_cache = self.image_filter._load_hash_file()
                return hash_file
            else:
                logger.error("[#update_log]âŒ ç”Ÿæˆå“ˆå¸Œæ–‡ä»¶å¤±è´¥")
                return None
        except Exception as e:
            logger.error(f"[#update_log]âŒ å‡†å¤‡å“ˆå¸Œæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

    def _robust_cleanup(self, temp_dir: str) -> None:
        """æ›´å¥å£®çš„æ–‡ä»¶æ¸…ç†æ–¹æ³•ï¼Œå¤„ç†æ–‡ä»¶è¢«å ç”¨çš„æƒ…å†µ"""
        if not os.path.exists(temp_dir):
            return

        def on_rm_error(func, path, exc_info):
            try:
                os.chmod(path, stat.S_IWRITE)
                os.unlink(path)
                logger.info(f"[#file_ops]æˆåŠŸåˆ é™¤ {path}")
            except Exception as e:
                logger.warning(f"[#file_ops]æ— æ³•åˆ é™¤ {path}: {e}")

        try:
            # å°è¯•æ ‡å‡†åˆ é™¤
            shutil.rmtree(temp_dir, onerror=on_rm_error)
        except Exception as e:
            logger.warning(f"[#file_ops]æ ‡å‡†åˆ é™¤å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åˆ é™¤: {temp_dir}")
            try:
                # ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤å¼ºåˆ¶åˆ é™¤ï¼ˆWindowsï¼‰
                if platform.system() == 'Windows':
                    subprocess.run(f'rmdir /s /q "{temp_dir}"', shell=True, check=True)
                else:  # Linux/MacOS
                    subprocess.run(f'rm -rf "{temp_dir}"', shell=True, check=True)
            except subprocess.CalledProcessError as e:
                logger.error(f"[#update_log]å¼ºåˆ¶åˆ é™¤å¤±è´¥: {temp_dir}")
                raise

    def process_archive(self, zip_path: str, extract_mode: str = ExtractMode.ALL, extract_params: dict = None, is_dehash_mode: bool = False) -> bool:
        """å¤„ç†å•ä¸ªåŽ‹ç¼©åŒ…"""
        logger.info(f"[#file_ops]å¼€å§‹å¤„ç†åŽ‹ç¼©åŒ…: {zip_path}")
        
        # åˆ—å‡ºåŽ‹ç¼©åŒ…å†…å®¹å¹¶é¢„å…ˆè¿‡æ»¤å›¾ç‰‡æ–‡ä»¶
        files = [f for f in ArchiveHandler.list_archive_contents(zip_path)
                if PathHandler.get_file_extension(f).lower() in SUPPORTED_EXTENSIONS]
        
        if not files:
            logger.info("[#file_ops]æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return False
            
        # èŽ·å–è¦è§£åŽ‹çš„æ–‡ä»¶ç´¢å¼•
        extract_params = extract_params or {}
        
        # åŽ»æ±‰åŒ–æ¨¡å¼ç‰¹æ®Šå¤„ç†ï¼šåˆå¹¶å‰Nå¼ å’ŒåŽNå¼ çš„ç´¢å¼•
        if is_dehash_mode:
            front_n = extract_params.get('front_n', 3)  # é»˜è®¤å‰3å¼ 
            back_n = extract_params.get('back_n', 5)    # é»˜è®¤åŽ5å¼ 
            
            # ç›´æŽ¥è®¡ç®—ç´¢å¼•ï¼Œé¿å…å¤šæ¬¡åˆ—è¡¨æ“ä½œ
            total_files = len(files)
            front_indices = range(min(front_n, total_files))
            back_indices = range(max(0, total_files - back_n), total_files)
            selected_indices = sorted(set(front_indices) | set(back_indices))
        else:
            selected_indices = ExtractMode.get_selected_indices(extract_mode, len(files), extract_params)
            
        if not selected_indices:
            logger.error("[#file_ops]æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶è¿›è¡Œè§£åŽ‹")
            return False
            
        # ç”Ÿæˆè§£åŽ‹ç›®å½•åç§°
        zip_name = os.path.splitext(os.path.basename(zip_path))[0]
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        extract_dir = os.path.join(os.path.dirname(zip_path), f"temp_{zip_name}_{timestamp}")
            
        # è§£åŽ‹é€‰å®šæ–‡ä»¶
        selected_files = [files[i] for i in selected_indices]
        success, extract_dir = ArchiveHandler.extract_files(zip_path, selected_files, extract_dir)
        if not success:
            return False
            
        try:
            # èŽ·å–è§£åŽ‹åŽçš„å›¾ç‰‡æ–‡ä»¶ï¼ˆä½¿ç”¨åˆ—è¡¨æŽ¨å¯¼å¼ä¼˜åŒ–ï¼‰
            image_files = [
                PathHandler.join_paths(root, file)
                for root, _, files in os.walk(extract_dir)
                for file in files
                if PathHandler.get_file_extension(file).lower() in SUPPORTED_EXTENSIONS
            ]
                        
            # å¤„ç†å›¾ç‰‡
            to_delete, removal_reasons = self.image_filter.process_images(
                image_files,
                enable_duplicate_filter=True,   # å¯ç”¨é‡å¤å›¾ç‰‡è¿‡æ»¤
                duplicate_filter_mode='hash' if self.image_filter.hash_file else 'watermark',  # å¦‚æžœæœ‰å“ˆå¸Œæ–‡ä»¶åˆ™ä½¿ç”¨å“ˆå¸Œæ¨¡å¼
                watermark_keywords=None if is_dehash_mode else self.watermark_keywords  # åŽ»æ±‰åŒ–æ¨¡å¼ä¸å¯ç”¨æ°´å°æ£€æµ‹
            )
            
            if not to_delete:
                logger.info("[#file_ops]æ²¡æœ‰éœ€è¦åˆ é™¤çš„å›¾ç‰‡")
                self._robust_cleanup(extract_dir)
                return False
                
            # å¤‡ä»½è¦åˆ é™¤çš„æ–‡ä»¶
            backup_results = BackupHandler.backup_removed_files(zip_path, to_delete, removal_reasons)
            
            # ä»ŽåŽ‹ç¼©åŒ…ä¸­åˆ é™¤æ–‡ä»¶ï¼ˆä½¿ç”¨åˆ—è¡¨æŽ¨å¯¼å¼ä¼˜åŒ–ï¼‰
            files_to_delete = [os.path.relpath(file_path, extract_dir) for file_path in to_delete]
                
            # ä½¿ç”¨7zåˆ é™¤æ–‡ä»¶
            delete_list_file = os.path.join(extract_dir, '@delete.txt')
            with open(delete_list_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(files_to_delete))
                    
            # åœ¨æ‰§è¡Œåˆ é™¤æ“ä½œå‰å¤‡ä»½åŽŸå§‹åŽ‹ç¼©åŒ…
            backup_success, backup_path = BackupHandler.backup_source_file(zip_path)
            if backup_success:
                logger.info(f"[#file_ops]âœ… æºæ–‡ä»¶å¤‡ä»½æˆåŠŸ: {backup_path}")
            else:
                logger.warning(f"[#file_ops]âš ï¸ æºæ–‡ä»¶å¤‡ä»½å¤±è´¥: {backup_path}")

            # ä½¿ç”¨7zåˆ é™¤æ–‡ä»¶
            cmd = ['7z', 'd', zip_path, f'@{delete_list_file}']
            result = subprocess.run(cmd, capture_output=True, text=True)
            os.remove(delete_list_file)
            
            if result.returncode != 0:
                logger.error(f"[#file_ops]ä»ŽåŽ‹ç¼©åŒ…åˆ é™¤æ–‡ä»¶å¤±è´¥: {result.stderr}")
                self._robust_cleanup(extract_dir)
                return False
                
            logger.info(f"[#file_ops]æˆåŠŸå¤„ç†åŽ‹ç¼©åŒ…: {zip_path}")
            self._robust_cleanup(extract_dir)
            return True
            
        except Exception as e:
            logger.error(f"[#file_ops]å¤„ç†åŽ‹ç¼©åŒ…å¤±è´¥ {zip_path}: {e}")
            self._robust_cleanup(extract_dir)
            return False

class Application:
    """åº”ç”¨ç¨‹åºç±»"""
    
    def __init__(self, max_workers: int = None):
        """åˆå§‹åŒ–åº”ç”¨ç¨‹åº
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        """
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.archive_queue = Queue()
        
    def _process_single_archive(self, args):
        """å¤„ç†å•ä¸ªåŽ‹ç¼©åŒ…çš„åŒ…è£…å‡½æ•°"""
        zip_path, filter_instance, extract_params, is_dehash_mode = args
        try:
            return filter_instance.process_archive(zip_path, extract_params=extract_params, is_dehash_mode=is_dehash_mode)
        except Exception as e:
            logger.error(f"[#update_log]å¤„ç†åŽ‹ç¼©åŒ…å¤±è´¥ {zip_path}: {e}")
            return False
    
    def process_directory(self, directory: str, filter_instance: RecruitCoverFilter, is_dehash_mode: bool = False, extract_params: dict = None):
        """å¤„ç†ç›®å½•"""
        try:
            # å®šä¹‰é»‘åå•å…³é”®è¯
            blacklist_keywords = ["ç”»é›†", "CG", "å›¾é›†"]
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„åŽ‹ç¼©åŒ…
            archives_to_process = []
            
            if os.path.isfile(directory):
                if directory.lower().endswith('.zip'):
                    archives_to_process.append(directory)
            else:
                for root, _, files in os.walk(directory):
                    # æ£€æŸ¥å½“å‰ç›®å½•è·¯å¾„æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
                    root_lower = root.lower()
                    if any(kw in root_lower for kw in blacklist_keywords):
                        logger.info(f"[#file_ops]è·³è¿‡é»‘åå•ç›®å½•: {root}")
                        continue
                    
                    for file in files:
                        if file.lower().endswith('.zip'):
                            zip_path = os.path.join(root, file)
                            archives_to_process.append(zip_path)
            
            if not archives_to_process:
                return
                
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†åŽ‹ç¼©åŒ…
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
                future_to_archive = {
                    executor.submit(
                        self._process_single_archive, 
                        (archive, filter_instance, extract_params, is_dehash_mode)
                    ): archive for archive in archives_to_process
                }
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for future in as_completed(future_to_archive):
                    archive = future_to_archive[future]
                    try:
                        success = future.result()
                        if success:
                            logger.info(f"[#file_ops]âœ… æˆåŠŸå¤„ç†: {os.path.basename(archive)}")
                        else:
                            logger.warning(f"[#file_ops]âš ï¸ å¤„ç†å¤±è´¥: {os.path.basename(archive)}")
                    except Exception as e:
                        logger.error(f"[#file_ops]å¤„ç†å‡ºé”™ {os.path.basename(archive)}: {e}")
                        
        except Exception as e:
            logger.error(f"[#update_log]å¤„ç†ç›®å½•å¤±è´¥ {directory}: {e}")

def setup_cli_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æžå™¨"""
    parser = argparse.ArgumentParser(description='æ‹›å‹Ÿå°é¢å›¾ç‰‡è¿‡æ»¤å·¥å…·')
    parser.add_argument('--debug', '-d', action='store_true',
                      help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--hash-file', '-hf', type=str,
                      help='å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®ï¼‰')
    parser.add_argument('--hamming-threshold', '-ht', type=int, default=16,
                      help='æ±‰æ˜Žè·ç¦»é˜ˆå€¼ (é»˜è®¤: 16)')
    parser.add_argument('--clipboard', '-c', action='store_true',
                      help='ä»Žå‰ªè´´æ¿è¯»å–è·¯å¾„')
    parser.add_argument('--watermark-keywords', '-wk', nargs='*',
                      help='æ°´å°å…³é”®è¯åˆ—è¡¨ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤åˆ—è¡¨')
    parser.add_argument('--duplicate-filter-mode', '-dfm', type=str,
                      choices=['quality', 'watermark', 'hash'],
                      default='quality', help='é‡å¤è¿‡æ»¤æ¨¡å¼ (é»˜è®¤: quality)')
    parser.add_argument('--extract-mode', '-em', type=str, 
                      choices=[ExtractMode.ALL, ExtractMode.FIRST_N, ExtractMode.LAST_N, ExtractMode.RANGE],
                      default=ExtractMode.ALL, help='è§£åŽ‹æ¨¡å¼ (é»˜è®¤: all)')
    parser.add_argument('--extract-n', '-en', type=int,
                      help='è§£åŽ‹æ•°é‡ (ç”¨äºŽ first_n å’Œ last_n æ¨¡å¼)')
    parser.add_argument('--extract-range', '-er', type=str,
                      help='è§£åŽ‹èŒƒå›´ (ç”¨äºŽ range æ¨¡å¼ï¼Œæ ¼å¼: start:end)')
    parser.add_argument('--dehash-mode', '-dm', action='store_true',
                      help='å¯ç”¨åŽ»æ±‰åŒ–æ¨¡å¼')
    parser.add_argument('--front-n', '-fn', type=int, default=3,
                      help='åŽ»æ±‰åŒ–æ¨¡å¼ï¼šå¤„ç†å‰Nå¼ å›¾ç‰‡ (é»˜è®¤: 3)')
    parser.add_argument('--back-n', '-bn', type=int, default=5,
                      help='åŽ»æ±‰åŒ–æ¨¡å¼ï¼šå¤„ç†åŽNå¼ å›¾ç‰‡ (é»˜è®¤: 5)')
    parser.add_argument('--workers', '-w', type=int, default=16,
                      help='æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°')
    parser.add_argument('path', nargs='*', help='è¦å¤„ç†çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    return parser

def run_application(args):
    """è¿è¡Œåº”ç”¨ç¨‹åº"""
    try:
        paths = InputHandler.get_input_paths(
            cli_paths=args.path,
            use_clipboard=args.clipboard,
            path_normalizer=PathHandler.normalize_path
        )
        
        if not paths:
            logger.error("[#update_log]æœªæä¾›ä»»ä½•æœ‰æ•ˆè·¯å¾„")
            return False
            
        filter_instance = RecruitCoverFilter(
            hash_file=args.hash_file,
            hamming_threshold=args.hamming_threshold,
            watermark_keywords=args.watermark_keywords,
            max_workers=args.workers  # ä¼ é€’çº¿ç¨‹æ•°å‚æ•°
        )
        
        # å‡†å¤‡è§£åŽ‹å‚æ•°
        extract_params = {}
        if args.dehash_mode:
            extract_params['front_n'] = args.front_n
            extract_params['back_n'] = args.back_n
        elif args.extract_mode in [ExtractMode.FIRST_N, ExtractMode.LAST_N]:
            extract_params['n'] = args.extract_n
        elif args.extract_mode == ExtractMode.RANGE:
            extract_params['range_str'] = args.extract_range
            
        # åˆ›å»ºåº”ç”¨ç¨‹åºå®žä¾‹ï¼Œä½¿ç”¨æŒ‡å®šçš„çº¿ç¨‹æ•°
        app = Application(max_workers=args.workers)
        
        for path in paths:
            if args.dehash_mode:
                if not filter_instance.image_filter.hash_file:
                    recruit_folder = r"E:\1EHV\[01æ‚]\zzzåŽ»å›¾"
                    hash_file = filter_instance.prepare_hash_file(recruit_folder)
                    if not hash_file:
                        logger.error(f"[#update_log]âŒ åŽ»æ±‰åŒ–æ¨¡å¼éœ€è¦å“ˆå¸Œæ–‡ä»¶ï¼Œä½†å‡†å¤‡å¤±è´¥")
                        continue
                
                app.process_directory(path, filter_instance, is_dehash_mode=True, extract_params=extract_params)
            else:
                app.process_directory(path, filter_instance, extract_params=extract_params)
            
        logger.info("[#update_log]âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"[#update_log]ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return False

def get_mode_config():
    """èŽ·å–æ¨¡å¼é…ç½®"""
    return {
        'use_debugger': True,
        'use_tui': True,
        'debug_config': {
            'base_modes': {
                "1": {
                    "name": "åŽ»æ°´å°æ¨¡å¼",
                    "description": "æ£€æµ‹å¹¶åˆ é™¤å¸¦æ°´å°çš„å›¾ç‰‡",
                    "base_args": ["-ht", "--duplicate-filter-mode", "watermark"],
                    "default_params": {
                        "ht": "16"
                    }
                },
                "2": {
                    "name": "åŽ»æ±‰åŒ–æ¨¡å¼",
                    "description": "å¤„ç†å‰åŽNå¼ å›¾ç‰‡å¹¶ä½¿ç”¨å“ˆå¸ŒåŽ»é‡",
                    "base_args": ["-dm", "-ht", "-fn", "-bn"],
                    "default_params": {
                        "ht": "16",
                        "fn": "3",
                        "bn": "5"
                    }
                }
            },
            'param_options': {
                "ht": {"name": "æ±‰æ˜Žè·ç¦»é˜ˆå€¼", "arg": "-ht", "default": "16", "type": int},
                "en": {"name": "è§£åŽ‹æ•°é‡", "arg": "-en", "default": "3", "type": int},
                "er": {"name": "è§£åŽ‹èŒƒå›´", "arg": "-er", "default": "0:3", "type": str},
                "fn": {"name": "å‰Nå¼ æ•°é‡", "arg": "-fn", "default": "3", "type": int},
                "bn": {"name": "åŽNå¼ æ•°é‡", "arg": "-bn", "default": "5", "type": int},
                "c": {"name": "ä»Žå‰ªè´´æ¿è¯»å–", "arg": "-c", "is_flag": True},
                "dfm": {"name": "é‡å¤è¿‡æ»¤æ¨¡å¼", "arg": "--duplicate-filter-mode", "default": "quality", "type": str}
            }
        },
        'tui_config': {
            'checkbox_options': [
                ("ä»Žå‰ªè´´æ¿è¯»å–", "clipboard", "-c"),
                ("åŽ»æ±‰åŒ–æ¨¡å¼", "dehash_mode", "-dm"),
            ],
            'input_options': [
                ("æ±‰æ˜Žè·ç¦»é˜ˆå€¼", "hamming_threshold", "-ht", "16", "è¾“å…¥æ•°å­—(é»˜è®¤16)"),
                ("è§£åŽ‹æ•°é‡", "extract_n", "-en", "3", "è¾“å…¥æ•°å­—(é»˜è®¤3)"),
                ("è§£åŽ‹èŒƒå›´", "extract_range", "-er", "0:3", "æ ¼å¼: start:end"),
                ("å‰Nå¼ æ•°é‡", "front_n", "-fn", "3", "è¾“å…¥æ•°å­—(é»˜è®¤3)"),
                ("åŽNå¼ æ•°é‡", "back_n", "-bn", "5", "è¾“å…¥æ•°å­—(é»˜è®¤5)"),
                ("å“ˆå¸Œæ–‡ä»¶è·¯å¾„", "hash_file", "-hf", "", "è¾“å…¥å“ˆå¸Œæ–‡ä»¶è·¯å¾„(å¯é€‰)"),
                ("é‡å¤è¿‡æ»¤æ¨¡å¼", "duplicate_filter_mode", "--duplicate-filter-mode", "quality", "quality/watermark/hash"),
            ],
            'preset_configs': {
                "åŽ»æ°´å°æ¨¡å¼": {
                    "description": "æ£€æµ‹å¹¶åˆ é™¤å¸¦æ°´å°çš„å›¾ç‰‡",
                    "checkbox_options": ["clipboard"],
                    "input_values": {
                        "hamming_threshold": "16",
                        "front_n": "3",
                        "duplicate_filter_mode": "watermark"
                    }
                },
                "åŽ»æ±‰åŒ–æ¨¡å¼": {
                    "description": "å¤„ç†å‰åŽNå¼ å›¾ç‰‡å¹¶ä½¿ç”¨å“ˆå¸ŒåŽ»é‡",
                    "checkbox_options": ["clipboard", "dehash_mode"],
                    "input_values": {
                        "hamming_threshold": "16",
                        "front_n": "3",
                        "back_n": "5",
                        "duplicate_filter_mode": "hash"
                    }
                }
            }
        }
    }

# è°ƒè¯•æ¨¡å¼å¼€å…³

if __name__ == '__main__':
    mode_manager = create_mode_manager(
        config=get_mode_config(),
        cli_parser_setup=setup_cli_parser,
        application_runner=run_application
    )
    
    if DEBUG_MODE:
        success = mode_manager.run_debug()
    elif len(sys.argv) > 1:
        success = mode_manager.run_cli()
    else:
        success = mode_manager.run_tui()
    
    if not success:
        sys.exit(1) 