from pathlib import Path
import sys
import os
import json
import logging
from typing import List, Dict, Set, Tuple
import time
import subprocess
import argparse
import pyperclip
from nodes.config.import_bundles import *
from nodes.record.logger_config import setup_logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from nodes.pics.calculate_hash_custom import ImageHashCalculator, PathURIGenerator
from nodes.pics.watermark_detector import WatermarkDetector

logger = logging.getLogger(__name__)

config = {
    'script_name': 'recruit_cover_filter',
    'console_enabled': False,
    'default_hash_file': os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
                                     'data', 'image_hashes.json')
}
logger, config_info = setup_logger(config)

# åˆå§‹åŒ– TextualLoggerManager
HAS_TUI = True
USE_DEBUGGER = True

TEXTUAL_LAYOUT = {
    "cur_stats": {
        "ratio": 1,
        "title": "ğŸ“Š æ€»ä½“è¿›åº¦",
        "style": "lightyellow"
    },
    "cur_progress": {
        "ratio": 1,
        "title": "ğŸ”„ å½“å‰è¿›åº¦",
        "style": "lightcyan"
    },
    "file_ops": {
        "ratio": 2,
        "title": "ğŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightpink"
    },
    "ocr_results": {
        "ratio": 2,
        "title": "ğŸ“ OCRç»“æœ",
        "style": "lightgreen"
    },
    "update_log": {
        "ratio": 1,
        "title": "ğŸ”§ ç³»ç»Ÿæ¶ˆæ¯",
        "style": "lightwhite"
    }
}

def initialize_textual_logger():
    """åˆå§‹åŒ–æ—¥å¿—å¸ƒå±€"""
    try:
        TextualLoggerManager.set_layout(TEXTUAL_LAYOUT, config_info['log_file'])
        logger.info("[#update_log]âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

class ExtractMode:
    """è§£å‹æ¨¡å¼ç±»"""
    
    ALL = "all"  # å…¨éƒ¨è§£å‹
    FIRST_N = "first_n"  # è§£å‹å‰Nå¼ 
    LAST_N = "last_n"  # è§£å‹åNå¼ 
    RANGE = "range"  # è§£å‹æŒ‡å®šèŒƒå›´
    
    @staticmethod
    def get_selected_indices(mode: str, total_files: int, params: dict) -> Set[int]:
        """
        æ ¹æ®è§£å‹æ¨¡å¼è·å–é€‰ä¸­çš„æ–‡ä»¶ç´¢å¼•
        
        Args:
            mode: è§£å‹æ¨¡å¼
            total_files: æ€»æ–‡ä»¶æ•°
            params: å‚æ•°å­—å…¸ï¼ŒåŒ…å« n æˆ– range_str
            
        Returns:
            Set[int]: é€‰ä¸­çš„æ–‡ä»¶ç´¢å¼•é›†åˆ
        """
        if mode == ExtractMode.ALL:
            return set(range(total_files))
            
        elif mode == ExtractMode.FIRST_N:
            n = min(params.get('n', 1), total_files)
            return set(range(n))
            
        elif mode == ExtractMode.LAST_N:
            n = min(params.get('n', 1), total_files)
            return set(range(total_files - n, total_files))
            
        elif mode == ExtractMode.RANGE:
            range_str = params.get('range_str', '')
            try:
                start, end = map(int, range_str.split(':'))
                start = max(0, start)
                end = min(total_files, end)
                return set(range(start, end))
            except:
                return set()
                
        return set()

class RecruitCoverFilter:
    """å°é¢å›¾ç‰‡è¿‡æ»¤å™¨"""
    
    def __init__(self, hash_file: str = None, cover_count: int = 3, hamming_threshold: int = 12):
        """
        åˆå§‹åŒ–è¿‡æ»¤å™¨
        
        Args:
            hash_file: å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            cover_count: å¤„ç†çš„å°é¢å›¾ç‰‡æ•°é‡
            hamming_threshold: æ±‰æ˜è·ç¦»é˜ˆå€¼
        """
        self.hash_file = hash_file or config['default_hash_file']
        self.cover_count = cover_count
        self.hamming_threshold = hamming_threshold
        self.hash_cache = self._load_hash_file()
        self.watermark_detector = WatermarkDetector()
        
    def _load_hash_file(self) -> Dict:
        """åŠ è½½å“ˆå¸Œæ–‡ä»¶"""
        try:
            if os.path.exists(self.hash_file):
                with open(self.hash_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logger.info(f"[#file_ops]æˆåŠŸåŠ è½½å“ˆå¸Œæ–‡ä»¶: {self.hash_file}")
                return data.get('hashes', {})  # é€‚é…æ–°çš„å“ˆå¸Œæ–‡ä»¶æ ¼å¼
            else:
                logger.error(f"[#file_ops]å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {self.hash_file}")
                return {}
        except Exception as e:
            logger.error(f"[#file_ops]åŠ è½½å“ˆå¸Œæ–‡ä»¶å¤±è´¥: {e}")
            return {}
            
    def _get_image_hash(self, image_path: str) -> str:
        """è·å–å›¾ç‰‡å“ˆå¸Œå€¼ï¼Œä¼˜å…ˆä»ç¼“å­˜è¯»å–"""
        image_uri = PathURIGenerator.generate(image_path)
        
        # ä»ç¼“å­˜ä¸­æŸ¥æ‰¾
        if image_uri in self.hash_cache:
            hash_data = self.hash_cache[image_uri]
            return hash_data.get('hash') if isinstance(hash_data, dict) else hash_data
            
        # è®¡ç®—æ–°çš„å“ˆå¸Œå€¼
        try:
            hash_value = ImageHashCalculator.calculate_phash(image_path)
            if hash_value:
                self.hash_cache[image_uri] = {'hash': hash_value}
                return hash_value
        except Exception as e:
            logger.error(f"[#file_ops]è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼å¤±è´¥ {image_path}: {e}")
            
        return None
        
    def _find_similar_images(self, image_files: List[str]) -> List[List[str]]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„å›¾ç‰‡ç»„"""
        similar_groups = []
        processed = set()
        
        for i, img1 in enumerate(image_files):
            if img1 in processed:
                continue
                
            hash1 = self._get_image_hash(img1)
            if not hash1:
                continue
                
            current_group = [img1]
            
            for j, img2 in enumerate(image_files[i+1:], i+1):
                if img2 in processed:
                    continue
                    
                hash2 = self._get_image_hash(img2)
                if not hash2:
                    continue
                    
                # è®¡ç®—æ±‰æ˜è·ç¦»
                distance = ImageHashCalculator.calculate_hamming_distance(hash1, hash2)
                if distance <= self.hamming_threshold:
                    current_group.append(img2)
                    logger.info(f"[#file_ops]æ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡: {os.path.basename(img2)} (è·ç¦»: {distance})")
                    
            if len(current_group) > 1:
                similar_groups.append(current_group)
                processed.update(current_group)
                logger.info(f"[#file_ops]æ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡ç»„: {len(current_group)}å¼ ")
                
        return similar_groups
        
    def process_images(self, image_files: List[str]) -> Tuple[Set[str], Dict[str, List[str]]]:
        """
        å¤„ç†å›¾ç‰‡åˆ—è¡¨ï¼Œè¿”å›è¦åˆ é™¤çš„å›¾ç‰‡å’Œåˆ é™¤åŸå› 
        
        Args:
            image_files: å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            Tuple[Set[str], Dict[str, List[str]]]: (è¦åˆ é™¤çš„æ–‡ä»¶é›†åˆ, åˆ é™¤åŸå› å­—å…¸)
        """
        # æ’åºå¹¶åªå–å‰Nå¼ 
        sorted_files = sorted(image_files)
        cover_files = sorted_files[:self.cover_count]
        
        if not cover_files:
            return set(), {}
            
        logger.info(f"[#file_ops]å¤„ç†å‰{self.cover_count}å¼ å›¾ç‰‡")
        
        # æŸ¥æ‰¾ç›¸ä¼¼å›¾ç‰‡ç»„
        similar_groups = self._find_similar_images(cover_files)
        
        # å¤„ç†æ¯ç»„ç›¸ä¼¼å›¾ç‰‡
        to_delete = set()
        removal_reasons = {}
        
        for group in similar_groups:
            # æ£€æµ‹æ¯å¼ å›¾ç‰‡çš„æ°´å°
            watermark_results = {}
            for img_path in group:
                has_watermark, texts = self.watermark_detector.detect_watermark(img_path)
                watermark_results[img_path] = (has_watermark, texts)
                logger.info(f"[#ocr_results]å›¾ç‰‡ {os.path.basename(img_path)} OCRç»“æœ: {texts}")
            
            # æ‰¾å‡ºæ— æ°´å°çš„å›¾ç‰‡
            clean_images = [img for img, (has_mark, _) in watermark_results.items() 
                          if not has_mark]
            
            if clean_images:
                # å¦‚æœæœ‰æ— æ°´å°ç‰ˆæœ¬ï¼Œåˆ é™¤å…¶ä»–ç‰ˆæœ¬
                keep_image = clean_images[0]
                logger.info(f"[#file_ops]ä¿ç•™æ— æ°´å°å›¾ç‰‡: {os.path.basename(keep_image)}")
                for img in group:
                    if img != keep_image:
                        to_delete.add(img)
                        removal_reasons[img] = {
                            'reason': 'recruit_cover',
                            'watermark_texts': watermark_results[img][1]
                        }
                        logger.info(f"[#file_ops]æ ‡è®°åˆ é™¤æœ‰æ°´å°å›¾ç‰‡: {os.path.basename(img)}")
            else:
                # å¦‚æœéƒ½æœ‰æ°´å°ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
                keep_image = group[0]
                logger.info(f"[#file_ops]ä¿ç•™ç¬¬ä¸€å¼ å›¾ç‰‡: {os.path.basename(keep_image)}")
                for img in group[1:]:
                    to_delete.add(img)
                    removal_reasons[img] = {
                        'reason': 'recruit_cover',
                        'watermark_texts': watermark_results[img][1]
                    }
                    logger.info(f"[#file_ops]æ ‡è®°åˆ é™¤é‡å¤å›¾ç‰‡: {os.path.basename(img)}")
        
        return to_delete, removal_reasons

    def process_archive(self, zip_path: str, extract_mode: str = ExtractMode.ALL, extract_params: dict = None) -> bool:
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…"""
        try:
            logger.info(f"[#file_ops]å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {zip_path}")
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = os.path.join(os.path.dirname(zip_path), f'temp_{int(time.time())}')
            os.makedirs(temp_dir, exist_ok=True)
            
            # è·å–å‹ç¼©åŒ…å†…å®¹
            cmd = ['7z', 'l', '-slt', zip_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.error(f"[#file_ops]è¯»å–å‹ç¼©åŒ…å†…å®¹å¤±è´¥: {result.stderr}")
                return False
                
            # è§£ææ–‡ä»¶åˆ—è¡¨
            files = []
            for line in result.stdout.split('\n'):
                line = line.strip()
                if line.startswith('Path = '):
                    file_path = line[7:]
                    if file_path.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                        files.append(file_path)
                        
            if not files:
                logger.info("[#file_ops]æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
                shutil.rmtree(temp_dir)
                return False
                
            # è·å–è¦è§£å‹çš„æ–‡ä»¶ç´¢å¼•
            extract_params = extract_params or {}
            selected_indices = ExtractMode.get_selected_indices(extract_mode, len(files), extract_params)
            
            if not selected_indices:
                logger.error("[#file_ops]æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶è¿›è¡Œè§£å‹")
                shutil.rmtree(temp_dir)
                return False
                
            # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨
            list_file = os.path.join(temp_dir, '@files.txt')
            selected_files = [files[i] for i in selected_indices]
            with open(list_file, 'w', encoding='utf-8') as f:
                for file in selected_files:
                    f.write(file + '\n')
                    
            # è§£å‹é€‰å®šæ–‡ä»¶
            cmd = ['7z', 'x', zip_path, f'-o{temp_dir}', f'@{list_file}', '-y']
            result = subprocess.run(cmd, capture_output=True, text=True)
            os.remove(list_file)
            
            if result.returncode != 0:
                logger.error(f"[#file_ops]è§£å‹å¤±è´¥: {result.stderr}")
                shutil.rmtree(temp_dir)
                return False
                
            # å¤„ç†è§£å‹åçš„å›¾ç‰‡
            image_files = []
            for root, _, files in os.walk(temp_dir):
                for file in files:
                    if file.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                        image_files.append(os.path.join(root, file))
                        
            # å¤„ç†å›¾ç‰‡
            to_delete, removal_reasons = self.process_images(image_files)
            
            if not to_delete:
                logger.info("[#file_ops]æ²¡æœ‰éœ€è¦åˆ é™¤çš„å›¾ç‰‡")
                shutil.rmtree(temp_dir)
                return False
                
            # åˆ é™¤æ ‡è®°çš„æ–‡ä»¶
            for file_path in to_delete:
                try:
                    os.remove(file_path)
                    reason = removal_reasons[file_path]
                    logger.info(f"[#file_ops]åˆ é™¤æ–‡ä»¶: {os.path.basename(file_path)}")
                    logger.info(f"[#ocr_results]åˆ é™¤åŸå› : {reason}")
                except Exception as e:
                    logger.error(f"[#file_ops]åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    
            # åˆ›å»ºæ–°çš„å‹ç¼©åŒ…
            new_zip = zip_path + '.new'
            cmd = ['7z', 'a', new_zip, os.path.join(temp_dir, '*')]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.error(f"[#file_ops]åˆ›å»ºæ–°å‹ç¼©åŒ…å¤±è´¥: {result.stderr}")
                shutil.rmtree(temp_dir)
                return False
                
            # å¤‡ä»½åŸæ–‡ä»¶å¹¶æ›¿æ¢
            backup_path = zip_path + '.bak'
            shutil.copy2(zip_path, backup_path)
            os.replace(new_zip, zip_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            shutil.rmtree(temp_dir)
            
            logger.info(f"[#file_ops]æˆåŠŸå¤„ç†å‹ç¼©åŒ…: {zip_path}")
            return True
            
        except Exception as e:
            logger.error(f"[#file_ops]å¤„ç†å‹ç¼©åŒ…å¤±è´¥ {zip_path}: {e}")
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
            return False

class InputHandler:
    """è¾“å…¥å¤„ç†ç±»"""
    
    @staticmethod
    def parse_arguments():
        """è§£æå‘½ä»¤è¡Œå‚æ•°"""
        parser = argparse.ArgumentParser(description='æ‹›å‹Ÿå°é¢å›¾ç‰‡è¿‡æ»¤å·¥å…·')
        parser.add_argument('--hash-file', '-hf', type=str,
                          help='å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®ï¼‰')
        parser.add_argument('--cover-count', '-cc', type=int, default=3,
                          help='å¤„ç†çš„å°é¢å›¾ç‰‡æ•°é‡ (é»˜è®¤: 3)')
        parser.add_argument('--hamming-threshold', '-ht', type=int, default=12,
                          help='æ±‰æ˜è·ç¦»é˜ˆå€¼ (é»˜è®¤: 12)')
        parser.add_argument('--clipboard', '-c', action='store_true',
                          help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
        parser.add_argument('--extract-mode', '-em', type=str, 
                          choices=[ExtractMode.ALL, ExtractMode.FIRST_N, ExtractMode.LAST_N, ExtractMode.RANGE],
                          default=ExtractMode.ALL, help='è§£å‹æ¨¡å¼ (é»˜è®¤: all)')
        parser.add_argument('--extract-n', '-en', type=int,
                          help='è§£å‹æ•°é‡ (ç”¨äº first_n å’Œ last_n æ¨¡å¼)')
        parser.add_argument('--extract-range', '-er', type=str,
                          help='è§£å‹èŒƒå›´ (ç”¨äº range æ¨¡å¼ï¼Œæ ¼å¼: start:end)')
        parser.add_argument('path', nargs='*', help='è¦å¤„ç†çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
        return parser.parse_args()

    @staticmethod
    def normalize_path(path: str) -> str:
        """è§„èŒƒåŒ–è·¯å¾„ï¼Œå¤„ç†å¼•å·å’Œè½¬ä¹‰å­—ç¬¦"""
        # ç§»é™¤é¦–å°¾çš„å¼•å·
        path = path.strip('"\'')
        # å¤„ç†è½¬ä¹‰å­—ç¬¦
        path = path.replace('\\\\', '\\')
        return path

    @staticmethod
    def get_input_paths(args):
        """è·å–è¾“å…¥è·¯å¾„"""
        paths = []
        
        # ä»å‘½ä»¤è¡Œå‚æ•°è·å–è·¯å¾„
        if args.path:
            paths.extend([InputHandler.normalize_path(p) for p in args.path])
            
        # ä»å‰ªè´´æ¿è·å–è·¯å¾„
        if args.clipboard or not paths:  # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä¹Ÿå°è¯•ä»å‰ªè´´æ¿è¯»å–
            try:
                clipboard_content = pyperclip.paste()
                if clipboard_content:
                    # å¤„ç†å‰ªè´´æ¿å†…å®¹ï¼Œæ”¯æŒå¤šè¡Œè·¯å¾„
                    clipboard_paths = [
                        InputHandler.normalize_path(p.strip())
                        for p in clipboard_content.splitlines()
                        if p.strip()
                    ]
                    paths.extend(clipboard_paths)
                    logger.info(f"[#file_ops]ä»å‰ªè´´æ¿è¯»å–äº† {len(clipboard_paths)} ä¸ªè·¯å¾„")
            except Exception as e:
                logger.error(f"[#update_log]ä»å‰ªè´´æ¿è¯»å–å¤±è´¥: {e}")
                
        # å¦‚æœä»ç„¶æ²¡æœ‰è·¯å¾„ï¼Œæç¤ºç”¨æˆ·è¾“å…¥
        if not paths:
            print("è¯·è¾“å…¥è¦å¤„ç†çš„æ–‡ä»¶å¤¹æˆ–å‹ç¼©åŒ…è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
            while True:
                line = input().strip()
                if not line:
                    break
                paths.append(InputHandler.normalize_path(line))
                
        # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
        valid_paths = []
        for p in paths:
            if os.path.exists(p):
                valid_paths.append(p)
            else:
                logger.warning(f"[#file_ops]è·¯å¾„ä¸å­˜åœ¨: {p}")
                
        return valid_paths

class Application:
    """åº”ç”¨ç¨‹åºç±»"""
    
    def process_directory(self, directory: str, filter_instance: RecruitCoverFilter):
        """å¤„ç†ç›®å½•"""
        try:
            if os.path.isfile(directory):
                if directory.lower().endswith('.zip'):
                    filter_instance.process_archive(directory)
            else:
                for root, _, files in os.walk(directory):
                    for file in files:
                        if file.lower().endswith('.zip'):
                            zip_path = os.path.join(root, file)
                            filter_instance.process_archive(zip_path)
        except Exception as e:
            logger.error(f"[#update_log]å¤„ç†ç›®å½•å¤±è´¥ {directory}: {e}")

    def main(self):
        """ä¸»å‡½æ•°"""
        try:
            args = InputHandler.parse_arguments()
            paths = InputHandler.get_input_paths(args)
            initialize_textual_logger()
        
            if not paths:
                logger.error("[#update_log]æœªæä¾›ä»»ä½•æœ‰æ•ˆè·¯å¾„")
                return
                
            filter_instance = RecruitCoverFilter(
                hash_file=args.hash_file,
                cover_count=args.cover_count,
                hamming_threshold=args.hamming_threshold
            )
            
            # å‡†å¤‡è§£å‹å‚æ•°
            extract_params = {}
            if args.extract_mode in [ExtractMode.FIRST_N, ExtractMode.LAST_N]:
                extract_params['n'] = args.extract_n
            elif args.extract_mode == ExtractMode.RANGE:
                extract_params['range_str'] = args.extract_range
            
            for path in paths:
                self.process_directory(path, filter_instance)
                
            logger.info("[#update_log]å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"[#update_log]ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == '__main__':
    Application().main() 