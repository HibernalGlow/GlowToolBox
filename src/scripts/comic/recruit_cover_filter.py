from pathlib import Path
import sys
import os
import json
import logging
from typing import List, Dict, Set, Tuple
import time
import subprocess
import argparse
import pyperclip
from nodes.config.import_bundles import *
from nodes.record.logger_config import setup_logger
from nodes.tui.mode_manager import create_mode_manager
from nodes.file_ops.backup_handler import BackupHandler
from nodes.file_ops.archive_handler import ArchiveHandler
from nodes.pics.image_filter import ImageFilter
from nodes.io.input_handler import InputHandler
from nodes.io.config_handler import ConfigHandler
from nodes.io.path_handler import PathHandler, ExtractMode
import platform
import stat
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import multiprocessing
import zipfile
from datetime import datetime
import re

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å¸¸é‡
SUPPORTED_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.avif', '.heic', '.heif', '.jxl'}
LOGS_DIR = r"D:\1VSCODE\GlowToolBox\logs\recruit_cover_filter"
PROCESSED_PATHS_FILE = r"D:\1VSCODE\GlowToolBox\data\processed_paths.txt"
PROCESSED_CACHE = set()

config = {
    'script_name': 'recruit_cover_filter',
    'console_enabled': False,
}
logger, config_info = setup_logger(config)
DEBUG_MODE = False

TEXTUAL_LAYOUT = {
    # "global_progress": {
    #     "ratio": 1,
    #     "title": "ğŸŒ æ€»ä½“è¿›åº¦",
    #     "style": "lightyellow"
    # },
    "path_progress": {
        "ratio": 1,
        "title": "ğŸ”„ å½“å‰è¿›åº¦",
        "style": "lightcyan"
    },
    "file_ops": {
        "ratio": 2,
        "title": "ğŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightpink"
    },
    "sys_log": {
        "ratio": 1,
        "title": "ğŸ”§ ç³»ç»Ÿæ¶ˆæ¯",
        "style": "lightgreen"
    }
}

def initialize_textual_logger(layout: dict, log_file: str) -> None:
    """
    åˆå§‹åŒ–æ—¥å¿—å¸ƒå±€
    
    Args:
        layout: å¸ƒå±€é…ç½®å­—å…¸
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    try:
        TextualLoggerManager.set_layout(layout, config_info['log_file'])
        logger.info("[#sys_log]âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}") 

class RecruitCoverFilter:
    """å°é¢å›¾ç‰‡è¿‡æ»¤å™¨"""
    
    def __init__(self, hash_file: str = None, hamming_threshold: int = 16, watermark_keywords: List[str] = None, max_workers: int = None):
        """åˆå§‹åŒ–è¿‡æ»¤å™¨"""
        self.image_filter = ImageFilter(hash_file, hamming_threshold)
        self.watermark_keywords = watermark_keywords
        self.max_workers = max_workers or multiprocessing.cpu_count()
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        initialize_textual_logger(TEXTUAL_LAYOUT, config_info['log_file'])
        
    def prepare_hash_file(self, recruit_folder: str, workers: int = 16, force_update: bool = False) -> str:
        """
        å‡†å¤‡å“ˆå¸Œæ–‡ä»¶
        
        Args:
            recruit_folder: æ‹›å‹Ÿæ–‡ä»¶å¤¹è·¯å¾„
            workers: å·¥ä½œçº¿ç¨‹æ•°
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            
        Returns:
            str: å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            from nodes.pics.hash_process_config import process_artist_folder
            hash_file = process_artist_folder(recruit_folder, workers, force_update)
            if hash_file:
                logger.info(f"[#sys_log]âœ… æˆåŠŸç”Ÿæˆå“ˆå¸Œæ–‡ä»¶: {hash_file}")
                self.image_filter.hash_file = hash_file
                self.image_filter.hash_cache = self.image_filter._load_hash_file()
                return hash_file
            else:
                logger.error("[#sys_log]âŒ ç”Ÿæˆå“ˆå¸Œæ–‡ä»¶å¤±è´¥")
                return None
        except Exception as e:
            logger.error(f"[#sys_log]âŒ å‡†å¤‡å“ˆå¸Œæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

    def _robust_cleanup(self, temp_dir: str) -> None:
        """æ›´å¥å£®çš„æ–‡ä»¶æ¸…ç†æ–¹æ³•ï¼Œå¤„ç†æ–‡ä»¶è¢«å ç”¨çš„æƒ…å†µ"""
        if not os.path.exists(temp_dir):
            return

        def on_rm_error(func, path, exc_info):
            try:
                os.chmod(path, stat.S_IWRITE)
                os.unlink(path)
                logger.info(f"[#file_ops]æˆåŠŸåˆ é™¤ {path}")
            except Exception as e:
                logger.warning(f"[#file_ops]æ— æ³•åˆ é™¤ {path}: {e}")

        try:
            # å°è¯•æ ‡å‡†åˆ é™¤
            shutil.rmtree(temp_dir, onerror=on_rm_error)
        except Exception as e:
            logger.warning(f"[#file_ops]æ ‡å‡†åˆ é™¤å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åˆ é™¤: {temp_dir}")
            try:
                # ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤å¼ºåˆ¶åˆ é™¤ï¼ˆWindowsï¼‰
                if platform.system() == 'Windows':
                    subprocess.run(f'rmdir /s /q "{temp_dir}"', shell=True, check=True)
                else:  # Linux/MacOS
                    subprocess.run(f'rm -rf "{temp_dir}"', shell=True, check=True)
            except subprocess.CalledProcessError as e:
                logger.error(f"[#sys_log]å¼ºåˆ¶åˆ é™¤å¤±è´¥: {temp_dir}")
                raise

    def process_archive(self, zip_path: str, extract_mode: str = ExtractMode.ALL, extract_params: dict = None, is_dehash_mode: bool = False) -> Tuple[bool, str]:
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…
        
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, å¤±è´¥åŸå› )
        """
        logger.info(f"[#file_ops]å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {zip_path}")
        
        # åˆ—å‡ºå‹ç¼©åŒ…å†…å®¹å¹¶é¢„å…ˆè¿‡æ»¤å›¾ç‰‡æ–‡ä»¶
        files = [f for f in ArchiveHandler.list_archive_contents(zip_path)
                if PathHandler.get_file_extension(f).lower() in SUPPORTED_EXTENSIONS]
        
        if not files:
            logger.info("[#file_ops]æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return False, "æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶"
            
        # è·å–è¦è§£å‹çš„æ–‡ä»¶ç´¢å¼•
        extract_params = extract_params or {}
        
        # å¦‚æœæŒ‡å®šäº†front_næˆ–back_nï¼Œå¼ºåˆ¶ä½¿ç”¨RANGEæ¨¡å¼
        if extract_params.get('front_n', 0) > 0 or extract_params.get('back_n', 0) > 0:
            extract_mode = ExtractMode.RANGE
            logger.info(f"[#file_ops]ä½¿ç”¨å‰åNå¼ æ¨¡å¼: front_n={extract_params.get('front_n', 0)}, back_n={extract_params.get('back_n', 0)}")
        
        # è·å–é€‰ä¸­çš„æ–‡ä»¶ç´¢å¼•
        selected_indices = ExtractMode.get_selected_indices(extract_mode, len(files), extract_params)
        
        # è®°å½•é€‰ä¸­çš„æ–‡ä»¶ä¿¡æ¯
        logger.info(f"[#file_ops]æ€»æ–‡ä»¶æ•°: {len(files)}, é€‰ä¸­æ–‡ä»¶æ•°: {len(selected_indices)}")
        if len(selected_indices) > 0:
            logger.info(f"[#file_ops]é€‰ä¸­çš„æ–‡ä»¶ç´¢å¼•: {sorted(selected_indices)}")
            
        if not selected_indices:
            logger.error("[#file_ops]æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶è¿›è¡Œè§£å‹")
            return False, "æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶è¿›è¡Œè§£å‹"
            
        # ç”Ÿæˆè§£å‹ç›®å½•åç§°
        zip_name = os.path.splitext(os.path.basename(zip_path))[0]
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        extract_dir = os.path.join(os.path.dirname(zip_path), f"temp_{zip_name}_{timestamp}")
            
        # è§£å‹é€‰å®šæ–‡ä»¶
        selected_files = [files[i] for i in selected_indices]
        logger.info(f"[#sys_log]å‡†å¤‡è§£å‹æ–‡ä»¶: {[os.path.basename(f) for f in selected_files]}")
        
        # æ›´æ–°è§£å‹è¿›åº¦
        logger.info(f"[#path_progress]è§£å‹æ–‡ä»¶: {os.path.basename(zip_path)}")
        logger.info(f"[#path_progress]å½“å‰è¿›åº¦: 0%")

        success, extract_dir = ArchiveHandler.extract_files(zip_path, selected_files, extract_dir)
        if not success:
            logger.info(f"[#path_progress]è§£å‹æ–‡ä»¶: {os.path.basename(zip_path)} (å¤±è´¥)")
            return False, "è§£å‹æ–‡ä»¶å¤±è´¥"
        logger.info(f"[#path_progress]è§£å‹æ–‡ä»¶: {os.path.basename(zip_path)}")
        logger.info(f"[#path_progress]å½“å‰è¿›åº¦: 50%")
            
        try:
            # è·å–è§£å‹åçš„å›¾ç‰‡æ–‡ä»¶
            image_files = [
                PathHandler.join_paths(root, file)
                for root, _, files in os.walk(extract_dir)
                for file in files
                if PathHandler.get_file_extension(file).lower() in SUPPORTED_EXTENSIONS
            ]
                        
            # å¤„ç†å›¾ç‰‡
            to_delete, removal_reasons = self.image_filter.process_images(
                image_files,
                enable_duplicate_filter=True,   # å¯ç”¨é‡å¤å›¾ç‰‡è¿‡æ»¤
                duplicate_filter_mode='hash' if self.image_filter.hash_file else 'watermark',  # å¦‚æœæœ‰å“ˆå¸Œæ–‡ä»¶åˆ™ä½¿ç”¨å“ˆå¸Œæ¨¡å¼
                watermark_keywords=None if is_dehash_mode else self.watermark_keywords  # å»æ±‰åŒ–æ¨¡å¼ä¸å¯ç”¨æ°´å°æ£€æµ‹
            )
            
            if not to_delete:
                logger.info("[#sys_log]æ²¡æœ‰éœ€è¦åˆ é™¤çš„å›¾ç‰‡")
                self._robust_cleanup(extract_dir)
                logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(zip_path)}")
                logger.info(f"[@path_progress]å½“å‰è¿›åº¦: 100%")
                return True, "æ²¡æœ‰éœ€è¦åˆ é™¤çš„å›¾ç‰‡"
                
            # å¤‡ä»½è¦åˆ é™¤çš„æ–‡ä»¶
            backup_results = BackupHandler.backup_removed_files(zip_path, to_delete, removal_reasons)
            
            # ä»å‹ç¼©åŒ…ä¸­åˆ é™¤æ–‡ä»¶
            files_to_delete = [os.path.relpath(file_path, extract_dir) for file_path in to_delete]
            logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(zip_path)}")
            logger.info(f"[@path_progress]å½“å‰è¿›åº¦: 75%")
                
            # ä½¿ç”¨7zåˆ é™¤æ–‡ä»¶
            delete_list_file = os.path.join(extract_dir, '@delete.txt')
            with open(delete_list_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(files_to_delete))
                    
            # åœ¨æ‰§è¡Œåˆ é™¤æ“ä½œå‰å¤‡ä»½åŸå§‹å‹ç¼©åŒ…
            backup_success, backup_path = BackupHandler.backup_source_file(zip_path)
            if backup_success:
                logger.info(f"[#sys_log]âœ… æºæ–‡ä»¶å¤‡ä»½æˆåŠŸ: {backup_path}")
            else:
                logger.warning(f"[#sys_log]âš ï¸ æºæ–‡ä»¶å¤‡ä»½å¤±è´¥: {backup_path}")
                return False, "æºæ–‡ä»¶å¤‡ä»½å¤±è´¥"

            # ä½¿ç”¨7zåˆ é™¤æ–‡ä»¶
            cmd = ['7z', 'd', zip_path, f'@{delete_list_file}']
            result = subprocess.run(cmd, capture_output=True, text=True)
            os.remove(delete_list_file)
            
            if result.returncode != 0:
                logger.error(f"[#sys_log]ä»å‹ç¼©åŒ…åˆ é™¤æ–‡ä»¶å¤±è´¥: {result.stderr}")
                self._robust_cleanup(extract_dir)
                logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(zip_path)} (å¤±è´¥)")
                return False, f"ä»å‹ç¼©åŒ…åˆ é™¤æ–‡ä»¶å¤±è´¥: {result.stderr}"
                
            logger.info(f"[#file_ops]æˆåŠŸå¤„ç†å‹ç¼©åŒ…: {zip_path}")
            self._robust_cleanup(extract_dir)
            logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(zip_path)}")
            logger.info(f"[@path_progress]å½“å‰è¿›åº¦: 100%")
            return True, ""
            
        except Exception as e:
            logger.error(f"[#sys_log]å¤„ç†å‹ç¼©åŒ…å¤±è´¥ {zip_path}: {e}")
            self._robust_cleanup(extract_dir)
            logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(zip_path)} (é”™è¯¯)")
            return False, f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}"

class Application:
    """åº”ç”¨ç¨‹åºç±»"""
    
    def __init__(self, max_workers: int = None):
        """åˆå§‹åŒ–åº”ç”¨ç¨‹åº
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        """
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.archive_queue = Queue()
        
    def _process_single_archive(self, args):
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…æˆ–ç›®å½•çš„åŒ…è£…å‡½æ•°
        
        Args:
            args: åŒ…å«å¤„ç†å‚æ•°çš„å…ƒç»„ (path, filter_instance, extract_params, is_dehash_mode)
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, å¤±è´¥åŸå› )
        """
        path, filter_instance, extract_params, is_dehash_mode = args
        try:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(path):
                raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
                
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å¯è®¿é—®
            if not os.access(path, os.R_OK):
                raise PermissionError(f"è·¯å¾„æ— æ³•è®¿é—®: {path}")
            
            # å®šä¹‰é»‘åå•å…³é”®è¯
            blacklist_keywords = ["ç”»é›†", "CG", "å›¾é›†"]
            
            # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰zipæ–‡ä»¶
            if os.path.isdir(path):
                success = True
                error_msg = ""
                for root, _, files in os.walk(path):
                    # æ£€æŸ¥å½“å‰ç›®å½•è·¯å¾„æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
                    root_lower = root.lower()
                    if any(kw in root_lower for kw in blacklist_keywords):
                        logger.info(f"[#sys_log]è·³è¿‡é»‘åå•ç›®å½•: {root}")
                        continue
                        
                    for file in files:
                        if file.lower().endswith('.zip'):
                            zip_path = os.path.join(root, file)
                            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
                            if any(kw in file.lower() for kw in blacklist_keywords):
                                logger.info(f"[#sys_log]è·³è¿‡é»‘åå•æ–‡ä»¶: {file}")
                                continue
                                
                            try:
                                if not zipfile.is_zipfile(zip_path):
                                    logger.warning(f"[#sys_log]è·³è¿‡æ— æ•ˆçš„ZIPæ–‡ä»¶: {zip_path}")
                                    continue
                                    
                                # å¤„ç†å•ä¸ªzipæ–‡ä»¶
                                file_success, file_error = filter_instance.process_archive(
                                    zip_path,
                                    extract_mode=ExtractMode.RANGE,  # é»˜è®¤ä½¿ç”¨RANGEæ¨¡å¼
                                    extract_params=extract_params,
                                    is_dehash_mode=is_dehash_mode
                                )
                                if not file_success:
                                    logger.warning(f"[#file_ops]å¤„ç†è¿”å›å¤±è´¥: {os.path.basename(zip_path)}, åŸå› : {file_error}")
                                    error_msg = file_error
                                success = success and file_success
                            except Exception as e:
                                error_msg = str(e)
                                logger.error(f"[#file_ops]å¤„ç†ZIPæ–‡ä»¶å¤±è´¥ {zip_path}: {error_msg}")
                                success = False
                return success, error_msg
                
            # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç¡®ä¿æ˜¯zipæ–‡ä»¶
            elif path.lower().endswith('.zip'):
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
                if any(kw in os.path.basename(path).lower() for kw in blacklist_keywords):
                    logger.info(f"[#file_ops]è·³è¿‡é»‘åå•æ–‡ä»¶: {os.path.basename(path)}")
                    return False, "é»‘åå•æ–‡ä»¶"
                    
                if not zipfile.is_zipfile(path):
                    raise ValueError(f"ä¸æ˜¯æœ‰æ•ˆçš„ZIPæ–‡ä»¶: {path}")
                    
                # å»æ±‰åŒ–æ¨¡å¼ç‰¹æ®Šå¤„ç†
                if is_dehash_mode:
                    if not filter_instance.image_filter.hash_file:
                        logger.error("[#sys_log]âŒ å»æ±‰åŒ–æ¨¡å¼éœ€è¦å“ˆå¸Œæ–‡ä»¶")
                        return False, "å»æ±‰åŒ–æ¨¡å¼éœ€è¦å“ˆå¸Œæ–‡ä»¶"
                    logger.info("[#sys_log]âœ… ä½¿ç”¨å»æ±‰åŒ–æ¨¡å¼å¤„ç†")
                
                # å¤„ç†å‹ç¼©åŒ…
                return filter_instance.process_archive(
                    path,
                    extract_mode=ExtractMode.RANGE,
                    extract_params=extract_params,
                    is_dehash_mode=is_dehash_mode
                )
            else:
                logger.warning(f"[#file_ops]è·³è¿‡éZIPæ–‡ä»¶: {path}")
                return False, "éZIPæ–‡ä»¶"
            
        except FileNotFoundError as e:
            logger.error(f"[#file_ops]è·¯å¾„ä¸å­˜åœ¨: {path}")
            raise
        except PermissionError as e:
            logger.error(f"[#file_ops]è·¯å¾„è®¿é—®æƒé™é”™è¯¯: {path}")
            raise
        except Exception as e:
            logger.error(f"[#file_ops]å¤„ç†è¿‡ç¨‹å‡ºé”™: {path}: {str(e)}")
            raise
            
    def process_directory(self, directory: str, filter_instance: RecruitCoverFilter, is_dehash_mode: bool = False, extract_params: dict = None):
        """å¤„ç†ç›®å½•æˆ–æ–‡ä»¶"""
        try:
            # åŠ è½½å·²å¤„ç†æ–‡ä»¶çš„ç¼“å­˜
            global PROCESSED_CACHE
            if not PROCESSED_CACHE:
                PROCESSED_CACHE = load_processed_files(PROCESSED_PATHS_FILE)
            
            # è·å–å½“å‰è·¯å¾„
            abs_path = os.path.abspath(directory)
            
            # æ£€æŸ¥å½“å‰è·¯å¾„æˆ–å…¶çˆ¶ç›®å½•æ˜¯å¦å·²å¤„ç†
            for processed_dir in PROCESSED_CACHE:
                if abs_path.startswith(processed_dir):
                    logger.info(f"[#sys_log]è·³è¿‡å·²å¤„ç†ç›®å½•: {abs_path}")
                    return True, "ç›®å½•å·²å¤„ç†"
            
            return self._process_single_archive((directory, filter_instance, extract_params, is_dehash_mode))
        
        except Exception as e:
            logger.error(f"[#sys_log]å¤„ç†å¤±è´¥ {directory}: {e}")
            return False, "å¤„ç†å¤±è´¥"

def setup_cli_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='æ‹›å‹Ÿå°é¢å›¾ç‰‡è¿‡æ»¤å·¥å…·')
    parser.add_argument('--debug', '-d', action='store_true',
                      help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--hash-file', '-hf', type=str,
                      help='å“ˆå¸Œæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®ï¼‰')
    parser.add_argument('--hamming-threshold', '-ht', type=int, default=16,
                      help='æ±‰æ˜è·ç¦»é˜ˆå€¼ (é»˜è®¤: 16)')
    parser.add_argument('--clipboard', '-c', action='store_true',
                      help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
    parser.add_argument('--watermark-keywords', '-wk', nargs='*',
                      help='æ°´å°å…³é”®è¯åˆ—è¡¨ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤åˆ—è¡¨')
    parser.add_argument('--duplicate-filter-mode', '-dfm', type=str,
                      choices=['quality', 'watermark', 'hash'],
                      default='watermark',
                      help='é‡å¤è¿‡æ»¤æ¨¡å¼ (hash=å»æ±‰åŒ–æ¨¡å¼, watermark=å»æ°´å°æ¨¡å¼, quality=è´¨é‡æ¨¡å¼)')
    parser.add_argument('--extract-mode', '-em', type=str, 
                      choices=[ExtractMode.ALL, ExtractMode.RANGE],
                      default=ExtractMode.ALL, help='è§£å‹æ¨¡å¼ (é»˜è®¤: all)')
    parser.add_argument('--extract-range', '-er', type=str,
                      help='è§£å‹èŒƒå›´ (ç”¨äº range æ¨¡å¼ï¼Œæ ¼å¼: start:end)')
    parser.add_argument('--front-n', '-fn', type=int, default=3,
                      help='å¤„ç†å‰Nå¼ å›¾ç‰‡ (é»˜è®¤: 3)')
    parser.add_argument('--back-n', '-bn', type=int, default=5,
                      help='å¤„ç†åNå¼ å›¾ç‰‡ (é»˜è®¤: 5)')
    parser.add_argument('--workers', '-w', type=int, default=16,
                      help='æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°')
    parser.add_argument('path', nargs='*', help='è¦å¤„ç†çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    return parser

def run_application(args):
    """è¿è¡Œåº”ç”¨ç¨‹åº"""
    try:
        # æ ¹æ®è¿‡æ»¤æ¨¡å¼åˆ¤æ–­æ˜¯å¦ä¸ºå»æ±‰åŒ–æ¨¡å¼
        is_dehash_mode = args.duplicate_filter_mode == 'hash'
        
        # æ·»åŠ æ¨¡å¼åˆ¤æ–­çš„æ—¥å¿—
        logger.info(f"[#sys_log]è¿è¡Œæ¨¡å¼: {'å»æ±‰åŒ–æ¨¡å¼' if is_dehash_mode else 'å»æ°´å°æ¨¡å¼'}")
        logger.info(f"[#sys_log]è¿‡æ»¤æ¨¡å¼: {args.duplicate_filter_mode}")

        paths = InputHandler.get_input_paths(
            cli_paths=args.path,
            use_clipboard=args.clipboard,
            path_normalizer=PathHandler.normalize_path
        )
        
        if not paths:
            logger.error("[#sys_log]æœªæä¾›ä»»ä½•æœ‰æ•ˆè·¯å¾„")
            return False
            
        # ä¿®æ”¹è¿‡æ»¤å™¨åˆå§‹åŒ–é€»è¾‘
        filter_instance = RecruitCoverFilter(
            hash_file=args.hash_file,
            hamming_threshold=args.hamming_threshold,
            # å¦‚æœæ˜¯å»æ±‰åŒ–æ¨¡å¼ï¼Œåˆ™ä¸ä½¿ç”¨æ°´å°å…³é”®è¯
            watermark_keywords=None if is_dehash_mode else args.watermark_keywords,
            max_workers=args.workers
        )

        # å¦‚æœæ˜¯å»æ±‰åŒ–æ¨¡å¼ä¸”æ²¡æœ‰æŒ‡å®šå“ˆå¸Œæ–‡ä»¶ï¼Œè‡ªåŠ¨å‡†å¤‡å“ˆå¸Œæ–‡ä»¶
        if is_dehash_mode and not args.hash_file:
            recruit_folder = r"E:\1EHV\[01æ‚]\zzzå»å›¾"
            hash_file = filter_instance.prepare_hash_file(recruit_folder)
            if not hash_file:
                logger.error("[#sys_log]âŒ å»æ±‰åŒ–æ¨¡å¼éœ€è¦å“ˆå¸Œæ–‡ä»¶ï¼Œä½†å‡†å¤‡å¤±è´¥")
                return False

        # å‡†å¤‡è§£å‹å‚æ•°
        extract_params = {
            'front_n': args.front_n,
            'back_n': args.back_n
        }
        
        if args.extract_mode == ExtractMode.RANGE and args.extract_range:
            extract_params['range_str'] = args.extract_range
            
        # åˆ›å»ºåº”ç”¨ç¨‹åºå®ä¾‹
        app = Application(max_workers=args.workers)
        
        # è®°å½•å¤„ç†å‚æ•°
        logger.info(f"[#sys_log]å¤„ç†å‚æ•°: front_n={args.front_n}, back_n={args.back_n}, mode={args.extract_mode}")
        if args.extract_range:
            logger.info(f"[#sys_log]è§£å‹èŒƒå›´: {args.extract_range}")
        
        total_count = len(paths)
        success_count = 0
        error_count = 0
        error_details = []
        
        # æ˜¾ç¤ºåˆå§‹å…¨å±€è¿›åº¦
        logger.info(f"[@global_progress]æ€»ä»»åŠ¡è¿›åº¦ (0/{total_count}) 0%")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†å‹ç¼©åŒ…
        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            future_to_archive = {
                executor.submit(
                    app._process_single_archive, 
                    (archive, filter_instance, extract_params, is_dehash_mode)
                ): archive for archive in paths
            }
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in as_completed(future_to_archive):
                archive = future_to_archive[future]
                try:
                    # æ˜¾ç¤ºå½“å‰å¤„ç†çš„æ–‡ä»¶è¿›åº¦
                    logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(archive)}")
                    logger.info(f"[@path_progress]å½“å‰è¿›åº¦: 0%")
                    
                    success, error_msg = future.result()
                    if success:
                        success_count += 1
                        logger.info(f"[#file_ops]âœ… æˆåŠŸå¤„ç†: {os.path.basename(archive)}")
                        # æ›´æ–°å½“å‰æ–‡ä»¶è¿›åº¦ä¸º100%
                        logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(archive)}")
                        logger.info(f"[@path_progress]å½“å‰è¿›åº¦: 100%")
                    else:
                        error_count += 1
                        error_msg = f"å¤„ç†è¿”å›å¤±è´¥: {os.path.basename(archive)}, åŸå› : {error_msg}"
                        error_details.append(error_msg)
                        logger.warning(f"[#file_ops]âš ï¸ {error_msg}")
                        # æ›´æ–°å½“å‰æ–‡ä»¶è¿›åº¦ä¸ºå¤±è´¥
                        logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(archive)} (å¤±è´¥)")
                except Exception as e:
                    error_count += 1
                    import traceback
                    error_trace = traceback.format_exc()
                    error_msg = f"å¤„ç†å‡ºé”™ {os.path.basename(archive)}: {str(e)}\n{error_trace}"
                    error_details.append(error_msg)
                    logger.error(f"[#file_ops]âŒ {error_msg}")
                    # æ›´æ–°å½“å‰æ–‡ä»¶è¿›åº¦ä¸ºé”™è¯¯
                    logger.info(f"[#path_progress]å¤„ç†æ–‡ä»¶: {os.path.basename(archive)} (é”™è¯¯)")
                
                # æ›´æ–°å…¨å±€è¿›åº¦
                completed = success_count + error_count
                progress = (completed / total_count) * 100
                logger.info(f"[@global_progress]æ€»ä»»åŠ¡è¿›åº¦ ({completed}/{total_count}) {progress:.1f}%")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        logger.info(f"[#sys_log]å¤„ç†å®Œæˆ âœ…æˆåŠŸ: {success_count} âŒå¤±è´¥: {error_count} æ€»æ•°: {total_count}")
        
        # å¦‚æœæœ‰é”™è¯¯ï¼Œè¾“å‡ºè¯¦ç»†ä¿¡æ¯
        if error_details:
            logger.info("[#sys_log]é”™è¯¯è¯¦æƒ…:")
            for i, error in enumerate(error_details, 1):
                logger.info(f"[#sys_log]{i}. {error}")
        
        return True
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[#sys_log]ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}\n{error_trace}")
        return False

def get_mode_config():
    """è·å–æ¨¡å¼é…ç½®"""
    return {
        'use_debugger': True,
        'use_tui': True,
        'debug_config': {
            'base_modes': {
                "1": {
                    "name": "å»æ°´å°æ¨¡å¼",
                    "description": "æ£€æµ‹å¹¶åˆ é™¤å¸¦æ°´å°çš„å›¾ç‰‡",
                    "base_args": ["-ht", "--duplicate-filter-mode", "watermark"],
                    "default_params": {
                        "ht": "16",
                        "front_n": "3",
                        "back_n": "0"
                    }
                },
                "2": {
                    "name": "å»æ±‰åŒ–æ¨¡å¼",
                    "description": "å¤„ç†å‰åNå¼ å›¾ç‰‡å¹¶ä½¿ç”¨å“ˆå¸Œå»é‡",
                    "base_args": ["-ht", "-fn", "-bn"],
                    "default_params": {
                        "ht": "16",
                        "front_n": "3",
                        "back_n": "5"
                    }
                }
            },
            'param_options': {
                "ht": {"name": "æ±‰æ˜è·ç¦»é˜ˆå€¼", "arg": "-ht", "default": "16", "type": int},
                "front_n": {"name": "å‰Nå¼ æ•°é‡", "arg": "-fn", "default": "3", "type": int},
                "back_n": {"name": "åNå¼ æ•°é‡", "arg": "-bn", "default": "5", "type": int},
                "c": {"name": "ä»å‰ªè´´æ¿è¯»å–", "arg": "-c", "is_flag": True},
                "dfm": {"name": "é‡å¤è¿‡æ»¤æ¨¡å¼", "arg": "--duplicate-filter-mode", "default": "quality", "type": str}
            }
        },
        'tui_config': {
            'checkbox_options': [
                ("ä»å‰ªè´´æ¿è¯»å–", "clipboard", "-c"),
            ],
            'input_options': [
                ("æ±‰æ˜è·ç¦»é˜ˆå€¼", "hamming_threshold", "-ht", "16", "è¾“å…¥æ•°å­—(é»˜è®¤16)"),
                ("å‰Nå¼ æ•°é‡", "front_n", "-fn", "3", "è¾“å…¥æ•°å­—(é»˜è®¤3)"),
                ("åNå¼ æ•°é‡", "back_n", "-bn", "5", "è¾“å…¥æ•°å­—(é»˜è®¤5)"),
                ("è§£å‹èŒƒå›´", "extract_range", "-er", "0:3", "æ ¼å¼: start:end"),
                ("å“ˆå¸Œæ–‡ä»¶è·¯å¾„", "hash_file", "-hf", "", "è¾“å…¥å“ˆå¸Œæ–‡ä»¶è·¯å¾„(å¯é€‰)"),
                ("é‡å¤è¿‡æ»¤æ¨¡å¼", "duplicate_filter_mode", "--duplicate-filter-mode", "quality", "quality/watermark/hash"),
            ],
            'preset_configs': {
                "å»æ°´å°æ¨¡å¼": {
                    "description": "æ£€æµ‹å¹¶åˆ é™¤å¸¦æ°´å°çš„å›¾ç‰‡",
                    "checkbox_options": ["clipboard"],
                    "input_values": {
                        "hamming_threshold": "16",
                        "front_n": "3",
                        "back_n": "0",
                        "duplicate_filter_mode": "watermark"
                    }
                },
                "å»æ±‰åŒ–æ¨¡å¼": {
                    "description": "å¤„ç†å‰åNå¼ å›¾ç‰‡å¹¶ä½¿ç”¨å“ˆå¸Œå»é‡",
                    "checkbox_options": ["clipboard"],
                    "input_values": {
                        "hamming_threshold": "16",
                        "front_n": "3",
                        "back_n": "5",
                        "duplicate_filter_mode": "hash"
                    }
                }
            }
        }
    }

# è°ƒè¯•æ¨¡å¼å¼€å…³

if __name__ == '__main__':
    mode_manager = create_mode_manager(
        config=get_mode_config(),
        cli_parser_setup=setup_cli_parser,
        application_runner=run_application
    )
    
    if DEBUG_MODE:
        success = mode_manager.run_debug()
    elif len(sys.argv) > 1:
        success = mode_manager.run_cli()
    else:
        success = mode_manager.run_tui()
    
    if not success:
        sys.exit(1) 

def load_processed_files(paths_file: str) -> Set[str]:
    """
    ä»è·¯å¾„æ–‡ä»¶åŠ è½½å·²å¤„ç†çš„ç›®å½•è®°å½•
    
    Args:
        paths_file: è·¯å¾„æ–‡ä»¶
        
    Returns:
        Set[str]: å·²å¤„ç†ç›®å½•è·¯å¾„çš„é›†åˆ
    """
    processed_dirs = set()
    try:
        if not os.path.exists(paths_file):
            logger.warning(f"[#sys_log]è·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {paths_file}")
            return processed_dirs
            
        with open(paths_file, 'r', encoding='utf-8') as f:
            processed_dirs = {line.strip() for line in f if line.strip()}
            
        logger.info(f"[#sys_log]ä»æ–‡ä»¶åŠ è½½äº† {len(processed_dirs)} ä¸ªå·²å¤„ç†ç›®å½•è®°å½•")
        return processed_dirs
        
    except Exception as e:
        logger.error(f"[#sys_log]åŠ è½½å¤„ç†è®°å½•å¤±è´¥: {e}")
        return set() 