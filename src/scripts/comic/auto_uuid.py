import os
import uuid
import json
import yaml
import time
import subprocess
import difflib
import shutil
import logging
import sys
import threading
import argparse
import multiprocessing
from pathlib import Path
from datetime import datetime
from nanoid import generate
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import pyperclip
from colorama import init, Fore, Style
import win32file
import win32con
import numpy as np
from nodes.record.logger_config import setup_logger
from nodes.tui.textual_preset import create_config_app
from nodes.tui.textual_logger import TextualLoggerManager
import orjson  # ä½¿ç”¨orjsonè¿›è¡Œæ›´å¿«çš„JSONå¤„ç†
import zipfile
from typing import Dict, Any, Optional, List

# å®šä¹‰æ—¥å¿—å¸ƒå±€é…ç½®
TEXTUAL_LAYOUT = {
    "current_stats": {
        "ratio": 2,
        "title": "ğŸ“Š æ€»ä½“è¿›åº¦",
        "style": "lightyellow"
    },
    "current_progress": {
        "ratio": 2,
        "title": "ğŸ”„ å½“å‰è¿›åº¦",
        "style": "lightcyan"
    },
    "process": {
        "ratio": 3,
        "title": "ğŸ“ å¤„ç†æ—¥å¿—",
        "style": "lightpink"
    },
    "update": {
        "ratio": 2,
        "title": "â„¹ï¸ æ›´æ–°æ—¥å¿—",
        "style": "lightblue"
    }
}

# åˆå§‹åŒ–æ—¥å¿—é…ç½®
config = {
    'script_name': 'comic_auto_uuid',
    'console_enabled': False
}
logger, config_info = setup_logger(config)

def init_TextualLogger():
    TextualLoggerManager.set_layout(TEXTUAL_LAYOUT, config_info['log_file'])

# åˆå§‹åŒ– colorama
init()

class JsonHandler:
    """JSONæ–‡ä»¶å¤„ç†ç±»"""
    
    @staticmethod
    def load(file_path: str) -> Dict[str, Any]:
        """å¿«é€ŸåŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(file_path, 'rb') as f:
                return orjson.loads(f.read())
        except Exception as e:
            logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {}
    
    @staticmethod
    def save(file_path: str, data: Dict[str, Any]) -> bool:
        """å¿«é€Ÿä¿å­˜JSONæ–‡ä»¶"""
        temp_path = f"{file_path}.tmp"
        try:
            # ä½¿ç”¨orjsonè¿›è¡Œå¿«é€Ÿåºåˆ—åŒ–
            json_bytes = orjson.dumps(
                data,
                option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS | orjson.OPT_SERIALIZE_NUMPY
            )
            
            with open(temp_path, 'wb') as f:
                f.write(json_bytes)
            
            if os.path.exists(file_path):
                os.replace(temp_path, file_path)
            else:
                os.rename(temp_path, file_path)
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False

    @staticmethod
    def convert_yaml_to_json(yaml_data: list) -> Dict[str, Any]:
        """å°†YAMLæ•°æ®è½¬æ¢ä¸ºæ–°çš„JSONæ ¼å¼"""
        json_data = {
            "timestamps": {}
        }
        
        for record in yaml_data:
            timestamp = record.get('Timestamp', '')
            if not timestamp:
                continue
                
            json_data["timestamps"][timestamp] = {
                "archive_name": record.get('ArchiveName', ''),
                "artist_name": record.get('ArtistName', ''),
                "relative_path": record.get('RelativePath', '')
            }
        
        return json_data

    @staticmethod
    def check_and_update_record(json_content: Dict[str, Any], archive_name: str, artist_name: str, relative_path: str, timestamp: str) -> bool:
        """æ£€æŸ¥å¹¶æ›´æ–°JSONè®°å½•
        
        Returns:
            bool: Trueè¡¨ç¤ºéœ€è¦æ›´æ–°ï¼ŒFalseè¡¨ç¤ºæ— éœ€æ›´æ–°
        """
        if "timestamps" not in json_content:
            return True
            
        latest_record = None
        if json_content["timestamps"]:
            latest_timestamp = max(json_content["timestamps"].keys())
            latest_record = json_content["timestamps"][latest_timestamp]
            
        if not latest_record:
            return True
            
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        need_update = False
        if latest_record.get("archive_name") != archive_name:
            need_update = True
        if latest_record.get("artist_name") != artist_name:
            need_update = True
        if latest_record.get("relative_path") != relative_path:
            need_update = True
            
        return need_update

    @staticmethod
    def update_record(json_content: Dict[str, Any], archive_name: str, artist_name: str, relative_path: str, timestamp: str) -> Dict[str, Any]:
        """æ›´æ–°JSONè®°å½•"""
        json_content["timestamps"][timestamp] = {
            "archive_name": archive_name,
            "artist_name": artist_name,
            "relative_path": relative_path
        }
        return json_content

class ArchiveHandler:
    """å‹ç¼©åŒ…å¤„ç†ç±»"""
    
    @staticmethod
    def check_archive_integrity(archive_path: str) -> bool:
        """æ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            
        Returns:
            bool: å‹ç¼©åŒ…æ˜¯å¦å®Œæ•´
        """
        try:
            # å°è¯•ä½¿ç”¨zipfile
            try:
                with zipfile.ZipFile(archive_path, 'r') as zf:
                    # æµ‹è¯•å‹ç¼©åŒ…å®Œæ•´æ€§
                    if zf.testzip() is not None:
                        logger.warning(f"[#process]å‹ç¼©åŒ…æŸå: {os.path.basename(archive_path)}")
                        return False
                    return True
            except zipfile.BadZipFile:
                # å¦‚æœä¸æ˜¯zipæ–‡ä»¶ï¼Œä½¿ç”¨7zæµ‹è¯•
                startupinfo = None
                if os.name == 'nt':
                    startupinfo = subprocess.STARTUPINFO()
                    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
                
                result = subprocess.run(
                    ['7z', 't', archive_path],
                    capture_output=True,
                    text=True,
                    encoding='gbk',
                    errors='ignore',
                    startupinfo=startupinfo,
                    check=False
                )
                
                if result.returncode != 0:
                    logger.warning(f"[#process]å‹ç¼©åŒ…æŸå: {os.path.basename(archive_path)}")
                    return False
                return True
                
        except Exception as e:
            logger.error(f"[#process]æ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§å¤±è´¥: {str(e)}")
            return False
    
    @staticmethod
    def load_yaml_uuid_from_archive(archive_path: str) -> Optional[str]:
        """ä»å‹ç¼©åŒ…ä¸­åŠ è½½YAMLæ–‡ä»¶çš„UUID"""
        # é¦–å…ˆæ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§
        if not ArchiveHandler.check_archive_integrity(archive_path):
            return None
            
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for name in zf.namelist():
                    if name.endswith('.yaml'):
                        return os.path.splitext(name)[0]
        except zipfile.BadZipFile:
            # å¦‚æœä¸æ˜¯zipæ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨7z
            return ArchiveHandler._load_uuid_from_7z(archive_path, '.yaml')
        except Exception as e:
            logger.error(f"[#process]è¯»å–å‹ç¼©åŒ…å¤±è´¥: {archive_path}")
        return None
    
    @staticmethod
    def load_json_uuid_from_archive(archive_path: str) -> Optional[str]:
        """ä»å‹ç¼©åŒ…ä¸­åŠ è½½JSONæ–‡ä»¶çš„UUID"""
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for name in zf.namelist():
                    if name.endswith('.json'):
                        return os.path.splitext(name)[0]
        except zipfile.BadZipFile:
            # å¦‚æœä¸æ˜¯zipæ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨7z
            return ArchiveHandler._load_uuid_from_7z(archive_path, '.json')
        except Exception as e:
            logger.error(f"è¯»å–å‹ç¼©åŒ…å¤±è´¥ {archive_path}: {e}")
        return None
    
    @staticmethod
    def _load_uuid_from_7z(archive_path: str, ext: str) -> Optional[str]:
        """ä½¿ç”¨7zå‘½ä»¤è¡Œå·¥å…·åŠ è½½UUID"""
        try:
            startupinfo = None
            if os.name == 'nt':
                startupinfo = subprocess.STARTUPINFO()
                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            
            result = subprocess.run(
                ['7z', 'l', archive_path],
                capture_output=True,
                text=True,
                encoding='gbk',
                errors='ignore',
                startupinfo=startupinfo,
                check=False
            )
            
            if result.returncode != 0:
                return None
            
            for line in result.stdout.splitlines():
                if not line.strip():
                    continue
                if line.endswith(ext):
                    return os.path.splitext(line.split()[-1])[0]
                    
        except Exception as e:
            logger.error(f"ä½¿ç”¨7zè¯»å–å‹ç¼©åŒ…å¤±è´¥ {archive_path}: {e}")
        return None
    
    @staticmethod
    def extract_yaml_from_archive(archive_path: str, yaml_uuid: str, temp_dir: str) -> Optional[str]:
        """ä»å‹ç¼©åŒ…ä¸­æå–YAMLæ–‡ä»¶
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            yaml_uuid: YAMLæ–‡ä»¶çš„UUIDï¼ˆä¸å«æ‰©å±•åï¼‰
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            
        Returns:
            Optional[str]: æå–çš„YAMLæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        yaml_path = os.path.join(temp_dir, f"{yaml_uuid}.yaml")
        
        try:
            # å°è¯•ä½¿ç”¨zipfile
            with zipfile.ZipFile(archive_path, 'r') as zf:
                zf.extract(f"{yaml_uuid}.yaml", temp_dir)
                return yaml_path
        except Exception:
            # å¦‚æœzipfileå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨7z
            try:
                subprocess.run(
                    ['7z', 'e', archive_path, f"{yaml_uuid}.yaml", f"-o{temp_dir}"],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    check=True
                )
                if os.path.exists(yaml_path):
                    return yaml_path
            except subprocess.CalledProcessError:
                logger.warning(f"[#process]æå–YAMLæ–‡ä»¶å¤±è´¥: {os.path.basename(archive_path)}")
        
        return None

    @staticmethod
    def delete_files_from_archive(archive_path: str, files_to_delete: List[str]) -> bool:
        """ä»å‹ç¼©åŒ…ä¸­åˆ é™¤æŒ‡å®šæ–‡ä»¶
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            files_to_delete: è¦åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # å°è¯•ä½¿ç”¨zipfile
            try:
                with zipfile.ZipFile(archive_path, 'a') as zf:
                    for file in files_to_delete:
                        try:
                            zf.remove(file)
                            logger.info(f"[#process]åˆ é™¤æ–‡ä»¶: {file}")
                        except KeyError:
                            pass
                return True
            except Exception:
                # å¦‚æœzipfileå¤±è´¥ï¼Œä½¿ç”¨7z
                for file in files_to_delete:
                    try:
                        subprocess.run(
                            ['7z', 'd', archive_path, file],
                            stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL,
                            check=True
                        )
                        logger.info(f"[#process]åˆ é™¤æ–‡ä»¶: {file}")
                    except subprocess.CalledProcessError:
                        continue
                return True
        except Exception as e:
            logger.error(f"[#process]åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

    @staticmethod
    def add_json_to_archive(archive_path: str, json_path: str, json_name: str) -> bool:
        """æ·»åŠ JSONæ–‡ä»¶åˆ°å‹ç¼©åŒ…
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            json_path: JSONæ–‡ä»¶è·¯å¾„
            json_name: è¦ä¿å­˜åœ¨å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶å
            
        Returns:
            bool: æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            # å°è¯•ä½¿ç”¨zipfile
            with zipfile.ZipFile(archive_path, 'a') as zf:
                zf.write(json_path, json_name)
                logger.info(f"[#process]æ·»åŠ JSONæ–‡ä»¶: {json_name}")
                return True
        except Exception:
            # å¦‚æœzipfileå¤±è´¥ï¼Œä½¿ç”¨7z
            try:
                subprocess.run(
                    ['7z', 'a', archive_path, json_path],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    check=True
                )
                logger.info(f"[#process]æ·»åŠ JSONæ–‡ä»¶: {json_name}")
                return True
            except subprocess.CalledProcessError:
                logger.error(f"[#process]æ·»åŠ JSONæ–‡ä»¶å¤±è´¥: {json_name}")
                return False

    @staticmethod
    def convert_yaml_archive_to_json(archive_path: str) -> Optional[Dict[str, Any]]:
        """è½¬æ¢å‹ç¼©åŒ…ä¸­çš„YAMLæ–‡ä»¶ä¸ºJSONæ ¼å¼"""
        try:
            # é¦–å…ˆæ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§
            if not ArchiveHandler.check_archive_integrity(archive_path):
                logger.warning(f"[#process]è·³è¿‡æŸåçš„å‹ç¼©åŒ…: {os.path.basename(archive_path)}")
                return None
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨YAMLæ–‡ä»¶
            yaml_uuid = ArchiveHandler.load_yaml_uuid_from_archive(archive_path)
            if not yaml_uuid:
                return None
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = os.path.join(os.path.dirname(archive_path), '.temp_extract')
            os.makedirs(temp_dir, exist_ok=True)
            
            try:
                # 1. æå–YAMLæ–‡ä»¶
                yaml_path = ArchiveHandler.extract_yaml_from_archive(archive_path, yaml_uuid, temp_dir)
                if not yaml_path or not os.path.exists(yaml_path):
                    logger.error(f"[#process]æ— æ³•æå–YAMLæ–‡ä»¶: {os.path.basename(archive_path)}")
                    return None
                
                # 2. è¯»å–å¹¶è½¬æ¢YAMLæ•°æ®
                with open(yaml_path, 'r', encoding='utf-8') as f:
                    yaml_data = yaml.safe_load(f)
                
                # 3. æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåJSONæ–‡ä»¶
                json_files = []
                try:
                    with zipfile.ZipFile(archive_path, 'r') as zf:
                        json_files = [f for f in zf.namelist() if f.endswith('.json')]
                except Exception:
                    # å¦‚æœzipfileå¤±è´¥ï¼Œä½¿ç”¨7zåˆ—å‡ºæ–‡ä»¶
                    try:
                        result = subprocess.run(
                            ['7z', 'l', archive_path],
                            capture_output=True,
                            text=True,
                            encoding='gbk',
                            errors='ignore',
                            check=True
                        )
                        if result.returncode == 0:
                            json_files = [line.split()[-1] for line in result.stdout.splitlines() 
                                        if line.strip() and line.endswith('.json')]
                    except subprocess.CalledProcessError:
                        pass
                
                # å¦‚æœå­˜åœ¨JSONæ–‡ä»¶ï¼Œåˆ é™¤å®ƒä»¬å¹¶ç”Ÿæˆæ–°çš„UUID
                if json_files:
                    logger.info(f"[#process]å‘ç°ç°æœ‰JSONæ–‡ä»¶ï¼Œå°†åˆ é™¤å¹¶ç”Ÿæˆæ–°UUID: {os.path.basename(archive_path)}")
                    ArchiveHandler.delete_files_from_archive(archive_path, json_files)
                    yaml_uuid = UuidHandler.generate_uuid(UuidHandler.load_existing_uuids())
                
                # 4. è½¬æ¢ä¸ºJSONæ ¼å¼
                json_data = JsonHandler.convert_yaml_to_json(yaml_data)
                json_data["uuid"] = yaml_uuid
                
                # 5. ä¿å­˜JSONæ–‡ä»¶
                json_path = os.path.join(temp_dir, f"{yaml_uuid}.json")
                if not JsonHandler.save(json_path, json_data):
                    logger.error(f"[#process]ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {os.path.basename(archive_path)}")
                    return None
                
                # 6. æ·»åŠ JSONåˆ°å‹ç¼©åŒ…å¹¶åˆ é™¤YAML
                if ArchiveHandler.add_json_to_archive(archive_path, json_path, f"{yaml_uuid}.json"):
                    # åˆ é™¤YAMLæ–‡ä»¶
                    ArchiveHandler.delete_files_from_archive(archive_path, [f"{yaml_uuid}.yaml"])
                    logger.info(f"[#process]âœ… YAMLè½¬æ¢å®Œæˆ: {os.path.basename(archive_path)}")
                    return json_data
                
                logger.error(f"[#process]æ›´æ–°å‹ç¼©åŒ…å¤±è´¥: {os.path.basename(archive_path)}")
                return None
                
            finally:
                shutil.rmtree(temp_dir, ignore_errors=True)
                
        except Exception as e:
            logger.error(f"[#process]è½¬æ¢å¤±è´¥ {os.path.basename(archive_path)}: {str(e)}")
            return None

# å®šä¹‰æ–‡ä»¶è·¯å¾„å’Œçº¿ç¨‹é”
# uuid_file_path = r'E:\1BACKUP\ehv\uuid.md'  # å­˜å‚¨å”¯ä¸€ UUID çš„ Markdown æ–‡ä»¶
uuid_lock = threading.Lock()  # ç”¨äºä¿æŠ¤UUIDæ–‡ä»¶æ“ä½œçš„çº¿ç¨‹é”

class FastUUIDLoader:
    def __init__(self, yaml_path):
        self.yaml_path = yaml_path
        self.cache_path = os.path.splitext(yaml_path)[0] + '.cache'
        self._data = None
        self._index = {}  # åˆå§‹åŒ–ç´¢å¼•å­—å…¸
        self._lock = threading.Lock()
        
        # åˆå§‹åŒ–è¿›åº¦å±æ€§
        self.progress = {
            'total_steps': 4,
            'current_step': 0,
            'message': 'åˆå§‹åŒ–ä¸­',
            'percentage': 0.0,
            'timestamp': time.time()
        }
        
        # è‡ªåŠ¨æ£€æµ‹å¹¶ç”Ÿæˆä¼˜åŒ–æ ¼å¼
        if not self._check_cache_valid():
            self._build_cache()
    
    def _check_cache_valid(self):
        """æ ¡éªŒç¼“å­˜æœ‰æ•ˆæ€§"""
        if not os.path.exists(self.cache_path):
            return False
        # æ ¡éªŒæ—¶é—´æˆ³å’Œå¤§å°
        yaml_mtime = os.path.getmtime(self.yaml_path)
        cache_mtime = os.path.getmtime(self.cache_path)
        return yaml_mtime <= cache_mtime
    
    def _build_cache(self):
        """ä¿®å¤ç´¢å¼•åˆå§‹åŒ–é—®é¢˜"""
        with self._lock:
            try:
                # åˆå§‹åŒ–ç´¢å¼•
                self._index = {}
                
                # å¢åŠ é˜¶æ®µæ ‡è¯†
                self._update_progress("å¼€å§‹è§£æYAMLæ–‡ä»¶", 5)
                
                # ä½¿ç”¨æ›´é«˜æ•ˆçš„è§£ææ–¹å¼
                with open(self.yaml_path, 'rb') as f:
                    data = yaml.load(f, Loader=yaml.CSafeLoader)
                
                # æ·»åŠ æ•°æ®æ ¡éªŒ
                if not data or not isinstance(data, list):
                    raise ValueError("æ— æ•ˆçš„YAMLæ•°æ®æ ¼å¼")
                
                self._update_progress("æ•°æ®æ ¡éªŒé€šè¿‡", 20)
                
                # åˆ†æ‰¹æ¬¡æ„å»ºç´¢å¼•
                batch_size = 5000
                total = len(data)
                self._update_progress("å¼€å§‹æ„å»ºç´¢å¼•", 30)
                
                for i in range(0, total, batch_size):
                    batch = data[i:i+batch_size]
                    for j, record in enumerate(batch):
                        uuid = record.get('UUID')
                        if uuid:
                            self._index[uuid] = i + j
                    # å®æ—¶æ›´æ–°è¿›åº¦
                    progress = 30 + 60 * (i + batch_size) / total
                    self._update_progress(f"ç´¢å¼•æ„å»ºä¸­ ({i+batch_size}/{total})", min(90, progress))
                
                self._update_progress("å†™å…¥ç¼“å­˜æ–‡ä»¶", 95)
                with open(self.cache_path, 'wb') as f:
                    np.savez_compressed(f, data=data, index=self._index)
                
                self._update_progress("å®Œæˆ", 100)
                
            except Exception as e:
                self._index = None  # æ„å»ºå¤±è´¥æ—¶é‡ç½®ç´¢å¼•
                raise

    def _update_progress(self, message, percentage):
        """æ›´æ–°è¿›åº¦ä¿¡æ¯"""
        self.progress.update({
            'message': message,
            'percentage': min(100, max(0, percentage)),
            'timestamp': time.time()
        })
        logger.info(f"[ç¼“å­˜æ„å»º] {message} - è¿›åº¦: {self.progress['percentage']:.1f}%")

    def get_loading_progress(self):
        """è·å–å½“å‰åŠ è½½è¿›åº¦"""
        return self.progress
    
    def _load_cache(self):
        """åŠ è½½ä¼˜åŒ–æ ¼å¼"""
        with open(self.cache_path, 'rb') as f:
            cache = np.load(f, allow_pickle=True)
            self._data = cache['data'].tolist()
            self._index = cache['index'].item() if 'index' in cache else {}
    
    def get_uuids(self):
        """è·å–UUIDé›†åˆ"""
        if self._index is None:
            self._load_cache()
        return set(self._index.keys())
    
    def get_record(self, uuid):
        """å¿«é€ŸæŸ¥è¯¢è®°å½•"""
        if self._index is None:
            self._load_cache()
        return self._data[self._index[uuid]]

def repair_uuid_records(uuid_record_path):
    """ä¿®å¤æŸåçš„UUIDè®°å½•æ–‡ä»¶ã€‚"""
    backup_path = f"{uuid_record_path}.bak"
    
    # å¦‚æœå­˜åœ¨å¤‡ä»½æ–‡ä»¶ï¼Œå°è¯•ä»å¤‡ä»½æ¢å¤
    if os.path.exists(backup_path):
        try:
            with open(backup_path, 'r', encoding='utf-8') as file:
                records = yaml.safe_load(file) or []
                if isinstance(records, list):
                    logger.info("[#process]ä»å¤‡ä»½æ–‡ä»¶æ¢å¤è®°å½•æˆåŠŸ")
                    return records
        except Exception:
            logger.error("[#process]ä»å¤‡ä»½æ–‡ä»¶æ¢å¤è®°å½•å¤±è´¥")
            pass
    
    # å°è¯•ä¿®å¤åŸæ–‡ä»¶
    try:
        with open(uuid_record_path, 'r', encoding='utf-8', errors='ignore') as file:
            content = file.read()
            # å°è¯•è§£ææ¯ä¸ªè®°å½•
            records = []
            current_record = {}
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    if current_record:
                        records.append(current_record)
                        current_record = {}
                    continue
                
                if line.startswith('- ') or line.startswith('UUID:'):
                    if current_record:
                        records.append(current_record)
                    current_record = {}
                
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip('- ').strip()
                    value = value.strip()
                    if key and value:
                        current_record[key] = value
            
            if current_record:
                records.append(current_record)
            
            # éªŒè¯è®°å½•
            valid_records = []
            for record in records:
                if 'UUID' in record:
                    valid_records.append(record)
            
            logger.info(f"[#process]æˆåŠŸä¿®å¤è®°å½•æ–‡ä»¶ï¼Œæ¢å¤äº† {len(valid_records)} æ¡è®°å½•")
            return valid_records
    except Exception as e:
        logger.error(f"[#process]ä¿®å¤UUIDè®°å½•æ–‡ä»¶å¤±è´¥: {e}")
        return []

def load_existing_uuids():
    """ä»JSONè®°å½•ä¸­åŠ è½½ç°æœ‰UUID"""
    logger.info("[#current_stats]ğŸ” å¼€å§‹åŠ è½½ç°æœ‰UUID...")
    start_time = time.time()
    
    json_record_path = r'E:\1BACKUP\ehv\uuid\uuid_records.json'
    if not os.path.exists(json_record_path):
        return set()
        
    try:
        with open(json_record_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
        uuids = set(records.keys())
        
        elapsed = time.time() - start_time
        logger.info(f"[#current_stats]âœ… åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(uuids)} ä¸ªUUIDï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
        return uuids
        
    except Exception as e:
        logger.error(f"[#process]åŠ è½½UUIDè®°å½•å¤±è´¥: {e}")
        return set()

def add_uuid_to_file(uuid, timestamp, archive_name, artist_name, relative_path=None):
    """å°†ç”Ÿæˆçš„ UUID æ·»åŠ åˆ°è®°å½•æ–‡ä»¶ä¸­ã€‚"""
    uuid_record_path = r'E:\1BACKUP\ehv\uuid\uuid_records.yaml'
    os.makedirs(os.path.dirname(uuid_record_path), exist_ok=True)
    
    # è¯»å–ç°æœ‰è®°å½•
    records = []
    if os.path.exists(uuid_record_path):
        try:
            with open(uuid_record_path, 'r', encoding='utf-8') as file:
                records = yaml.safe_load(file) or []
        except Exception as e:
            print(f"è¯»å–è®°å½•æ–‡ä»¶å¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
            records = repair_uuid_records(uuid_record_path) or []
    
    # æ·»åŠ æ–°è®°å½•
    record = {
        'UUID': uuid,
        'CreatedAt': timestamp,
        'ArchiveName': archive_name,
        'ArtistName': artist_name,
        'LastModified': timestamp,
        'LastPath': relative_path or os.path.join(artist_name, archive_name) if artist_name else archive_name
    }
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥UUIDçš„è®°å½•
    for existing_record in records:
        if existing_record.get('UUID') == uuid:  # ä½¿ç”¨get()é¿å…KeyError
            existing_record.update({
                'LastModified': timestamp,
                'ArchiveName': archive_name,
                'ArtistName': artist_name,
                'LastPath': relative_path or os.path.join(artist_name, archive_name) if artist_name else archive_name
            })
            break
    else:
        records.append(record)
    
    # å†™å…¥è®°å½•ï¼ˆä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿å¹¶å‘å®‰å…¨ï¼‰
    with uuid_lock:
        try:
            # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_path = f"{uuid_record_path}.tmp"
            with open(temp_path, 'w', encoding='utf-8') as file:
                yaml.dump(records, file, allow_unicode=True, sort_keys=False)
            
            # éªŒè¯ä¸´æ—¶æ–‡ä»¶
            with open(temp_path, 'r', encoding='utf-8') as file:
                yaml.safe_load(file)
            
            # åˆ›å»ºå¤‡ä»½
            if os.path.exists(uuid_record_path):
                backup_path = f"{uuid_record_path}.bak"
                shutil.copy2(uuid_record_path, backup_path)
            
            # æ›¿æ¢åŸæ–‡ä»¶
            os.replace(temp_path, uuid_record_path)
            
        except Exception as e:
            print(f"å†™å…¥UUIDè®°å½•æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            if os.path.exists(temp_path):
                os.remove(temp_path)

def generate_uuid(existing_uuids):
    """ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ 16 ä½ UUIDã€‚"""
    while True:
        new_uuid = generate(size=16)  # ç”Ÿæˆ 16 ä½çš„ UUID
        if new_uuid not in existing_uuids:
            return new_uuid

class PathHandler:
    """è·¯å¾„å¤„ç†ç±»"""
    
    @staticmethod
    def get_artist_name(target_directory: str, archive_path: str, mode: str = 'multi') -> str:
        """ä»å‹ç¼©æ–‡ä»¶è·¯å¾„ä¸­æå–è‰ºæœ¯å®¶åç§°
        
        Args:
            target_directory: ç›®æ ‡ç›®å½•è·¯å¾„
            archive_path: å‹ç¼©æ–‡ä»¶è·¯å¾„
            mode: å¤„ç†æ¨¡å¼ï¼Œ'multi'è¡¨ç¤ºå¤šäººæ¨¡å¼ï¼Œ'single'è¡¨ç¤ºå•äººæ¨¡å¼
            
        Returns:
            str: è‰ºæœ¯å®¶åç§°
        """
        if mode == 'single':
            # å•äººæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ç›®æ ‡ç›®å½•çš„æœ€åä¸€ä¸ªæ–‡ä»¶å¤¹åä½œä¸ºç”»å¸ˆå
            return Path(target_directory).name
        else:
            # å¤šäººæ¨¡å¼ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„çš„ç¬¬ä¸€çº§å­æ–‡ä»¶å¤¹åä½œä¸ºç”»å¸ˆå
            try:
                # å°†è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                archive_path = Path(archive_path)
                target_path = Path(target_directory)
                
                # è·å–ç›¸å¯¹äºç›®æ ‡ç›®å½•çš„è·¯å¾„
                relative_path = archive_path.relative_to(target_path)
                
                # è·å–ç¬¬ä¸€çº§å­æ–‡ä»¶å¤¹å
                if len(relative_path.parts) > 0:
                    return relative_path.parts[0]
                
                logger.warning(f"[#process]æ— æ³•ä»è·¯å¾„æå–ç”»å¸ˆå: {archive_path}")
                return ""
                
            except Exception as e:
                logger.error(f"[#process]æå–ç”»å¸ˆåå¤±è´¥: {str(e)}")
                return ""
    
    @staticmethod
    def get_relative_path(target_directory: str, archive_path: str) -> str:
        """è·å–ç›¸å¯¹è·¯å¾„
        
        Args:
            target_directory: ç›®æ ‡ç›®å½•è·¯å¾„
            archive_path: å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: ç›¸å¯¹è·¯å¾„ï¼Œä¸åŒ…å«æ–‡ä»¶å
        """
        try:
            # å°†è·¯å¾„è½¬æ¢ä¸ºPathå¯¹è±¡å¹¶è§„èŒƒåŒ–
            archive_path = Path(archive_path).resolve()
            target_path = Path(target_directory).resolve()
            
            # è·å–ç›¸å¯¹è·¯å¾„
            relative_path = archive_path.relative_to(target_path)
            
            # å¦‚æœæ˜¯ç›´æ¥åœ¨ç›®æ ‡ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œè¿”å›"."
            if not relative_path.parent.parts:
                return "."
                
            # è¿”å›çˆ¶ç›®å½•çš„ç›¸å¯¹è·¯å¾„ï¼ˆä¸åŒ…å«æ–‡ä»¶åï¼‰
            return str(relative_path.parent)
            
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œè®°å½•é”™è¯¯ä½†è¿”å›ä¸€ä¸ªå®‰å…¨çš„é»˜è®¤å€¼
            logger.error(f"[#process]è·å–ç›¸å¯¹è·¯å¾„å¤±è´¥ ({archive_path}): {str(e)}")
            return "."
    
    @staticmethod
    def get_uuid_path(uuid_directory: str, timestamp: str) -> str:
        """æ ¹æ®æ—¶é—´æˆ³ç”ŸæˆæŒ‰å¹´æœˆæ—¥åˆ†å±‚çš„UUIDæ–‡ä»¶è·¯å¾„"""
        date = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
        year = str(date.year)
        month = f"{date.month:02d}"
        day = f"{date.day:02d}"
        
        # åˆ›å»ºå¹´æœˆæ—¥å±‚çº§ç›®å½•
        year_dir = os.path.join(uuid_directory, year)
        month_dir = os.path.join(year_dir, month)
        day_dir = os.path.join(month_dir, day)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(day_dir, exist_ok=True)
        
        return day_dir
    
    @staticmethod
    def get_short_path(long_path: str) -> str:
        """å°†é•¿è·¯å¾„è½¬æ¢ä¸ºçŸ­è·¯å¾„æ ¼å¼"""
        try:
            import win32api
            return win32api.GetShortPathName(long_path)
        except ImportError:
            return long_path

class UuidHandler:
    """UUIDå¤„ç†ç±»"""
    
    @staticmethod
    def generate_uuid(existing_uuids: set) -> str:
        """ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„16ä½UUID"""
        while True:
            new_uuid = generate(size=16)
            if new_uuid not in existing_uuids:
                return new_uuid
    
    @staticmethod
    def load_existing_uuids() -> set:
        """ä»JSONè®°å½•ä¸­åŠ è½½ç°æœ‰UUID"""
        logger.info("[#current_stats]ğŸ” å¼€å§‹åŠ è½½ç°æœ‰UUID...")
        start_time = time.time()
        
        json_record_path = r'E:\1BACKUP\ehv\uuid\uuid_records.json'
        if not os.path.exists(json_record_path):
            return set()
            
        try:
            with open(json_record_path, 'r', encoding='utf-8') as f:
                records = json.load(f)
            uuids = set(records.keys())
            
            elapsed = time.time() - start_time
            logger.info(f"[#current_stats]âœ… åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(uuids)} ä¸ªUUIDï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
            return uuids
            
        except Exception as e:
            logger.error(f"[#process]åŠ è½½UUIDè®°å½•å¤±è´¥: {e}")
            return set()

class YamlHandler:
    """YAMLæ–‡ä»¶å¤„ç†ç±»"""
    
    @staticmethod
    def read_yaml(yaml_path: str) -> list:
        """è¯»å–YAMLæ–‡ä»¶å†…å®¹ï¼Œå¦‚æœæ–‡ä»¶æŸååˆ™å°è¯•ä¿®å¤"""
        if not os.path.exists(yaml_path):
            return []
            
        try:
            with open(yaml_path, 'r', encoding='utf-8') as file:
                data = yaml.safe_load(file)
                if not isinstance(data, list):
                    data = [data] if data is not None else []
                return data
        except yaml.YAMLError as e:
            logger.error(f"YAMLæ–‡ä»¶ {yaml_path} å·²æŸåï¼Œå°è¯•ä¿®å¤...")
            return YamlHandler.repair_yaml_file(yaml_path)
        except Exception as e:
            logger.error(f"è¯»å–YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
            return []
    
    @staticmethod
    def write_yaml(yaml_path: str, data: list) -> bool:
        """å°†æ•°æ®å†™å…¥YAMLæ–‡ä»¶ï¼Œç¡®ä¿å†™å…¥å®Œæ•´æ€§"""
        temp_path = yaml_path + '.tmp'
        try:
            with open(temp_path, 'w', encoding='utf-8') as file:
                yaml.dump(data, file, allow_unicode=True)
            
            try:
                with open(temp_path, 'r', encoding='utf-8') as file:
                    yaml.safe_load(file)
            except yaml.YAMLError:
                logger.error(f"å†™å…¥çš„YAMLæ–‡ä»¶éªŒè¯å¤±è´¥: {yaml_path}")
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                return False
                
            if os.path.exists(yaml_path):
                os.replace(temp_path, yaml_path)
            else:
                os.rename(temp_path, yaml_path)
            return True
                
        except Exception as e:
            logger.error(f"å†™å…¥YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False
    
    @staticmethod
    def repair_yaml_file(yaml_path: str) -> list:
        """ä¿®å¤æŸåçš„YAMLæ–‡ä»¶"""
        try:
            with open(yaml_path, 'r', encoding='utf-8') as file:
                lines = file.readlines()

            if not lines:
                return []

            valid_data = []
            current_record = []
            
            for line in lines:
                current_record.append(line)
                if line.strip() == '' or line == lines[-1]:
                    try:
                        record_str = ''.join(current_record)
                        parsed_data = yaml.safe_load(record_str)
                        if isinstance(parsed_data, list):
                            valid_data.extend(parsed_data)
                        elif parsed_data is not None:
                            valid_data.append(parsed_data)
                    except yaml.YAMLError:
                        pass
                    current_record = []

            if not valid_data:
                return []

            YamlHandler.write_yaml(yaml_path, valid_data)
            return valid_data

        except Exception as e:
            logger.error(f"ä¿®å¤YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
            return []

class FileSystemHandler:
    """æ–‡ä»¶ç³»ç»Ÿæ“ä½œç±»"""
    
    @staticmethod
    def warm_up_cache(target_directory: str, max_workers: int = 32) -> None:
        """å¹¶è¡Œé¢„çƒ­ç³»ç»Ÿç¼“å­˜"""
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹é¢„çƒ­ç³»ç»Ÿç¼“å­˜")
        
        # é¦–å…ˆè®¡ç®—æ€»æ–‡ä»¶æ•°
        total_files = 0
        for root, _, files in os.walk(target_directory):
            total_files += sum(1 for file in files if file.endswith(('.zip', '.rar', '.7z')))
        
        logger.info("[#current_progress]æ‰«ææ–‡ä»¶ä¸­...")
        archive_files = []
        current_count = 0
        for root, _, files in os.walk(target_directory):
            for file in files:
                if file.endswith(('.zip', '.rar', '.7z')):
                    archive_files.append(os.path.join(root, file))
                    current_count += 1
                    logger.info(f"[@current_progress]å·²æ‰«æ {current_count}/{total_files} ä¸ªæ–‡ä»¶ ({(current_count/total_files*100):.1f}%)")
        
        logger.info(f"[#current_stats]ğŸ“Š æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶å¾…é¢„çƒ­")
        
        def read_file_header_with_progress(file_path):
            try:
                handle = win32file.CreateFile(
                    file_path,
                    win32con.GENERIC_READ,
                    win32con.FILE_SHARE_READ | win32con.FILE_SHARE_WRITE,
                    None,
                    win32con.OPEN_EXISTING,
                    win32con.FILE_FLAG_SEQUENTIAL_SCAN,
                    None
                )
                try:
                    win32file.ReadFile(handle, 32)
                finally:
                    handle.Close()
                logger.info(f"[#process]âœ… å·²é¢„çƒ­: {os.path.basename(file_path)}")
            except Exception as e:
                logger.error(f"[#process]é¢„çƒ­å¤±è´¥: {os.path.basename(file_path)} - {str(e)}")

        with ThreadPoolExecutor(max_workers=128) as executor:
            futures = [executor.submit(read_file_header_with_progress, file) for file in archive_files]
            completed = 0
            for future in as_completed(futures):
                completed += 1
                logger.info(f"[@current_progress]é¢„çƒ­è¿›åº¦ {completed}/{total_files} ({(completed/total_files*100):.1f}%)")
        
        logger.info("[#current_stats]âœ¨ ç¼“å­˜é¢„çƒ­å®Œæˆ")

class ArchiveProcessor:
    """å‹ç¼©æ–‡ä»¶å¤„ç†ç±»"""
    
    def __init__(self, target_directory: str, uuid_directory: str, max_workers: int = 5):
        self.target_directory = target_directory
        self.uuid_directory = uuid_directory
        self.max_workers = max_workers
    
    def process_archives(self) -> bool:
        """å¤„ç†æ‰€æœ‰å‹ç¼©æ–‡ä»¶"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        os.makedirs(self.uuid_directory, exist_ok=True)

        logger.info("[#current_stats]ğŸ” å¼€å§‹æ‰«æå‹ç¼©æ–‡ä»¶")
        scan_task = logger.info("[#current_progress]æ‰«ææ–‡ä»¶")
        
        archive_files = []
        file_count = 0
        for root, _, files in os.walk(self.target_directory):
            for file in files:
                if file.endswith(('.zip', '.rar', '.7z')):
                    full_path = os.path.join(root, file)
                    archive_files.append((full_path, os.path.getmtime(full_path)))
                    file_count += 1
                    logger.info(f"[@current_progress]æ‰«æè¿›åº¦ ({file_count}) {(file_count/len(files)*100):.1f}%")
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        archive_files.sort(key=lambda x: x[1], reverse=True)
        archive_files = [file_path for file_path, _ in archive_files]
        
        logger.info(f"[#current_stats]ğŸ“Š å…±å‘ç° {file_count} ä¸ªå‹ç¼©æ–‡ä»¶")
        
        # åŠ è½½ç°æœ‰UUID
        logger.info("[#current_stats]ğŸ’¾ æ­£åœ¨åŠ è½½ç°æœ‰UUID...")
        existing_uuids = UuidHandler.load_existing_uuids()
        logger.info(f"[#current_stats]ğŸ“ å·²åŠ è½½ {len(existing_uuids)} ä¸ªç°æœ‰UUID")
        
        process_task = logger.info("[#current_progress]å¤„ç†å‹ç¼©æ–‡ä»¶")
        
        # æ·»åŠ è·³è¿‡è®¡æ•°å™¨
        skip_count = 0
        
        def process_with_progress(archive_path):
            nonlocal skip_count
            try:
                start_time = time.time()
                result = self.process_single_archive(archive_path, timestamp)
                
                # è®°å½•å¤„ç†æ—¶é•¿
                duration = time.time() - start_time
                if duration > 30:
                    logger.warning(f"[#process]â±ï¸ å¤„ç†æ—¶é—´è¿‡é•¿: {os.path.basename(archive_path)} è€—æ—¶{duration:.1f}ç§’")
                
                return result
            except Exception as e:
                logger.error(f"[#process]ğŸ”¥ ä¸¥é‡é”™è¯¯: {str(e)}")
                raise
        
        # ä¿®æ”¹ä»»åŠ¡åˆ†å‘æ–¹å¼
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ä½¿ç”¨æ‰¹é‡æäº¤ä»»åŠ¡
            batch_size = 100
            futures = []
            
            for i in range(0, len(archive_files), batch_size):
                batch = archive_files[i:i+batch_size]
                futures.extend(executor.submit(process_with_progress, path) for path in batch)
                
                # å®æ—¶æ˜¾ç¤ºæäº¤è¿›åº¦
                submitted = min(i + batch_size, len(archive_files))
                total_files = len(archive_files)
                logger.info(f"[@current_progress]æäº¤è¿›åº¦ ({submitted}/{total_files}) {(submitted/total_files*100):.1f}%")

            # æ·»åŠ è¶…æ—¶æœºåˆ¶
            completed = 0
            for future in as_completed(futures, timeout=300):
                try:
                    result = future.result(timeout=60)  # æ¯ä¸ªä»»åŠ¡æœ€å¤š60ç§’
                    completed += 1
                    logger.info(f"[@current_progress]å¤„ç†è¿›åº¦ ({completed}/{total_files}) {(completed/total_files*100):.1f}%")
                    if result == "SKIP_LIMIT_REACHED":
                        logger.info("[#process]â© è¾¾åˆ°è·³è¿‡é™åˆ¶ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡...")
                        for f in futures:
                            f.cancel()
                        break
                except TimeoutError:
                    logger.warning("[#process]âŒ› ä»»åŠ¡è¶…æ—¶ï¼Œå·²è·³è¿‡")
                    skip_count += 1
                except Exception as e:
                    logger.error(f"[#process]ä»»åŠ¡å¤±è´¥: {str(e)}")
                    skip_count = 0

        if skip_count >= 100:
            logger.info("[#current_stats]ğŸ”„ ç”±äºè¿ç»­è·³è¿‡æ¬¡æ•°è¾¾åˆ°100ï¼Œæå‰ç»“æŸå½“å‰é˜¶æ®µ")
        else:
            logger.info("[#current_stats]âœ¨ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ")
        
        return skip_count >= 100
    
    def process_single_archive(self, archive_path: str, timestamp: str) -> bool:
        """å¤„ç†å•ä¸ªå‹ç¼©æ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨YAMLæ–‡ä»¶å¹¶è½¬æ¢ä¸ºJSON
            json_data = ArchiveHandler.convert_yaml_archive_to_json(archive_path)
            if json_data:
                logger.info(f"[#process]æ£€æµ‹åˆ°YAMLæ–‡ä»¶: {os.path.basename(archive_path)}")
                logger.info(f"[#process]YAMLè½¬æ¢å®Œæˆ: {os.path.basename(archive_path)}")
                return True  # å¦‚æœæ˜¯YAMLè½¬æ¢æµç¨‹,å®Œæˆåç›´æ¥è¿”å›
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            artist_name = PathHandler.get_artist_name(self.target_directory, archive_path, args.mode if hasattr(args, 'mode') else 'multi')
            archive_name = os.path.basename(archive_path)
            relative_path = PathHandler.get_relative_path(self.target_directory, archive_path)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨UUID JSONæ–‡ä»¶
            json_uuid = ArchiveHandler.load_json_uuid_from_archive(archive_path)
            if json_uuid:
                # éªŒè¯JSONæ–‡ä»¶å†…å®¹
                try:
                    with zipfile.ZipFile(archive_path, 'r') as zf:
                        with zf.open(f"{json_uuid}.json") as f:
                            json_content = orjson.loads(f.read())
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬çš„UUIDè®°å½•æ–‡ä»¶
                            if "uuid" in json_content:
                                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                                if JsonHandler.check_and_update_record(json_content, archive_name, artist_name, relative_path, timestamp):
                                    logger.info(f"[#process]æ£€æµ‹åˆ°è®°å½•éœ€è¦æ›´æ–°: {os.path.basename(archive_path)}")
                                    # æ›´æ–°è®°å½•
                                    json_content = JsonHandler.update_record(json_content, archive_name, artist_name, relative_path, timestamp)
                                    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¹¶æ›´æ–°å‹ç¼©åŒ…
                                    temp_dir = os.path.join(os.path.dirname(archive_path), '.temp_update')
                                    os.makedirs(temp_dir, exist_ok=True)
                                    try:
                                        temp_json = os.path.join(temp_dir, f"{json_uuid}.json")
                                        if JsonHandler.save(temp_json, json_content):
                                            # æ›´æ–°å‹ç¼©åŒ…ä¸­çš„JSON
                                            try:
                                                with zipfile.ZipFile(archive_path, 'a') as zf:
                                                    zf.write(temp_json, f"{json_uuid}.json")
                                                logger.info(f"[#update]âœ… å·²æ›´æ–°å‹ç¼©åŒ…ä¸­çš„JSONè®°å½•: {archive_name}")
                                            except Exception:
                                                subprocess.run(
                                                    ['7z', 'u', archive_path, temp_json],
                                                    stdout=subprocess.DEVNULL,
                                                    stderr=subprocess.DEVNULL,
                                                    check=True
                                                )
                                                logger.info(f"[#update]âœ… å·²æ›´æ–°å‹ç¼©åŒ…ä¸­çš„JSONè®°å½•: {archive_name}")
                                    finally:
                                        shutil.rmtree(temp_dir, ignore_errors=True)
                                else:
                                    logger.info(f"[#process]è®°å½•æ— éœ€æ›´æ–°: {os.path.basename(archive_path)}")
                                return True
                            else:
                                logger.info(f"[#process]å‹ç¼©åŒ…ä¸­çš„JSONä¸æ˜¯UUIDè®°å½•ï¼Œå°†åˆ›å»ºæ–°è®°å½•: {os.path.basename(archive_path)}")
                except Exception:
                    logger.info(f"[#process]å‹ç¼©åŒ…ä¸­çš„JSONæ— æ³•è¯»å–æˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œå°†åˆ›å»ºæ–°è®°å½•: {os.path.basename(archive_path)}")
            
            # è·å–æˆ–åˆ›å»ºæ–°çš„UUID
            uuid_value = UuidHandler.generate_uuid(UuidHandler.load_existing_uuids())
            json_filename = f"{uuid_value}.json"
            
            logger.info(f"[#current_stats]å¤„ç†æ–‡ä»¶: {archive_name}")
            logger.info(f"[#current_stats]è‰ºæœ¯å®¶: {artist_name}")
            logger.info(f"[#current_stats]ç›¸å¯¹è·¯å¾„: {relative_path}")
            
            # è·å–æŒ‰å¹´æœˆæ—¥åˆ†å±‚çš„ç›®å½•è·¯å¾„
            day_dir = PathHandler.get_uuid_path(self.uuid_directory, timestamp)
            json_path = os.path.join(day_dir, json_filename)
            
            # å‡†å¤‡æ–°çš„è®°å½•æ•°æ®
            new_record = {
                "archive_name": archive_name,
                "artist_name": artist_name,
                "relative_path": relative_path
            }
            
            # åˆ›å»ºæ–°çš„JSONæ–‡ä»¶
            json_data = {
                "uuid": uuid_value,
                "timestamps": {
                    timestamp: new_record
                }
            }
            
            # ä¿å­˜JSONæ–‡ä»¶
            if JsonHandler.save(json_path, json_data):
                logger.info(f"[#process]åˆ›å»ºæ–°JSON: {json_filename}")
                logger.info(f"[#update]âœ… å·²æ›´æ–°JSONæ–‡ä»¶: {json_filename}")
                
                # æ·»åŠ JSONåˆ°å‹ç¼©åŒ…
                try:
                    with zipfile.ZipFile(archive_path, 'a') as zf:
                        zf.write(json_path, json_filename)
                    logger.info(f"[#update]âœ… å·²æ·»åŠ JSONåˆ°å‹ç¼©åŒ…: {archive_name}")
                except Exception:
                    # å¦‚æœä¸æ˜¯zipæ–‡ä»¶ï¼Œä½¿ç”¨7z
                    subprocess.run(
                        ['7z', 'a', archive_path, json_path],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                        check=True
                    )
                    logger.info(f"[#update]âœ… å·²æ·»åŠ JSONåˆ°å‹ç¼©åŒ…: {archive_name}")
            else:
                logger.error(f"[#process]JSONæ–‡ä»¶ä¿å­˜å¤±è´¥: {archive_name}")
                
            return True

        except subprocess.CalledProcessError:
            logger.error(f"[#process]å‘ç°æŸåçš„å‹ç¼©åŒ…: {archive_path}")
            return True
        except Exception as e:
            logger.error(f"[#process]å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™ {archive_path}: {str(e)}")
            return True

def main():
    """ä¸»å‡½æ•°"""
    # å®šä¹‰å¤é€‰æ¡†é€‰é¡¹
    checkbox_options = [
        ("æ— ç”»å¸ˆæ¨¡å¼ - ä¸æ·»åŠ ç”»å¸ˆå", "no_artist", "--no-artist"),
        ("ä¿æŒæ—¶é—´æˆ³ - ä¿æŒæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´", "keep_timestamp", "--keep-timestamp", True),
        ("å¤šç”»å¸ˆæ¨¡å¼ - å¤„ç†æ•´ä¸ªç›®å½•", "multi_mode", "--mode multi"),
        ("å•ç”»å¸ˆæ¨¡å¼ - åªå¤„ç†å•ä¸ªç”»å¸ˆçš„æ–‡ä»¶å¤¹", "single_mode", "--mode single"),
        ("ä»å‰ªè´´æ¿è¯»å–è·¯å¾„", "clipboard", "-c", True),  # é»˜è®¤å¼€å¯
        ("è‡ªåŠ¨åºåˆ— - æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹", "auto_sequence", "-a"),  # æ·»åŠ åºåˆ—æ¨¡å¼é€‰é¡¹
        ("é‡ç»„UUID - æŒ‰æ—¶é—´é‡ç»„UUIDæ–‡ä»¶", "reorganize", "-r"),  # æ·»åŠ é‡ç»„é€‰é¡¹
        ("æ›´æ–°è®°å½• - æ›´æ–°UUIDè®°å½•æ–‡ä»¶", "update_records", "-u"),  # æ·»åŠ æ›´æ–°è®°å½•é€‰é¡¹
        ("è½¬æ¢YAML - è½¬æ¢ç°æœ‰YAMLåˆ°JSON", "convert_yaml", "--convert"),  # æ·»åŠ YAMLè½¬æ¢é€‰é¡¹
    ]

    # å®šä¹‰è¾“å…¥æ¡†é€‰é¡¹
    input_options = [
        ("è·¯å¾„", "path", "--path", "", "è¾“å…¥è¦å¤„ç†çš„è·¯å¾„ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤è·¯å¾„"),
    ]

    # é¢„è®¾é…ç½®
    preset_configs = {
        "æ ‡å‡†å¤šç”»å¸ˆ": {
            "description": "æ ‡å‡†å¤šç”»å¸ˆæ¨¡å¼ï¼Œä¼šæ·»åŠ ç”»å¸ˆå",
            "checkbox_options": ["keep_timestamp", "multi_mode", "clipboard"],
            "input_values": {"path": ""}
        },
        "æ ‡å‡†å•ç”»å¸ˆ": {
            "description": "æ ‡å‡†å•ç”»å¸ˆæ¨¡å¼ï¼Œä¼šæ·»åŠ ç”»å¸ˆå", 
            "checkbox_options": ["keep_timestamp", "single_mode", "clipboard"],
            "input_values": {"path": ""}
        },
        "æ— ç”»å¸ˆæ¨¡å¼": {
            "description": "ä¸æ·»åŠ ç”»å¸ˆåçš„é‡å‘½åæ¨¡å¼",
            "checkbox_options": ["no_artist", "keep_timestamp", "clipboard"],
            "input_values": {"path": ""}
        },
        "å®Œæ•´åºåˆ—": {
            "description": "æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹ï¼šUUID-JSON -> è‡ªåŠ¨æ–‡ä»¶å -> UUID-JSON",
            "checkbox_options": ["keep_timestamp", "clipboard", "auto_sequence"],
            "input_values": {"path": ""}
        },
        "UUIDæ›´æ–°": {
            "description": "é‡ç»„UUIDæ–‡ä»¶ç»“æ„å¹¶æ›´æ–°è®°å½•",
            "checkbox_options": ["reorganize", "update_records"],
            "input_values": {"path": ""}
        },
        "å®Œæ•´ç»´æŠ¤": {
            "description": "æ‰§è¡Œå®Œæ•´åºåˆ—å¹¶æ›´æ–°UUIDè®°å½•",
            "checkbox_options": ["keep_timestamp", "clipboard", "auto_sequence", "reorganize", "update_records"],
            "input_values": {"path": ""}
        },
        "YAMLè½¬æ¢": {
            "description": "è½¬æ¢ç°æœ‰YAMLæ–‡ä»¶åˆ°JSONæ ¼å¼",
            "checkbox_options": ["convert_yaml"],
            "input_values": {"path": ""}
        }
    }

    # åˆ›å»ºå¹¶è¿è¡Œé…ç½®ç•Œé¢
    app = create_config_app(
        program=__file__,
        checkbox_options=checkbox_options,
        input_options=input_options,
        title="UUID-JSON å·¥å…·",
        preset_configs=preset_configs
    )
    app.run()

def reorganize_uuid_files(uuid_directory=r'E:\1BACKUP\ehv\uuid'):
    """æ ¹æ®æœ€åä¿®æ”¹æ—¶é—´é‡æ–°ç»„ç»‡UUIDæ–‡ä»¶çš„ç›®å½•ç»“æ„"""
    logger.info("[#current_stats]ğŸ”„ å¼€å§‹é‡æ–°ç»„ç»‡UUIDæ–‡ä»¶...")
    
    json_record_path = os.path.join(uuid_directory, 'uuid_records.json')
    if not os.path.exists(json_record_path):
        logger.error("[#process]âŒ UUIDè®°å½•æ–‡ä»¶ä¸å­˜åœ¨")
        return
        
    try:
        with open(json_record_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
            
        total_records = len(records)
        processed = 0
        
        for uuid, data in records.items():
            if not data.get("timestamps"):
                continue
                
            latest_timestamp = max(data["timestamps"].keys())
            
            try:
                date = datetime.strptime(latest_timestamp, "%Y-%m-%d %H:%M:%S")
                year = str(date.year)
                month = f"{date.month:02d}"
                day = f"{date.day:02d}"
                
                year_dir = os.path.join(uuid_directory, year)
                month_dir = os.path.join(year_dir, month)
                day_dir = os.path.join(month_dir, day)
                target_path = os.path.join(day_dir, f"{uuid}.json")
                
                current_json_path = None
                for root, _, files in os.walk(uuid_directory):
                    if f"{uuid}.json" in files:
                        current_json_path = os.path.join(root, f"{uuid}.json")
                        break
                
                if current_json_path and current_json_path != target_path:
                    os.makedirs(day_dir, exist_ok=True)
                    shutil.move(current_json_path, target_path)
                    logger.info(f"[#process]âœ… å·²ç§»åŠ¨: {uuid}.json")
                
                processed += 1
                logger.info(f"[@current_progress]é‡ç»„è¿›åº¦ {processed}/{total_records} ({(processed/total_records*100):.1f}%)")
                    
            except ValueError as e:
                logger.error(f"[#process]âŒ UUID {uuid} çš„æ—¶é—´æˆ³æ ¼å¼æ— æ•ˆ: {latest_timestamp}")
                
    except Exception as e:
        logger.error(f"[#process]é‡ç»„UUIDæ–‡ä»¶å¤±è´¥: {e}")
    
    logger.info("[#current_stats]âœ¨ UUIDæ–‡ä»¶é‡ç»„å®Œæˆ")

def update_json_records(uuid_directory=r'E:\1BACKUP\ehv\uuid'):
    """æ›´æ–°JSONè®°å½•æ–‡ä»¶ï¼Œç¡®ä¿æ‰€æœ‰è®°å½•éƒ½è¢«ä¿å­˜"""
    logger.info("[#current_stats]ğŸ”„ å¼€å§‹æ›´æ–°JSONè®°å½•...")
    
    json_record_path = os.path.join(uuid_directory, 'uuid_records.json')
    
    existing_records = JsonHandler.load(json_record_path)
    
    total_files = 0
    processed = 0
    
    # é¦–å…ˆè®¡ç®—æ€»æ–‡ä»¶æ•°
    for root, _, files in os.walk(uuid_directory):
        total_files += sum(1 for file in files if file.endswith('.json') and file != 'uuid_records.json')
    
    # éå†ç›®å½•ç»“æ„æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    for root, _, files in os.walk(uuid_directory):
        for file in files:
            if file.endswith('.json') and file != 'uuid_records.json':
                uuid = os.path.splitext(file)[0]
                json_path = os.path.join(root, file)
                try:
                    file_data = JsonHandler.load(json_path)
                    if uuid not in existing_records:
                        existing_records[uuid] = file_data
                        logger.info(f"[#process]âœ… æ·»åŠ æ–°è®°å½•: {uuid}")
                    else:
                        existing_records[uuid]["timestamps"].update(file_data.get("timestamps", {}))
                        logger.info(f"[#process]âœ… æ›´æ–°è®°å½•: {uuid}")
                        
                except Exception as e:
                    logger.error(f"[#process]å¤„ç†JSONæ–‡ä»¶å¤±è´¥ {json_path}: {e}")
                
                processed += 1
                logger.info(f"[@current_progress]æ›´æ–°è¿›åº¦ {processed}/{total_files} ({(processed/total_files*100):.1f}%)")
    
    if JsonHandler.save(json_record_path, existing_records):
        logger.info("[#current_stats]âœ… JSONè®°å½•æ›´æ–°å®Œæˆ")
    else:
        logger.error("[#process]âŒ JSONè®°å½•æ›´æ–°å¤±è´¥")

def convert_yaml_to_json_structure():
    """å°†ç°æœ‰çš„YAMLæ–‡ä»¶ç»“æ„è½¬æ¢ä¸ºJSONç»“æ„"""
    logger.info("[#current_stats]ğŸ”„ å¼€å§‹è½¬æ¢YAMLåˆ°JSONç»“æ„...")
    
    uuid_directory = r'E:\1BACKUP\ehv\uuid'
    yaml_record_path = os.path.join(uuid_directory, 'uuid_records.yaml')
    json_record_path = os.path.join(uuid_directory, 'uuid_records.json')
    
    # è½¬æ¢ä¸»è®°å½•æ–‡ä»¶
    if os.path.exists(yaml_record_path):
        try:
            with open(yaml_record_path, 'r', encoding='utf-8') as f:
                yaml_data = yaml.safe_load(f)
                
            total_records = len(yaml_data)
            processed = 0
            
            json_records = {}
            for record in yaml_data:
                uuid = record.get('UUID')
                if not uuid:
                    continue
                    
                if uuid not in json_records:
                    json_records[uuid] = {"timestamps": {}}
                    
                timestamp = record.get('LastModified') or record.get('CreatedAt')
                if timestamp:
                    json_records[uuid]["timestamps"][timestamp] = {
                        "archive_name": record.get('ArchiveName', ''),
                        "artist_name": record.get('ArtistName', ''),
                        "relative_path": record.get('LastPath', '')
                    }
                
                processed += 1
                logger.info(f"[@current_progress]è½¬æ¢è¿›åº¦ {processed}/{total_records} ({(processed/total_records*100):.1f}%)")
            
            JsonHandler.save(json_record_path, json_records)
            logger.info("[#current_stats]âœ… ä¸»è®°å½•æ–‡ä»¶è½¬æ¢å®Œæˆ")
            
        except Exception as e:
            logger.error(f"[#process]è½¬æ¢ä¸»è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
    
    # è½¬æ¢ç›®å½•ä¸­çš„YAMLæ–‡ä»¶
    yaml_files = []
    for root, _, files in os.walk(uuid_directory):
        yaml_files.extend([os.path.join(root, f) for f in files if f.endswith('.yaml') and f != 'uuid_records.yaml'])
    
    total_files = len(yaml_files)
    processed = 0
    
    for yaml_path in yaml_files:
        try:
            with open(yaml_path, 'r', encoding='utf-8') as f:
                yaml_data = yaml.safe_load(f)
                
            json_path = os.path.join(os.path.dirname(yaml_path), f"{os.path.splitext(os.path.basename(yaml_path))[0]}.json")
            
            json_data = JsonHandler.convert_yaml_to_json(yaml_data)
            json_data["uuid"] = os.path.splitext(os.path.basename(yaml_path))[0]
            
            if JsonHandler.save(json_path, json_data):
                os.remove(yaml_path)
                logger.info(f"[#process]âœ… è½¬æ¢å®Œæˆ: {os.path.basename(yaml_path)}")
            
            processed += 1
            logger.info(f"[@current_progress]æ–‡ä»¶è½¬æ¢è¿›åº¦ {processed}/{total_files} ({(processed/total_files*100):.1f}%)")
            
        except Exception as e:
            logger.error(f"[#process]è½¬æ¢æ–‡ä»¶å¤±è´¥ {os.path.basename(yaml_path)}: {e}")
    
    logger.info("[#current_stats]âœ¨ YAMLåˆ°JSONè½¬æ¢å®Œæˆ")

class CommandManager:
    """å‘½ä»¤è¡Œå‚æ•°ç®¡ç†å™¨"""
    
    @staticmethod
    def init_parser():
        parser = argparse.ArgumentParser(description='å¤„ç†æ–‡ä»¶UUIDå’ŒJSONç”Ÿæˆ')
        parser.add_argument('-c', '--clipboard', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
        parser.add_argument('-m', '--mode', choices=['multi', 'single'], help='å¤„ç†æ¨¡å¼ï¼šmulti(å¤šäººæ¨¡å¼)æˆ–single(å•äººæ¨¡å¼)')
        parser.add_argument('--no-artist', action='store_true', help='æ— ç”»å¸ˆæ¨¡å¼ - ä¸æ·»åŠ ç”»å¸ˆå')
        parser.add_argument('--keep-timestamp', action='store_true', help='ä¿æŒæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´')
        parser.add_argument('--path', help='è¦å¤„ç†çš„è·¯å¾„')
        parser.add_argument('-a', '--auto-sequence', action='store_true', help='è‡ªåŠ¨æ‰§è¡Œå®Œæ•´åºåˆ—ï¼šUUID-JSON -> è‡ªåŠ¨æ–‡ä»¶å -> UUID-JSON')
        parser.add_argument('-r', '--reorganize', action='store_true', help='é‡æ–°ç»„ç»‡ UUID æ–‡ä»¶ç»“æ„')
        parser.add_argument('-u', '--update-records', action='store_true', help='æ›´æ–° UUID è®°å½•æ–‡ä»¶')
        parser.add_argument('--convert', action='store_true', help='è½¬æ¢YAMLåˆ°JSONç»“æ„')
        return parser

    @staticmethod
    def get_target_directory(args):
        if args.clipboard:
            try:
                target_directory = pyperclip.paste().strip().strip('"')
                if not os.path.exists(target_directory):
                    logger.error(f"[#process]å‰ªè´´æ¿ä¸­çš„è·¯å¾„æ— æ•ˆ: {target_directory}")
                    sys.exit(1)
                logger.info(f"[#current_stats]å·²ä»å‰ªè´´æ¿è¯»å–è·¯å¾„: {target_directory}")
            except Exception as e:
                logger.error(f"[#process]ä»å‰ªè´´æ¿è¯»å–è·¯å¾„å¤±è´¥: {e}")
                sys.exit(1)
        else:
            target_directory = args.path or r"E:\1EHV"
            logger.info(f"[#current_stats]ä½¿ç”¨è·¯å¾„: {target_directory}")
        return target_directory

class TaskExecutor:
    """ä»»åŠ¡æ‰§è¡Œå™¨"""
    
    def __init__(self, args, target_directory: str):
        self.args = args
        self.target_directory = target_directory
        self.max_workers = min(32, (multiprocessing.cpu_count() * 4) + 1)
        self.confirmed_artists = set()
        self.uuid_directory = r'E:\1BACKUP\ehv\uuid'
        self.archive_processor = ArchiveProcessor(self.target_directory, self.uuid_directory, self.max_workers)
        self.uuid_record_manager = UuidRecordManager(self.uuid_directory)

    def _confirm_artists(self) -> None:
        """ç¡®è®¤ç”»å¸ˆä¿¡æ¯"""
        print("\næ­£åœ¨æ‰«æç”»å¸ˆä¿¡æ¯...")
        artists = set()
        
        # æ‰«ææ‰€æœ‰å‹ç¼©æ–‡ä»¶ä»¥è·å–ç”»å¸ˆä¿¡æ¯
        for root, _, files in os.walk(self.target_directory):
            for file in files:
                if file.endswith(('.zip', '.rar', '.7z')):
                    archive_path = os.path.join(root, file)
                    artist = PathHandler.get_artist_name(self.target_directory, archive_path, self.args.mode)
                    if artist:
                        artists.add(artist)
        
        # æ˜¾ç¤ºç”»å¸ˆä¿¡æ¯å¹¶ç­‰å¾…ç¡®è®¤
        if self.args.mode == 'single':
            if len(artists) > 1:
                print("\nâš ï¸ è­¦å‘Šï¼šåœ¨å•äººæ¨¡å¼ä¸‹æ£€æµ‹åˆ°å¤šä¸ªç”»å¸ˆåç§°ï¼š")
                for i, artist in enumerate(sorted(artists), 1):
                    print(f"{i}. {artist}")
                print("\nè¯·ç¡®è®¤è¿™æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Ÿå¦‚æœä¸ç¬¦åˆï¼Œè¯·æ£€æŸ¥ç›®å½•ç»“æ„ã€‚")
            elif len(artists) == 1:
                print(f"\næ£€æµ‹åˆ°ç”»å¸ˆ: {next(iter(artists))}")
            else:
                print("\nâš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°ç”»å¸ˆåç§°ï¼")
            
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
            
        else:  # å¤šäººæ¨¡å¼
            print(f"\nå…±æ£€æµ‹åˆ° {len(artists)} ä¸ªç”»å¸ˆç›®å½•ï¼š")
            for i, artist in enumerate(sorted(artists), 1):
                print(f"{i}. {artist}")
            
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        
        self.confirmed_artists = artists

    def execute_tasks(self) -> None:
        """æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡"""
        # é¦–å…ˆç¡®è®¤ç”»å¸ˆä¿¡æ¯
        self._confirm_artists()
        
        # ç„¶ååˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        init_TextualLogger()
        
        logger.info(f"[#current_stats]å½“å‰æ¨¡å¼: {'å¤šäººæ¨¡å¼' if self.args.mode == 'multi' else 'å•äººæ¨¡å¼'}")
        if self.confirmed_artists:
            logger.info(f"[#current_stats]å·²ç¡®è®¤ç”»å¸ˆ: {', '.join(sorted(self.confirmed_artists))}")
            logger.info(f"[#current_stats]å¼€å§‹ä¸‹ä¸€æ­¥")

        if self.args.convert:
            self._execute_convert_task()
            return

        if self.args.reorganize:
            self._execute_reorganize_task()

        if self.args.update_records:
            self._execute_update_records_task()

        if self.args.auto_sequence:
            self._execute_auto_sequence()
        elif not self.args.reorganize and not self.args.update_records:
            self._execute_normal_process()

        self._validate_json_records()

    def _execute_convert_task(self) -> None:
        """æ‰§è¡ŒYAMLè½¬JSONä»»åŠ¡"""
        self.uuid_record_manager.convert_yaml_to_json_structure()
        sys.exit(0)

    def _execute_reorganize_task(self) -> None:
        """æ‰§è¡Œé‡ç»„ä»»åŠ¡"""
        logger.info("[#current_stats]ğŸ“ å¼€å§‹é‡æ–°ç»„ç»‡ UUID æ–‡ä»¶...")
        self.uuid_record_manager.reorganize_uuid_files()

    def _execute_update_records_task(self) -> None:
        """æ‰§è¡Œæ›´æ–°è®°å½•ä»»åŠ¡"""
        logger.info("[#current_stats]ğŸ“ å¼€å§‹æ›´æ–° UUID è®°å½•...")
        self.uuid_record_manager.update_json_records()

    def _execute_auto_sequence(self) -> None:
        """æ‰§è¡Œè‡ªåŠ¨åºåˆ—ä»»åŠ¡"""
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹æ‰§è¡Œå®Œæ•´åºåˆ—...")
        
        # ç¬¬1æ­¥ï¼šUUID-JSONå¤„ç†
        logger.info("[#current_stats]ğŸ“ ç¬¬1æ­¥ï¼šæ‰§è¡ŒUUID-JSONå¤„ç†...")
        self._process_uuid_json()
        
        # ç¬¬2æ­¥ï¼šè‡ªåŠ¨æ–‡ä»¶åå¤„ç†
        logger.info("[#current_stats]ğŸ“ ç¬¬2æ­¥ï¼šæ‰§è¡Œè‡ªåŠ¨æ–‡ä»¶åå¤„ç†...")
        self._run_auto_filename_script()
        
        # ç¬¬3æ­¥ï¼šå†æ¬¡UUID-JSONå¤„ç†
        logger.info("[#current_stats]ğŸ“ ç¬¬3æ­¥ï¼šå†æ¬¡æ‰§è¡ŒUUID-JSONå¤„ç†...")
        self._process_uuid_json()
        
        logger.info("[#current_stats]âœ¨ å®Œæ•´åºåˆ—æ‰§è¡Œå®Œæˆï¼")

    def _execute_normal_process(self) -> None:
        """æ‰§è¡Œæ™®é€šå¤„ç†æµç¨‹"""
        if self.args.mode == 'multi':
            FileSystemHandler.warm_up_cache(self.target_directory, self.max_workers)
        self.archive_processor.process_archives()

    def _process_uuid_json(self) -> None:
        """å¤„ç†UUID-JSONç›¸å…³ä»»åŠ¡"""
        if self.args.mode == 'multi':
            FileSystemHandler.warm_up_cache(self.target_directory, self.max_workers)
        skip_limit_reached = self.archive_processor.process_archives()
        
        if skip_limit_reached:
            logger.info("[#current_stats]â© ç”±äºè¿ç»­è·³è¿‡æ¬¡æ•°è¾¾åˆ°é™åˆ¶ï¼Œæå‰è¿›å…¥ä¸‹ä¸€é˜¶æ®µ")

    def _run_auto_filename_script(self) -> None:
        """è¿è¡Œè‡ªåŠ¨æ–‡ä»¶åè„šæœ¬"""
        auto_filename_script = os.path.join(os.path.dirname(__file__), '011-è‡ªåŠ¨å”¯ä¸€æ–‡ä»¶å.py')
        if not os.path.exists(auto_filename_script):
            logger.error(f"[#process]æ‰¾ä¸åˆ°è‡ªåŠ¨æ–‡ä»¶åè„šæœ¬: {auto_filename_script}")
            return

        try:
            cmd = [sys.executable, auto_filename_script]
            if self.args.clipboard:
                cmd.extend(['-c'])
            if self.args.mode:
                cmd.extend(['-m', self.args.mode])

            startupinfo = None
            if os.name == 'nt':
                startupinfo = subprocess.STARTUPINFO()
                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True,
                encoding='gbk',
                errors='ignore',
                startupinfo=startupinfo
            )

            for line in result.stdout.splitlines():
                if line.strip():
                    logger.info(line)

            logger.info("[#current_stats]âœ… è‡ªåŠ¨æ–‡ä»¶åå¤„ç†å®Œæˆ")
        except subprocess.CalledProcessError as e:
            logger.error(f"[#process]è‡ªåŠ¨æ–‡ä»¶åå¤„ç†å¤±è´¥: {str(e)}")
            if e.output:
                logger.error(f"[#process]é”™è¯¯è¾“å‡º: {e.output}")

    def _validate_json_records(self) -> None:
        """éªŒè¯JSONè®°å½•æ–‡ä»¶"""
        json_record_path = os.path.join(self.uuid_directory, 'uuid_records.json')
        if os.path.exists(json_record_path):
            try:
                with open(json_record_path, 'r', encoding='utf-8') as f:
                    json.load(f)
                logger.info("[#current_stats]âœ… JSONè®°å½•æ–‡ä»¶éªŒè¯é€šè¿‡")
            except json.JSONDecodeError as e:
                logger.error(f"[#process]âŒ JSONè®°å½•æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
                sys.exit(1)
        else:
            logger.warning("[#process]âš ï¸ JSONè®°å½•æ–‡ä»¶ä¸å­˜åœ¨")

class UuidRecordManager:
    """UUIDè®°å½•ç®¡ç†ç±»"""
    
    def __init__(self, uuid_directory: str = r'E:\1BACKUP\ehv\uuid'):
        self.uuid_directory = uuid_directory
    
    def reorganize_uuid_files(self) -> None:
        """æ ¹æ®æœ€åä¿®æ”¹æ—¶é—´é‡æ–°ç»„ç»‡UUIDæ–‡ä»¶çš„ç›®å½•ç»“æ„"""
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹é‡æ–°ç»„ç»‡UUIDæ–‡ä»¶...")
        
        json_record_path = os.path.join(self.uuid_directory, 'uuid_records.json')
        if not os.path.exists(json_record_path):
            logger.error("[#process]âŒ UUIDè®°å½•æ–‡ä»¶ä¸å­˜åœ¨")
            return
            
        try:
            with open(json_record_path, 'r', encoding='utf-8') as f:
                records = json.load(f)
                
            total_records = len(records)
            processed = 0
            
            for uuid, data in records.items():
                if not data.get("timestamps"):
                    continue
                    
                latest_timestamp = max(data["timestamps"].keys())
                
                try:
                    date = datetime.strptime(latest_timestamp, "%Y-%m-%d %H:%M:%S")
                    year = str(date.year)
                    month = f"{date.month:02d}"
                    day = f"{date.day:02d}"
                    
                    year_dir = os.path.join(self.uuid_directory, year)
                    month_dir = os.path.join(year_dir, month)
                    day_dir = os.path.join(month_dir, day)
                    target_path = os.path.join(day_dir, f"{uuid}.json")
                    
                    current_json_path = None
                    for root, _, files in os.walk(self.uuid_directory):
                        if f"{uuid}.json" in files:
                            current_json_path = os.path.join(root, f"{uuid}.json")
                            break
                    
                    if current_json_path and current_json_path != target_path:
                        os.makedirs(day_dir, exist_ok=True)
                        shutil.move(current_json_path, target_path)
                        logger.info(f"[#process]âœ… å·²ç§»åŠ¨: {uuid}.json")
                    
                    processed += 1
                    logger.info(f"[@current_progress]é‡ç»„è¿›åº¦ {processed}/{total_records} ({(processed/total_records*100):.1f}%)")
                        
                except ValueError as e:
                    logger.error(f"[#process]âŒ UUID {uuid} çš„æ—¶é—´æˆ³æ ¼å¼æ— æ•ˆ: {latest_timestamp}")
                    
        except Exception as e:
            logger.error(f"[#process]é‡ç»„UUIDæ–‡ä»¶å¤±è´¥: {e}")
        
        logger.info("[#current_stats]âœ¨ UUIDæ–‡ä»¶é‡ç»„å®Œæˆ")
    
    def update_json_records(self) -> None:
        """æ›´æ–°JSONè®°å½•æ–‡ä»¶ï¼Œç¡®ä¿æ‰€æœ‰è®°å½•éƒ½è¢«ä¿å­˜"""
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹æ›´æ–°JSONè®°å½•...")
        
        json_record_path = os.path.join(self.uuid_directory, 'uuid_records.json')
        
        existing_records = JsonHandler.load(json_record_path)
        
        total_files = 0
        processed = 0
        
        # é¦–å…ˆè®¡ç®—æ€»æ–‡ä»¶æ•°
        for root, _, files in os.walk(self.uuid_directory):
            total_files += sum(1 for file in files if file.endswith('.json') and file != 'uuid_records.json')
        
        # éå†ç›®å½•ç»“æ„æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        for root, _, files in os.walk(self.uuid_directory):
            for file in files:
                if file.endswith('.json') and file != 'uuid_records.json':
                    uuid = os.path.splitext(file)[0]
                    json_path = os.path.join(root, file)
                    try:
                        file_data = JsonHandler.load(json_path)
                        if uuid not in existing_records:
                            existing_records[uuid] = file_data
                            logger.info(f"[#process]âœ… æ·»åŠ æ–°è®°å½•: {uuid}")
                        else:
                            existing_records[uuid]["timestamps"].update(file_data.get("timestamps", {}))
                            logger.info(f"[#process]âœ… æ›´æ–°è®°å½•: {uuid}")
                            
                    except Exception as e:
                        logger.error(f"[#process]å¤„ç†JSONæ–‡ä»¶å¤±è´¥ {json_path}: {e}")
                
                processed += 1
                logger.info(f"[@current_progress]æ›´æ–°è¿›åº¦ {processed}/{total_files} ({(processed/total_files*100):.1f}%)")
        
        if JsonHandler.save(json_record_path, existing_records):
            logger.info("[#current_stats]âœ… JSONè®°å½•æ›´æ–°å®Œæˆ")
        else:
            logger.error("[#process]âŒ JSONè®°å½•æ›´æ–°å¤±è´¥")
    
    def convert_yaml_to_json_structure(self) -> None:
        """å°†ç°æœ‰çš„YAMLæ–‡ä»¶ç»“æ„è½¬æ¢ä¸ºJSONç»“æ„"""
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹è½¬æ¢YAMLåˆ°JSONç»“æ„...")
        
        yaml_record_path = os.path.join(self.uuid_directory, 'uuid_records.yaml')
        json_record_path = os.path.join(self.uuid_directory, 'uuid_records.json')
        
        # è½¬æ¢ä¸»è®°å½•æ–‡ä»¶
        if os.path.exists(yaml_record_path):
            try:
                with open(yaml_record_path, 'r', encoding='utf-8') as f:
                    yaml_data = yaml.safe_load(f)
                    
                total_records = len(yaml_data)
                processed = 0
                
                json_records = {}
                for record in yaml_data:
                    uuid = record.get('UUID')
                    if not uuid:
                        continue
                        
                    if uuid not in json_records:
                        json_records[uuid] = {"timestamps": {}}
                        
                    timestamp = record.get('LastModified') or record.get('CreatedAt')
                    if timestamp:
                        json_records[uuid]["timestamps"][timestamp] = {
                            "archive_name": record.get('ArchiveName', ''),
                            "artist_name": record.get('ArtistName', ''),
                            "relative_path": record.get('LastPath', '')
                        }
                    
                    processed += 1
                    logger.info(f"[@current_progress]è½¬æ¢è¿›åº¦ {processed}/{total_records} ({(processed/total_records*100):.1f}%)")
                
                JsonHandler.save(json_record_path, json_records)
                logger.info("[#current_stats]âœ… ä¸»è®°å½•æ–‡ä»¶è½¬æ¢å®Œæˆ")
                
            except Exception as e:
                logger.error(f"[#process]è½¬æ¢ä¸»è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
        
        # è½¬æ¢ç›®å½•ä¸­çš„YAMLæ–‡ä»¶
        yaml_files = []
        for root, _, files in os.walk(self.uuid_directory):
            yaml_files.extend([os.path.join(root, f) for f in files if f.endswith('.yaml') and f != 'uuid_records.yaml'])
        
        total_files = len(yaml_files)
        processed = 0
        
        for yaml_path in yaml_files:
            try:
                with open(yaml_path, 'r', encoding='utf-8') as f:
                    yaml_data = yaml.safe_load(f)
                    
                json_path = os.path.join(os.path.dirname(yaml_path), f"{os.path.splitext(os.path.basename(yaml_path))[0]}.json")
                
                json_data = JsonHandler.convert_yaml_to_json(yaml_data)
                json_data["uuid"] = os.path.splitext(os.path.basename(yaml_path))[0]
                
                if JsonHandler.save(json_path, json_data):
                    os.remove(yaml_path)
                    logger.info(f"[#process]âœ… è½¬æ¢å®Œæˆ: {os.path.basename(yaml_path)}")
                
                processed += 1
                logger.info(f"[@current_progress]æ–‡ä»¶è½¬æ¢è¿›åº¦ {processed}/{total_files} ({(processed/total_files*100):.1f}%)")
                
            except Exception as e:
                logger.error(f"[#process]è½¬æ¢æ–‡ä»¶å¤±è´¥ {os.path.basename(yaml_path)}: {e}")
        
        logger.info("[#current_stats]âœ¨ YAMLåˆ°JSONè½¬æ¢å®Œæˆ")

if __name__ == '__main__':
    # åˆå§‹åŒ–å‘½ä»¤è¡Œè§£æå™¨
    parser = CommandManager.init_parser()
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œå¯åŠ¨TUIç•Œé¢
    if len(sys.argv) == 1:
        main()
        sys.exit(0)

    # è·å–ç›®æ ‡ç›®å½•
    target_directory = CommandManager.get_target_directory(args)

    # æ‰§è¡Œä»»åŠ¡
    executor = TaskExecutor(args, target_directory)
    executor.execute_tasks()
    