import os
import uuid
import json
import yaml
import time
import subprocess
import difflib
import shutil
import logging
import sys
import threading
import argparse
import multiprocessing
from pathlib import Path
from datetime import datetime
from nanoid import generate
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import pyperclip
from colorama import init, Fore, Style
import win32file
import win32con
import numpy as np
from nodes.record.logger_config import setup_logger
from nodes.tui.textual_preset import create_config_app
from nodes.tui.textual_logger import TextualLoggerManager
import orjson  # ä½¿ç”¨orjsonè¿›è¡Œæ›´å¿«çš„JSONå¤„ç†
import zipfile
from typing import Dict, Any, Optional, List
import mmap

# å®šä¹‰æ—¥å¿—å¸ƒå±€é…ç½®
TEXTUAL_LAYOUT = {
    "current_stats": {
        "ratio": 2,
        "title": "ğŸ“Š æ€»ä½“è¿›åº¦",
        "style": "lightyellow"
    },
    "current_progress": {
        "ratio": 2,
        "title": "ğŸ”„ å½“å‰è¿›åº¦",
        "style": "lightcyan"
    },
    "process": {
        "ratio": 3,
        "title": "ğŸ“ å¤„ç†æ—¥å¿—",
        "style": "lightpink"
    },
    "update": {
        "ratio": 2,
        "title": "â„¹ï¸ æ›´æ–°æ—¥å¿—",
        "style": "lightblue"
    }
}

# åˆå§‹åŒ–æ—¥å¿—é…ç½®
config = {
    'script_name': 'comic_auto_uuid',
    'console_enabled': False
}
logger, config_info = setup_logger(config)

def init_TextualLogger():
    TextualLoggerManager.set_layout(TEXTUAL_LAYOUT, config_info['log_file'])

# åˆå§‹åŒ– colorama
init()

class JsonHandler:
    """JSONæ–‡ä»¶å¤„ç†ç±»"""
    
    @staticmethod
    def load(file_path: str) -> Dict[str, Any]:
        """å¿«é€ŸåŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(file_path, 'rb') as f:
                return orjson.loads(f.read())
        except Exception as e:
            logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {}
    
    @staticmethod
    def save(file_path: str, data: Dict[str, Any]) -> bool:
        """å¿«é€Ÿä¿å­˜JSONæ–‡ä»¶"""
        temp_path = f"{file_path}.tmp"
        try:
            # ä½¿ç”¨orjsonè¿›è¡Œå¿«é€Ÿåºåˆ—åŒ–
            json_bytes = orjson.dumps(
                data,
                option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS | orjson.OPT_SERIALIZE_NUMPY
            )
            
            with open(temp_path, 'wb') as f:
                f.write(json_bytes)
            
            if os.path.exists(file_path):
                os.replace(temp_path, file_path)
            else:
                os.rename(temp_path, file_path)
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False

    @staticmethod
    def convert_yaml_to_json(yaml_data: list) -> Dict[str, Any]:
        """å°†YAMLæ•°æ®è½¬æ¢ä¸ºæ–°çš„JSONæ ¼å¼"""
        json_data = {
            "timestamps": {}
        }
        
        for record in yaml_data:
            timestamp = record.get('Timestamp', '')
            if not timestamp:
                continue
                
            json_data["timestamps"][timestamp] = {
                "archive_name": record.get('ArchiveName', ''),
                "artist_name": record.get('ArtistName', ''),
                "relative_path": record.get('RelativePath', '')
            }
        
        return json_data

    @staticmethod
    def check_and_update_record(json_content: Dict[str, Any], archive_name: str, artist_name: str, relative_path: str, timestamp: str) -> bool:
        """æ£€æŸ¥å¹¶æ›´æ–°JSONè®°å½•
        
        Returns:
            bool: Trueè¡¨ç¤ºéœ€è¦æ›´æ–°ï¼ŒFalseè¡¨ç¤ºæ— éœ€æ›´æ–°
        """
        if "timestamps" not in json_content:
            return True
            
        latest_record = None
        if json_content["timestamps"]:
            latest_timestamp = max(json_content["timestamps"].keys())
            latest_record = json_content["timestamps"][latest_timestamp]
            
        if not latest_record:
            return True
            
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        need_update = False
        if latest_record.get("archive_name") != archive_name:
            need_update = True
        if latest_record.get("artist_name") != artist_name:
            need_update = True
        if latest_record.get("relative_path") != relative_path:
            need_update = True
            
        return need_update

    @staticmethod
    def update_record(json_content: Dict[str, Any], archive_name: str, artist_name: str, relative_path: str, timestamp: str) -> Dict[str, Any]:
        """æ›´æ–°JSONè®°å½•"""
        json_content["timestamps"][timestamp] = {
            "archive_name": archive_name,
            "artist_name": artist_name,
            "relative_path": relative_path
        }
        return json_content

class ArchiveHandler:
    """å‹ç¼©åŒ…å¤„ç†ç±»"""
    
    @staticmethod
    def check_archive_integrity(archive_path: str) -> bool:
        """æ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            
        Returns:
            bool: å‹ç¼©åŒ…æ˜¯å¦å®Œæ•´
        """
        try:
            # å°è¯•ä½¿ç”¨zipfile
            try:
                with zipfile.ZipFile(archive_path, 'r') as zf:
                    # æµ‹è¯•å‹ç¼©åŒ…å®Œæ•´æ€§
                    if zf.testzip() is not None:
                        logger.warning(f"[#process]å‹ç¼©åŒ…æŸå: {os.path.basename(archive_path)}")
                        return False
                    return True
            except zipfile.BadZipFile:
                # å¦‚æœä¸æ˜¯zipæ–‡ä»¶ï¼Œä½¿ç”¨7zæµ‹è¯•
                startupinfo = None
                if os.name == 'nt':
                    startupinfo = subprocess.STARTUPINFO()
                    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
                
                result = subprocess.run(
                    ['7z', 't', archive_path],
                    capture_output=True,
                    text=True,
                    encoding='gbk',
                    errors='ignore',
                    startupinfo=startupinfo,
                    check=False
                )
                
                if result.returncode != 0:
                    logger.warning(f"[#process]å‹ç¼©åŒ…æŸå: {os.path.basename(archive_path)}")
                    return False
                return True
                
        except Exception as e:
            logger.error(f"[#process]æ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§å¤±è´¥: {str(e)}")
            return False
    
    @staticmethod
    def delete_files_from_archive(archive_path: str, files_to_delete: List[str]) -> bool:
        """ä½¿ç”¨BandZipå‘½ä»¤è¡Œåˆ é™¤æ–‡ä»¶"""
        if not files_to_delete:
            return True

        archive_name = os.path.basename(archive_path)
        logger.info(f"[#process]å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {archive_name}")
        logger.info(f"[#process]éœ€è¦åˆ é™¤çš„æ–‡ä»¶: {files_to_delete}")

        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        backup_path = archive_path + ".bak"
        temp_path = archive_path + ".temp"
        success = False

        try:
            # å¤‡ä»½åŸæ–‡ä»¶
            shutil.copy2(archive_path, backup_path)
            logger.info(f"[#process][å¤‡ä»½] åˆ›å»ºåŸæ–‡ä»¶å¤‡ä»½: {backup_path}")

            # ä½¿ç”¨BandZipåˆ é™¤æ–‡ä»¶
            deleted_count = 0
            for file in files_to_delete:
                try:
                    # ä½¿ç”¨BandZipçš„bzå‘½ä»¤åˆ é™¤æ–‡ä»¶
                    result = subprocess.run(
                        [
                            'bz', 'd',          # åˆ é™¤å‘½ä»¤
                            archive_path,        # å‹ç¼©åŒ…è·¯å¾„
                            file,               # è¦åˆ é™¤çš„æ–‡ä»¶
                            '/q',               # å®‰é™æ¨¡å¼
                            '/y',               # è‡ªåŠ¨ç¡®è®¤
                            '/utf8'             # ä½¿ç”¨UTF-8ç¼–ç 
                        ],
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        errors='ignore'
                    )

                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                    if result.returncode == 0:
                        deleted_count += 1
                        logger.info(f"[#process][åˆ é™¤æˆåŠŸ] {file}")
                    else:
                        logger.warning(f"[#process]åˆ é™¤å¤±è´¥: {file}")
                        logger.debug(f"[#process]BandZipè¾“å‡º: {result.stdout}\n{result.stderr}")

                except Exception as e:
                    logger.error(f"[#process]åˆ é™¤æ–‡ä»¶å¤±è´¥ {file}: {e}")

            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶è¢«åˆ é™¤
            if deleted_count == 0:
                logger.warning("[#process]æœªæˆåŠŸåˆ é™¤ä»»ä½•æ–‡ä»¶")
                # æ¢å¤å¤‡ä»½
                if os.path.exists(backup_path):
                    shutil.copy2(backup_path, archive_path)
                    logger.info("[#process][æ¢å¤] ä»å¤‡ä»½æ¢å¤åŸæ–‡ä»¶")
                success = False
            else:
                logger.info(f"[#process][å®Œæˆ] æˆåŠŸåˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶")
                success = True

            return success

        except Exception as e:
            logger.error(f"[#process]å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # æ¢å¤å¤‡ä»½
            if os.path.exists(backup_path):
                try:
                    shutil.copy2(backup_path, archive_path)
                    logger.info("[#process][æ¢å¤] ä»å¤‡ä»½æ¢å¤åŸæ–‡ä»¶")
                except Exception as e:
                    logger.error(f"[#process]æ¢å¤å¤‡ä»½å¤±è´¥: {e}")
            return False

        finally:
            # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶å’Œå¤‡ä»½æ–‡ä»¶
            for path in [backup_path, temp_path]:
                if os.path.exists(path):
                    try:
                        os.remove(path)
                        logger.debug(f"[#process][æ¸…ç†] åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {os.path.basename(path)}")
                    except Exception as e:
                        logger.error(f"[#process]åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {os.path.basename(path)}: {e}")
            
            # æ¸…ç†åŒåçš„å…¶ä»–ä¸´æ—¶æ–‡ä»¶
            dir_path = os.path.dirname(archive_path)
            base_name = os.path.splitext(archive_name)[0]
            for file in os.listdir(dir_path):
                if file.startswith(base_name) and (file.endswith('.bak') or file.endswith('.temp')):
                    try:
                        os.remove(os.path.join(dir_path, file))
                        logger.debug(f"[#process][æ¸…ç†] åˆ é™¤ç›¸å…³ä¸´æ—¶æ–‡ä»¶: {file}")
                    except Exception as e:
                        logger.error(f"[#process]åˆ é™¤ç›¸å…³ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {file}: {e}")
    
    @staticmethod
    def load_yaml_uuid_from_archive(archive_path: str) -> Optional[str]:
        """ä»å‹ç¼©åŒ…ä¸­åŠ è½½YAMLæ–‡ä»¶çš„UUID"""
        # é¦–å…ˆæ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§
        if not ArchiveHandler.check_archive_integrity(archive_path):
            return None
            
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for name in zf.namelist():
                    if name.endswith('.yaml'):
                        return os.path.splitext(name)[0]
        except zipfile.BadZipFile:
            # å¦‚æœä¸æ˜¯zipæ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨7z
            return ArchiveHandler._load_uuid_from_7z(archive_path, '.yaml')
        except Exception as e:
            logger.error(f"[#process]è¯»å–å‹ç¼©åŒ…å¤±è´¥: {archive_path}")
        return None
    
    @staticmethod
    def load_json_uuid_from_archive(archive_path: str) -> Optional[str]:
        """ä»å‹ç¼©åŒ…ä¸­åŠ è½½JSONæ–‡ä»¶çš„UUID"""
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for name in zf.namelist():
                    if name.endswith('.json'):
                        return os.path.splitext(name)[0]
        except zipfile.BadZipFile:
            # å¦‚æœä¸æ˜¯zipæ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨7z
            return ArchiveHandler._load_uuid_from_7z(archive_path, '.json')
        except Exception as e:
            logger.error(f"è¯»å–å‹ç¼©åŒ…å¤±è´¥ {archive_path}: {e}")
        return None
    
    @staticmethod
    def _load_uuid_from_7z(archive_path: str, ext: str) -> Optional[str]:
        """ä½¿ç”¨7zå‘½ä»¤è¡Œå·¥å…·åŠ è½½UUID"""
        try:
            startupinfo = None
            if os.name == 'nt':
                startupinfo = subprocess.STARTUPINFO()
                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            
            result = subprocess.run(
                ['7z', 'l', archive_path],
                capture_output=True,
                text=True,
                encoding='gbk',
                errors='ignore',
                startupinfo=startupinfo,
                check=False
            )
            
            if result.returncode != 0:
                return None
            
            for line in result.stdout.splitlines():
                if not line.strip():
                    continue
                if line.endswith(ext):
                    return os.path.splitext(line.split()[-1])[0]
                    
        except Exception as e:
            logger.error(f"ä½¿ç”¨7zè¯»å–å‹ç¼©åŒ…å¤±è´¥ {archive_path}: {e}")
        return None
    
    @staticmethod
    def extract_yaml_from_archive(archive_path: str, yaml_uuid: str, temp_dir: str) -> Optional[str]:
        """ä»å‹ç¼©åŒ…ä¸­æå–YAMLæ–‡ä»¶
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            yaml_uuid: YAMLæ–‡ä»¶çš„UUIDï¼ˆä¸å«æ‰©å±•åï¼‰
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            
        Returns:
            Optional[str]: æå–çš„YAMLæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        yaml_path = os.path.join(temp_dir, f"{yaml_uuid}.yaml")
        
        try:
            # å°è¯•ä½¿ç”¨zipfile
            with zipfile.ZipFile(archive_path, 'r') as zf:
                zf.extract(f"{yaml_uuid}.yaml", temp_dir)
                return yaml_path
        except Exception:
            # å¦‚æœzipfileå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨7z
            try:
                subprocess.run(
                    ['7z', 'e', archive_path, f"{yaml_uuid}.yaml", f"-o{temp_dir}"],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    check=True
                )
                if os.path.exists(yaml_path):
                    return yaml_path
            except subprocess.CalledProcessError:
                logger.warning(f"[#process]æå–YAMLæ–‡ä»¶å¤±è´¥: {os.path.basename(archive_path)}")
        
        return None

    @staticmethod
    @staticmethod
    def add_json_to_archive(archive_path: str, json_path: str, json_name: str) -> bool:
        """æ·»åŠ JSONæ–‡ä»¶åˆ°å‹ç¼©åŒ…
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            json_path: JSONæ–‡ä»¶è·¯å¾„
            json_name: è¦ä¿å­˜åœ¨å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶å
            
        Returns:
            bool: æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            # å°è¯•ä½¿ç”¨zipfile
            with zipfile.ZipFile(archive_path, 'a') as zf:
                # å¦‚æœå­˜åœ¨åŒåæ–‡ä»¶ï¼Œå…ˆåˆ é™¤
                try:
                    zf.remove(json_name)
                except KeyError:
                    pass
                zf.write(json_path, json_name)
                logger.info(f"[#process]æ·»åŠ JSONæ–‡ä»¶: {json_name}")
                return True
        except Exception:
            # å¦‚æœzipfileå¤±è´¥ï¼Œä½¿ç”¨7z
            try:
                # ä½¿ç”¨7z uå‘½ä»¤æ›´æ–°æ–‡ä»¶
                subprocess.run(
                    ['7z', 'u', archive_path, json_path, f"-w{os.path.dirname(json_path)}"],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    check=True
                )
                logger.info(f"[#process]æ·»åŠ JSONæ–‡ä»¶: {json_name}")
                return True
            except subprocess.CalledProcessError:
                logger.error(f"[#process]æ·»åŠ JSONæ–‡ä»¶å¤±è´¥: {json_name}")
                return False

    @staticmethod
    def convert_yaml_archive_to_json(archive_path: str) -> Optional[Dict[str, Any]]:
        """è½¬æ¢å‹ç¼©åŒ…ä¸­çš„YAMLæ–‡ä»¶ä¸ºJSONæ ¼å¼"""
        try:
            # é¦–å…ˆæ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§
            if not ArchiveHandler.check_archive_integrity(archive_path):
                logger.warning(f"[#process]è·³è¿‡æŸåçš„å‹ç¼©åŒ…: {os.path.basename(archive_path)}")
                return None
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨YAMLæ–‡ä»¶
            yaml_uuid = ArchiveHandler.load_yaml_uuid_from_archive(archive_path)
            if not yaml_uuid:
                return None
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = os.path.join(os.path.dirname(archive_path), '.temp_extract')
            os.makedirs(temp_dir, exist_ok=True)
            
            try:
                # 1. æå–YAMLæ–‡ä»¶
                yaml_path = ArchiveHandler.extract_yaml_from_archive(archive_path, yaml_uuid, temp_dir)
                if not yaml_path or not os.path.exists(yaml_path):
                    logger.error(f"[#process]æ— æ³•æå–YAMLæ–‡ä»¶: {os.path.basename(archive_path)}")
                    return None
                
                # 2. è¯»å–å¹¶è½¬æ¢YAMLæ•°æ®
                with open(yaml_path, 'r', encoding='utf-8') as f:
                    yaml_data = yaml.safe_load(f)
                
                # 3. æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåJSONæ–‡ä»¶
                json_files = []
                try:
                    with zipfile.ZipFile(archive_path, 'r') as zf:
                        json_files = [f for f in zf.namelist() if f.endswith('.json')]
                except Exception:
                    # å¦‚æœzipfileå¤±è´¥ï¼Œä½¿ç”¨7zåˆ—å‡ºæ–‡ä»¶
                    try:
                        result = subprocess.run(
                            ['7z', 'l', archive_path],
                            capture_output=True,
                            text=True,
                            encoding='gbk',
                            errors='ignore',
                            check=True
                        )
                        if result.returncode == 0:
                            json_files = [line.split()[-1] for line in result.stdout.splitlines() 
                                        if line.strip() and line.endswith('.json')]
                    except subprocess.CalledProcessError:
                        pass
                
                # å¦‚æœå­˜åœ¨JSONæ–‡ä»¶ï¼Œåˆ é™¤å®ƒä»¬å¹¶ç”Ÿæˆæ–°çš„UUID
                if json_files:
                    logger.info(f"[#process]å‘ç°ç°æœ‰JSONæ–‡ä»¶ï¼Œå°†åˆ é™¤å¹¶ç”Ÿæˆæ–°UUID: {os.path.basename(archive_path)}")
                    ArchiveHandler.delete_files_from_archive(archive_path, json_files)
                    yaml_uuid = UuidHandler.generate_uuid(UuidHandler.load_existing_uuids())
                
                # 4. è½¬æ¢ä¸ºJSONæ ¼å¼
                json_data = JsonHandler.convert_yaml_to_json(yaml_data)
                json_data["uuid"] = yaml_uuid
                
                # 5. ä¿å­˜JSONæ–‡ä»¶
                json_path = os.path.join(temp_dir, f"{yaml_uuid}.json")
                if not JsonHandler.save(json_path, json_data):
                    logger.error(f"[#process]ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {os.path.basename(archive_path)}")
                    return None
                
                # 6. æ·»åŠ JSONåˆ°å‹ç¼©åŒ…å¹¶åˆ é™¤YAML
                if ArchiveHandler.add_json_to_archive(archive_path, json_path, f"{yaml_uuid}.json"):
                    # åˆ é™¤YAMLæ–‡ä»¶
                    ArchiveHandler.delete_files_from_archive(archive_path, [f"{yaml_uuid}.yaml"])
                    logger.info(f"[#process]âœ… YAMLè½¬æ¢å®Œæˆ: {os.path.basename(archive_path)}")
                    return json_data
                
                logger.error(f"[#process]æ›´æ–°å‹ç¼©åŒ…å¤±è´¥: {os.path.basename(archive_path)}")
                return None
                
            finally:
                shutil.rmtree(temp_dir, ignore_errors=True)
                
        except Exception as e:
            logger.error(f"[#process]è½¬æ¢å¤±è´¥ {os.path.basename(archive_path)}: {str(e)}")
            return None

# å®šä¹‰æ–‡ä»¶è·¯å¾„å’Œçº¿ç¨‹é”
# uuid_file_path = r'E:\1BACKUP\ehv\uuid.md'  # å­˜å‚¨å”¯ä¸€ UUID çš„ Markdown æ–‡ä»¶
uuid_lock = threading.Lock()  # ç”¨äºä¿æŠ¤UUIDæ–‡ä»¶æ“ä½œçš„çº¿ç¨‹é”

class FastUUIDLoader:
    def __init__(self, yaml_path):
        self.yaml_path = yaml_path
        self.cache_path = os.path.splitext(yaml_path)[0] + '.cache'
        self._data = None
        self._index = {}  # åˆå§‹åŒ–ç´¢å¼•å­—å…¸
        self._lock = threading.Lock()
        
        # åˆå§‹åŒ–è¿›åº¦å±æ€§
        self.progress = {
            'total_steps': 4,
            'current_step': 0,
            'message': 'åˆå§‹åŒ–ä¸­',
            'percentage': 0.0,
            'timestamp': time.time()
        }
        
        # è‡ªåŠ¨æ£€æµ‹å¹¶ç”Ÿæˆä¼˜åŒ–æ ¼å¼
        if not self._check_cache_valid():
            self._build_cache()
    
    def _check_cache_valid(self):
        """æ ¡éªŒç¼“å­˜æœ‰æ•ˆæ€§"""
        if not os.path.exists(self.cache_path):
            return False
        # æ ¡éªŒæ—¶é—´æˆ³å’Œå¤§å°
        yaml_mtime = os.path.getmtime(self.yaml_path)
        cache_mtime = os.path.getmtime(self.cache_path)
        return yaml_mtime <= cache_mtime
    
    def _build_cache(self):
        """ä¿®å¤ç´¢å¼•åˆå§‹åŒ–é—®é¢˜"""
        with self._lock:
            try:
                # åˆå§‹åŒ–ç´¢å¼•
                self._index = {}
                
                # å¢åŠ é˜¶æ®µæ ‡è¯†
                self._update_progress("å¼€å§‹è§£æYAMLæ–‡ä»¶", 5)
                
                # ä½¿ç”¨æ›´é«˜æ•ˆçš„è§£ææ–¹å¼
                with open(self.yaml_path, 'rb') as f:
                    data = yaml.load(f, Loader=yaml.CSafeLoader)
                
                # æ·»åŠ æ•°æ®æ ¡éªŒ
                if not data or not isinstance(data, list):
                    raise ValueError("æ— æ•ˆçš„YAMLæ•°æ®æ ¼å¼")
                
                self._update_progress("æ•°æ®æ ¡éªŒé€šè¿‡", 20)
                
                # åˆ†æ‰¹æ¬¡æ„å»ºç´¢å¼•
                batch_size = 5000
                total = len(data)
                self._update_progress("å¼€å§‹æ„å»ºç´¢å¼•", 30)
                
                for i in range(0, total, batch_size):
                    batch = data[i:i+batch_size]
                    for j, record in enumerate(batch):
                        uuid = record.get('UUID')
                        if uuid:
                            self._index[uuid] = i + j
                    # å®æ—¶æ›´æ–°è¿›åº¦
                    progress = 30 + 60 * (i + batch_size) / total
                    self._update_progress(f"ç´¢å¼•æ„å»ºä¸­ ({i+batch_size}/{total})", min(90, progress))
                
                self._update_progress("å†™å…¥ç¼“å­˜æ–‡ä»¶", 95)
                with open(self.cache_path, 'wb') as f:
                    np.savez_compressed(f, data=data, index=self._index)
                
                self._update_progress("å®Œæˆ", 100)
                
            except Exception as e:
                self._index = None  # æ„å»ºå¤±è´¥æ—¶é‡ç½®ç´¢å¼•
                raise

    def _update_progress(self, message, percentage):
        """æ›´æ–°è¿›åº¦ä¿¡æ¯"""
        self.progress.update({
            'message': message,
            'percentage': min(100, max(0, percentage)),
            'timestamp': time.time()
        })
        logger.info(f"[ç¼“å­˜æ„å»º] {message} - è¿›åº¦: {self.progress['percentage']:.1f}%")

    def get_loading_progress(self):
        """è·å–å½“å‰åŠ è½½è¿›åº¦"""
        return self.progress
    
    def _load_cache(self):
        """åŠ è½½ä¼˜åŒ–æ ¼å¼"""
        with open(self.cache_path, 'rb') as f:
            cache = np.load(f, allow_pickle=True)
            self._data = cache['data'].tolist()
            self._index = cache['index'].item() if 'index' in cache else {}
    
    def get_uuids(self):
        """è·å–UUIDé›†åˆ"""
        if self._index is None:
            self._load_cache()
        return set(self._index.keys())
    
    def get_record(self, uuid):
        """å¿«é€ŸæŸ¥è¯¢è®°å½•"""
        if self._index is None:
            self._load_cache()
        return self._data[self._index[uuid]]

def repair_uuid_records(uuid_record_path):
    """ä¿®å¤æŸåçš„UUIDè®°å½•æ–‡ä»¶ã€‚"""
    backup_path = f"{uuid_record_path}.bak"
    
    # å¦‚æœå­˜åœ¨å¤‡ä»½æ–‡ä»¶ï¼Œå°è¯•ä»å¤‡ä»½æ¢å¤
    if os.path.exists(backup_path):
        try:
            with open(backup_path, 'r', encoding='utf-8') as file:
                records = yaml.safe_load(file) or []
                if isinstance(records, list):
                    logger.info("[#process]ä»å¤‡ä»½æ–‡ä»¶æ¢å¤è®°å½•æˆåŠŸ")
                    return records
        except Exception:
            logger.error("[#process]ä»å¤‡ä»½æ–‡ä»¶æ¢å¤è®°å½•å¤±è´¥")
            pass
    
    # å°è¯•ä¿®å¤åŸæ–‡ä»¶
    try:
        with open(uuid_record_path, 'r', encoding='utf-8', errors='ignore') as file:
            content = file.read()
            # å°è¯•è§£ææ¯ä¸ªè®°å½•
            records = []
            current_record = {}
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    if current_record:
                        records.append(current_record)
                        current_record = {}
                    continue
                
                if line.startswith('- ') or line.startswith('UUID:'):
                    if current_record:
                        records.append(current_record)
                    current_record = {}
                
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip('- ').strip()
                    value = value.strip()
                    if key and value:
                        current_record[key] = value
            
            if current_record:
                records.append(current_record)
            
            # éªŒè¯è®°å½•
            valid_records = []
            for record in records:
                if 'UUID' in record:
                    valid_records.append(record)
            
            logger.info(f"[#process]æˆåŠŸä¿®å¤è®°å½•æ–‡ä»¶ï¼Œæ¢å¤äº† {len(valid_records)} æ¡è®°å½•")
            return valid_records
    except Exception as e:
        logger.error(f"[#process]ä¿®å¤UUIDè®°å½•æ–‡ä»¶å¤±è´¥: {e}")
        return []

def load_existing_uuids():
    """ä»JSONè®°å½•ä¸­åŠ è½½ç°æœ‰UUID"""
    # ä»…ä»å½“å‰ç›®å½•åŠ è½½UUID
    uuids = set()
    for root, _, files in os.walk(os.path.dirname(args.path or r"E:\1EHV")):
        for file in files:
            if file.endswith('.json'):
                try:
                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if 'uuid' in data:
                            uuids.add(data['uuid'])
                except Exception:
                    continue
    return uuids

def add_uuid_to_file(uuid, timestamp, archive_name, artist_name, relative_path=None, cache=None):
    """å°†ç”Ÿæˆçš„ UUID æ·»åŠ åˆ°ç¼“å­˜"""
    record = {
        'UUID': uuid,
        'CreatedAt': timestamp,
        'ArchiveName': archive_name,
        'ArtistName': artist_name,
        'LastModified': timestamp,
        'LastPath': relative_path or os.path.join(artist_name, archive_name) if artist_name else archive_name
    }
    
    # ä½¿ç”¨ç¼“å­˜ä»£æ›¿ç›´æ¥å†™å…¥
    if cache is not None:
        cache[uuid] = {
            "timestamps": {
                timestamp: {
                    "archive_name": archive_name,
                    "artist_name": artist_name,
                    "relative_path": relative_path
                }
            }
        }
        return

class PathHandler:
    """è·¯å¾„å¤„ç†ç±»"""
    
    @staticmethod
    def get_artist_name(target_directory: str, archive_path: str, mode: str = 'multi') -> str:
        """ä»å‹ç¼©æ–‡ä»¶è·¯å¾„ä¸­æå–è‰ºæœ¯å®¶åç§°
        
        Args:
            target_directory: ç›®æ ‡ç›®å½•è·¯å¾„
            archive_path: å‹ç¼©æ–‡ä»¶è·¯å¾„
            mode: å¤„ç†æ¨¡å¼ï¼Œ'multi'è¡¨ç¤ºå¤šäººæ¨¡å¼ï¼Œ'single'è¡¨ç¤ºå•äººæ¨¡å¼
            
        Returns:
            str: è‰ºæœ¯å®¶åç§°
        """
        if mode == 'single':
            # å•äººæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ç›®æ ‡ç›®å½•çš„æœ€åä¸€ä¸ªæ–‡ä»¶å¤¹åä½œä¸ºç”»å¸ˆå
            return Path(target_directory).name
        else:
            # å¤šäººæ¨¡å¼ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„çš„ç¬¬ä¸€çº§å­æ–‡ä»¶å¤¹åä½œä¸ºç”»å¸ˆå
            try:
                # å°†è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                archive_path = Path(archive_path)
                target_path = Path(target_directory)
                
                # è·å–ç›¸å¯¹äºç›®æ ‡ç›®å½•çš„è·¯å¾„
                relative_path = archive_path.relative_to(target_path)
                
                # è·å–ç¬¬ä¸€çº§å­æ–‡ä»¶å¤¹å
                if len(relative_path.parts) > 0:
                    return relative_path.parts[0]
                
                logger.warning(f"[#process]æ— æ³•ä»è·¯å¾„æå–ç”»å¸ˆå: {archive_path}")
                return ""
                
            except Exception as e:
                logger.error(f"[#process]æå–ç”»å¸ˆåå¤±è´¥: {str(e)}")
                return ""
    
    @staticmethod
    def get_relative_path(target_directory: str, archive_path: str) -> str:
        """è·å–ç›¸å¯¹è·¯å¾„
        
        Args:
            target_directory: ç›®æ ‡ç›®å½•è·¯å¾„
            archive_path: å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: ç›¸å¯¹è·¯å¾„ï¼Œä¸åŒ…å«æ–‡ä»¶å
        """
        try:
            # å°†è·¯å¾„è½¬æ¢ä¸ºPathå¯¹è±¡å¹¶è§„èŒƒåŒ–
            archive_path = Path(archive_path).resolve()
            target_path = Path(target_directory).resolve()
            
            # è·å–ç›¸å¯¹è·¯å¾„
            relative_path = archive_path.relative_to(target_path)
            
            # å¦‚æœæ˜¯ç›´æ¥åœ¨ç›®æ ‡ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œè¿”å›"."
            if not relative_path.parent.parts:
                return "."
                
            # è¿”å›çˆ¶ç›®å½•çš„ç›¸å¯¹è·¯å¾„ï¼ˆä¸åŒ…å«æ–‡ä»¶åï¼‰ï¼Œä¿æŒåŸå§‹è·¯å¾„åˆ†éš”ç¬¦
            relative_str = str(relative_path.parent)
            # å¦‚æœè·¯å¾„ä¸­åŒ…å«åæ–œæ ï¼Œä¿æŒåŸæ ·
            if '\\' in archive_path.as_posix():
                relative_str = relative_str.replace('/', '\\')
            return relative_str
            
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œè®°å½•é”™è¯¯ä½†è¿”å›ä¸€ä¸ªå®‰å…¨çš„é»˜è®¤å€¼
            logger.error(f"[#process]è·å–ç›¸å¯¹è·¯å¾„å¤±è´¥ ({archive_path}): {str(e)}")
            return "."
    
    @staticmethod
    def get_uuid_path(uuid_directory: str, timestamp: str) -> str:
        """æ ¹æ®æ—¶é—´æˆ³ç”ŸæˆæŒ‰å¹´æœˆæ—¥åˆ†å±‚çš„UUIDæ–‡ä»¶è·¯å¾„"""
        date = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
        year = str(date.year)
        month = f"{date.month:02d}"
        day = f"{date.day:02d}"
        
        # åˆ›å»ºå¹´æœˆæ—¥å±‚çº§ç›®å½•
        year_dir = os.path.join(uuid_directory, year)
        month_dir = os.path.join(year_dir, month)
        day_dir = os.path.join(month_dir, day)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(day_dir, exist_ok=True)
        
        return day_dir
    
    @staticmethod
    def get_short_path(long_path: str) -> str:
        """å°†é•¿è·¯å¾„è½¬æ¢ä¸ºçŸ­è·¯å¾„æ ¼å¼"""
        try:
            import win32api
            return win32api.GetShortPathName(long_path)
        except ImportError:
            return long_path

class UuidHandler:
    """UUIDå¤„ç†ç±»"""
    
    @staticmethod
    def generate_uuid(existing_uuids: set) -> str:
        """ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„16ä½UUID"""
        while True:
            new_uuid = generate(size=16)
            if new_uuid not in existing_uuids:
                return new_uuid
    
    @staticmethod
    def load_existing_uuids() -> set:
        """ä»JSONè®°å½•ä¸­åŠ è½½ç°æœ‰UUID"""
        logger.info("[#current_stats]ğŸ” å¼€å§‹åŠ è½½ç°æœ‰UUID...")
        start_time = time.time()
        
        json_record_path = r'E:\1BACKUP\ehv\uuid\uuid_records.json'
        if not os.path.exists(json_record_path):
            return set()
            
        try:
            with open(json_record_path, 'r', encoding='utf-8') as f:
                records = json.load(f)
            # ä»recordé”®è·å–æ•°æ®
            uuids = set(records.get("record", {}).keys())
            
            elapsed = time.time() - start_time
            logger.info(f"[#current_stats]âœ… åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(uuids)} ä¸ªUUIDï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
            return uuids
            
        except Exception as e:
            logger.error(f"[#process]åŠ è½½UUIDè®°å½•å¤±è´¥: {e}")
            return set()

class YamlHandler:
    """YAMLæ–‡ä»¶å¤„ç†ç±»"""
    
    @staticmethod
    def read_yaml(yaml_path: str) -> list:
        """è¯»å–YAMLæ–‡ä»¶å†…å®¹ï¼Œå¦‚æœæ–‡ä»¶æŸååˆ™å°è¯•ä¿®å¤"""
        if not os.path.exists(yaml_path):
            return []
            
        try:
            with open(yaml_path, 'r', encoding='utf-8') as file:
                data = yaml.safe_load(file)
                if not isinstance(data, list):
                    data = [data] if data is not None else []
                return data
        except yaml.YAMLError as e:
            logger.error(f"YAMLæ–‡ä»¶ {yaml_path} å·²æŸåï¼Œå°è¯•ä¿®å¤...")
            return YamlHandler.repair_yaml_file(yaml_path)
        except Exception as e:
            logger.error(f"è¯»å–YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
            return []
    
    @staticmethod
    def write_yaml(yaml_path: str, data: list) -> bool:
        """å°†æ•°æ®å†™å…¥YAMLæ–‡ä»¶ï¼Œç¡®ä¿å†™å…¥å®Œæ•´æ€§"""
        temp_path = yaml_path + '.tmp'
        try:
            with open(temp_path, 'w', encoding='utf-8') as file:
                yaml.dump(data, file, allow_unicode=True)
            
            try:
                with open(temp_path, 'r', encoding='utf-8') as file:
                    yaml.safe_load(file)
            except yaml.YAMLError:
                logger.error(f"å†™å…¥çš„YAMLæ–‡ä»¶éªŒè¯å¤±è´¥: {yaml_path}")
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                return False
                
            if os.path.exists(yaml_path):
                os.replace(temp_path, yaml_path)
            else:
                os.rename(temp_path, yaml_path)
            return True
                
        except Exception as e:
            logger.error(f"å†™å…¥YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False
    
    @staticmethod
    def repair_yaml_file(yaml_path: str) -> list:
        """ä¿®å¤æŸåçš„YAMLæ–‡ä»¶"""
        try:
            with open(yaml_path, 'r', encoding='utf-8') as file:
                lines = file.readlines()

            if not lines:
                return []

            valid_data = []
            current_record = []
            
            for line in lines:
                current_record.append(line)
                if line.strip() == '' or line == lines[-1]:
                    try:
                        record_str = ''.join(current_record)
                        parsed_data = yaml.safe_load(record_str)
                        if isinstance(parsed_data, list):
                            valid_data.extend(parsed_data)
                        elif parsed_data is not None:
                            valid_data.append(parsed_data)
                    except yaml.YAMLError:
                        pass
                    current_record = []

            if not valid_data:
                return []

            YamlHandler.write_yaml(yaml_path, valid_data)
            return valid_data

        except Exception as e:
            logger.error(f"ä¿®å¤YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
            return []

class ArchiveProcessor:
    """å‹ç¼©æ–‡ä»¶å¤„ç†ç±»"""
    
    def __init__(self, target_directory: str, uuid_directory: str, 
                 max_workers: int = 5, order: str = 'mtime'):
        self.target_directory = target_directory
        self.uuid_directory = uuid_directory
        self.max_workers = max_workers
        self.order = order  # ä¿å­˜æ’åºæ–¹å¼
        self.total_archives = 0  # æ€»æ–‡ä»¶æ•°
        self.processed_archives = 0  # å·²å¤„ç†æ–‡ä»¶æ•°
    
    def process_archives(self) -> bool:
        """å¤„ç†æ‰€æœ‰å‹ç¼©æ–‡ä»¶ï¼ˆSSDä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            logger.info("[#current_stats]ğŸ” å¼€å§‹æ‰«æå‹ç¼©æ–‡ä»¶")
            
            # ç›´æ¥å¿«é€Ÿæ‰«æSSD
            archive_files = []
            for root, _, files in os.walk(self.target_directory):
                for file in files:
                    if file.endswith(('.zip', '.rar', '.7z')):
                        archive_files.append(os.path.join(root, file))
            
            self.total_archives = len(archive_files)
            self.processed_archives = 0
            logger.info(f"[#current_stats]å…±å‘ç° {self.total_archives} ä¸ªå‹ç¼©æ–‡ä»¶")
            
            # ä½¿ç”¨å†…å­˜ç¼“å­˜å¤„ç†
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(self.process_single_archive, path, timestamp) 
                         for path in archive_files]
                
                for future in as_completed(futures):
                    future.result()
                    self.processed_archives += 1
                    progress = (self.processed_archives / self.total_archives) * 100
                    logger.info(f"[@current_progress]å¤„ç†è¿›åº¦: ({self.processed_archives}/{self.total_archives}) {progress:.1f}%")
            
            return True
        finally:
            logger.info("[#current_stats]âœ¨ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
    
    def process_single_archive(self, archive_path: str, timestamp: str) -> bool:
        """å¤„ç†å•ä¸ªå‹ç¼©æ–‡ä»¶
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä¿å­˜åŸå§‹æ—¶é—´æˆ³
            # original_mtime = os.path.getmtime(archive_path)
            # original_atime = os.path.getatime(archive_path)
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            artist_name = PathHandler.get_artist_name(self.target_directory, archive_path, args.mode if hasattr(args, 'mode') else 'multi')
            archive_name = os.path.basename(archive_path)
            relative_path = PathHandler.get_relative_path(self.target_directory, archive_path)
            
            # æ£€æŸ¥å‹ç¼©åŒ…ä¸­çš„JSONæ–‡ä»¶å’ŒYAMLæ–‡ä»¶
            valid_json_files, yaml_files, all_json_files = self._find_valid_json_files(archive_path)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡åä½†æ—¶é—´æˆ³ä¸åŒçš„JSONæ–‡ä»¶
            json_base_names = {}
            for name, _ in valid_json_files:
                base_name = os.path.splitext(name)[0]
                if base_name in json_base_names:
                    logger.info(f"[#process]å‘ç°é‡åJSONæ–‡ä»¶ï¼Œå°†é‡æ–°ç”Ÿæˆ: {os.path.basename(archive_path)}")
                    return self._handle_multiple_json(archive_path, valid_json_files, yaml_files, all_json_files, archive_name, artist_name, relative_path, timestamp)
                json_base_names[base_name] = True
            
            # å¦‚æœå­˜åœ¨YAMLæ–‡ä»¶ï¼Œéœ€è¦åˆ é™¤å¹¶é‡æ–°ç”ŸæˆJSON
            if yaml_files:
                logger.info(f"[#process]å‘ç°YAMLæ–‡ä»¶ï¼Œå°†åˆ é™¤å¹¶ç”Ÿæˆæ–°JSON: {os.path.basename(archive_path)}")
                return self._handle_multiple_json(archive_path, valid_json_files, yaml_files, all_json_files, archive_name, artist_name, relative_path, timestamp)
            
            # æ ¹æ®JSONæ–‡ä»¶æ•°é‡å†³å®šå¤„ç†æ–¹å¼
            if len(valid_json_files) == 1 and len(all_json_files) == 1:
                return self._handle_single_json(archive_path, valid_json_files[0], archive_name, artist_name, relative_path, timestamp)
            else:
                return self._handle_multiple_json(archive_path, valid_json_files, yaml_files, all_json_files, archive_name, artist_name, relative_path, timestamp)
                
        except subprocess.CalledProcessError:
            logger.error(f"[#process]å‘ç°æŸåçš„å‹ç¼©åŒ…: {archive_path}")
            return True
        except Exception as e:
            logger.error(f"[#process]å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™ {archive_path}: {str(e)}")
            return True
    
    def _find_valid_json_files(self, archive_path: str) -> tuple[List[tuple], List[str], List[str]]:
        """æŸ¥æ‰¾å‹ç¼©åŒ…ä¸­çš„æœ‰æ•ˆJSONæ–‡ä»¶å’ŒYAMLæ–‡ä»¶
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            
        Returns:
            tuple: (æœ‰æ•ˆJSONæ–‡ä»¶åˆ—è¡¨[(æ–‡ä»¶å, JSONå†…å®¹)], YAMLæ–‡ä»¶åˆ—è¡¨[æ–‡ä»¶å], æ‰€æœ‰JSONæ–‡ä»¶åˆ—è¡¨[æ–‡ä»¶å])
        """
        valid_json_files = []
        yaml_files = []
        all_json_files = []  # å­˜å‚¨æ‰€æœ‰JSONæ–‡ä»¶ï¼ŒåŒ…æ‹¬æ— æ•ˆçš„
        
        try:
            # å°è¯•ä½¿ç”¨zipfile
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for name in zf.namelist():
                    if name.endswith('.json'):
                        all_json_files.append(name)
                        try:
                            with zf.open(name) as f:
                                json_content = orjson.loads(f.read())
                                if "uuid" in json_content and "timestamps" in json_content:
                                    valid_json_files.append((name, json_content))
                        except Exception:
                            continue
                    elif name.endswith('.yaml'):
                        yaml_files.append(name)
        except zipfile.BadZipFile:
            # å¦‚æœä¸æ˜¯zipæ–‡ä»¶ï¼Œä½¿ç”¨7z
            try:
                temp_dir = os.path.join(os.path.dirname(archive_path), '.temp_extract')
                os.makedirs(temp_dir, exist_ok=True)
                try:
                    # æå–æ‰€æœ‰JSONå’ŒYAMLæ–‡ä»¶
                    subprocess.run(
                        ['7z', 'e', archive_path, '*.json', '*.yaml', f"-o{temp_dir}"],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                        check=True
                    )
                    # å¤„ç†JSONæ–‡ä»¶
                    for file in os.listdir(temp_dir):
                        if file.endswith('.json'):
                            all_json_files.append(file)
                            try:
                                with open(os.path.join(temp_dir, file), 'rb') as f:
                                    json_content = orjson.loads(f.read())
                                    if "uuid" in json_content and "timestamps" in json_content:
                                        valid_json_files.append((file, json_content))
                            except Exception:
                                continue
                        elif file.endswith('.yaml'):
                            yaml_files.append(file)
                finally:
                    shutil.rmtree(temp_dir, ignore_errors=True)
            except subprocess.CalledProcessError:
                pass
                
        return valid_json_files, yaml_files, all_json_files
    
    def _handle_single_json(self, archive_path: str, json_file: tuple, archive_name: str, 
                          artist_name: str, relative_path: str, timestamp: str) -> bool:
        """å¤„ç†å•ä¸ªJSONæ–‡ä»¶çš„æƒ…å†µ
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            json_file: (æ–‡ä»¶å, JSONå†…å®¹)å…ƒç»„
            archive_name: å‹ç¼©åŒ…åç§°
            artist_name: è‰ºæœ¯å®¶åç§°
            relative_path: ç›¸å¯¹è·¯å¾„
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        json_name, json_content = json_file
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if not JsonHandler.check_and_update_record(json_content, archive_name, artist_name, relative_path, timestamp):
            logger.info(f"[#process]è®°å½•æ— éœ€æ›´æ–°: {os.path.basename(archive_path)}")
            return True
            
        # æ›´æ–°è®°å½•
        logger.info(f"[#process]æ£€æµ‹åˆ°è®°å½•éœ€è¦æ›´æ–°: {os.path.basename(archive_path)}")
        json_content = JsonHandler.update_record(json_content, archive_name, artist_name, relative_path, timestamp)
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¹¶æ›´æ–°å‹ç¼©åŒ…
        temp_dir = os.path.join(os.path.dirname(archive_path), '.temp_update')
        os.makedirs(temp_dir, exist_ok=True)
        try:
            temp_json = os.path.join(temp_dir, json_name)
            if JsonHandler.save(temp_json, json_content):
                if ArchiveHandler.add_json_to_archive(archive_path, temp_json, json_name):
                    logger.info(f"[#update]âœ… å·²æ›´æ–°å‹ç¼©åŒ…ä¸­çš„JSONè®°å½•: {archive_name}")
                    return True
            return False
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    def _handle_multiple_json(self, archive_path: str, valid_json_files: List[tuple], yaml_files: List[str], 
                            all_json_files: List[str], archive_name: str, artist_name: str, 
                            relative_path: str, timestamp: str) -> bool:
        """å¤„ç†å¤šä¸ªJSONæ–‡ä»¶æˆ–éœ€è¦ç”Ÿæˆæ–°JSONçš„æƒ…å†µ
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            valid_json_files: æœ‰æ•ˆçš„JSONæ–‡ä»¶åˆ—è¡¨
            yaml_files: YAMLæ–‡ä»¶åˆ—è¡¨
            all_json_files: æ‰€æœ‰JSONæ–‡ä»¶åˆ—è¡¨
            archive_name: å‹ç¼©åŒ…åç§°
            artist_name: è‰ºæœ¯å®¶åç§°
            relative_path: ç›¸å¯¹è·¯å¾„
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        # åˆ é™¤æ‰€æœ‰JSONå’ŒYAMLæ–‡ä»¶
        files_to_delete = all_json_files  # åˆ é™¤æ‰€æœ‰JSONæ–‡ä»¶ï¼ŒåŒ…æ‹¬æ— æ•ˆçš„
        files_to_delete.extend(yaml_files)
        
        if files_to_delete:
            logger.info(f"[#process]åˆ é™¤ç°æœ‰æ–‡ä»¶: {os.path.basename(archive_path)}")
            # å¼ºåŠ›åˆ é™¤ï¼šå…ˆå°è¯•ä½¿ç”¨é€šé…ç¬¦åˆ é™¤
            try:
                ArchiveHandler.delete_files_from_archive(archive_path, ['*.json', '*.yaml'])
            except Exception:
                pass
            # ç„¶åå†é€ä¸ªåˆ é™¤å…·ä½“æ–‡ä»¶
            ArchiveHandler.delete_files_from_archive(archive_path, files_to_delete)
        
        # åˆ›å»ºæ–°çš„UUIDè®°å½•
        uuid_value = UuidHandler.generate_uuid(UuidHandler.load_existing_uuids())
        json_filename = f"{uuid_value}.json"
        
        # è·å–æŒ‰å¹´æœˆæ—¥åˆ†å±‚çš„ç›®å½•è·¯å¾„
        day_dir = PathHandler.get_uuid_path(self.uuid_directory, timestamp)
        json_path = os.path.join(day_dir, json_filename)
        
        # å‡†å¤‡æ–°çš„è®°å½•æ•°æ®
        new_record = {
            "archive_name": archive_name,
            "artist_name": artist_name,
            "relative_path": relative_path
        }
        
        # åˆ›å»ºæ–°çš„JSONæ–‡ä»¶
        json_data = {
            "uuid": uuid_value,
            "timestamps": {
                timestamp: new_record
            }
        }
        
        # ä¿å­˜å¹¶æ·»åŠ æ–°JSONæ–‡ä»¶
        if JsonHandler.save(json_path, json_data):
            logger.info(f"[#process]åˆ›å»ºæ–°JSON: {json_filename}")
            if ArchiveHandler.add_json_to_archive(archive_path, json_path, json_filename):
                logger.info(f"[#update]âœ… å·²æ·»åŠ æ–°JSONåˆ°å‹ç¼©åŒ…: {archive_name}")
                return True
            else:
                logger.error(f"[#process]æ·»åŠ JSONåˆ°å‹ç¼©åŒ…å¤±è´¥: {archive_name}")
        else:
            logger.error(f"[#process]JSONæ–‡ä»¶ä¿å­˜å¤±è´¥: {archive_name}")
            
        return False

    def _batch_update_records(self, force=False):
        """æ‰¹é‡æ›´æ–°è®°å½•åˆ°JSONï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        if len(self.uuid_cache) >= self.batch_size or force:
            json_record_path = r'E:\1BACKUP\ehv\uuid\uuid_records.json'
            
            # åŠ è½½ç°æœ‰è®°å½•
            existing_records = JsonHandler.load(json_record_path) or {"record": {}}
            
            # åˆå¹¶åˆ°recordé”®ä¸‹
            for uuid, data in self.uuid_cache.items():
                if uuid not in existing_records["record"]:
                    existing_records["record"][uuid] = data
                else:
                    # åˆå¹¶æ—¶é—´æˆ³è®°å½•
                    existing_records["record"][uuid]["timestamps"].update(data["timestamps"])
            
            # ä½¿ç”¨åŸå­æ“ä½œä¿å­˜
            temp_path = f"{json_record_path}.tmp"
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(existing_records, f, ensure_ascii=False, indent=2)
            
            # æ–‡ä»¶æ›¿æ¢æ“ä½œ
            if os.path.exists(json_record_path):
                os.replace(temp_path, json_record_path)
            else:
                os.rename(temp_path, json_record_path)
            
            logger.info(f"[#process]âœ… æ‰¹é‡æ›´æ–° {len(self.uuid_cache)} æ¡è®°å½•")
            self.uuid_cache.clear()

def main():
    """ä¸»å‡½æ•°"""
    # å®šä¹‰å¤é€‰æ¡†é€‰é¡¹
    checkbox_options = [
        ("æ— ç”»å¸ˆæ¨¡å¼ - ä¸æ·»åŠ ç”»å¸ˆå", "no_artist", "--no-artist"),
        ("ä¿æŒæ—¶é—´æˆ³ - ä¿æŒæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´", "keep_timestamp", "--keep-timestamp", True),
        ("å¤šç”»å¸ˆæ¨¡å¼ - å¤„ç†æ•´ä¸ªç›®å½•", "multi_mode", "--mode multi"),
        ("å•ç”»å¸ˆæ¨¡å¼ - åªå¤„ç†å•ä¸ªç”»å¸ˆçš„æ–‡ä»¶å¤¹", "single_mode", "--mode single"),
        ("ä»å‰ªè´´æ¿è¯»å–è·¯å¾„", "clipboard", "-c", True),  # é»˜è®¤å¼€å¯
        ("è‡ªåŠ¨åºåˆ— - æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹", "auto_sequence", "-a"),  # æ·»åŠ åºåˆ—æ¨¡å¼é€‰é¡¹
        ("é‡ç»„UUID - æŒ‰æ—¶é—´é‡ç»„UUIDæ–‡ä»¶", "reorganize", "-r"),  # æ·»åŠ é‡ç»„é€‰é¡¹
        ("æ›´æ–°è®°å½• - æ›´æ–°UUIDè®°å½•æ–‡ä»¶", "update_records", "-u"),  # æ·»åŠ æ›´æ–°è®°å½•é€‰é¡¹
        ("è½¬æ¢YAML - è½¬æ¢ç°æœ‰YAMLåˆ°JSON", "convert_yaml", "--convert"),  # æ·»åŠ YAMLè½¬æ¢é€‰é¡¹
        ("æŒ‰è·¯å¾„æ’åº - æŒ‰æ–‡ä»¶è·¯å¾„å‡åºå¤„ç†", "order_path", "--order path"),
        ("æŒ‰æ—¶é—´æ’åº - æŒ‰ä¿®æ”¹æ—¶é—´å€’åºå¤„ç†", "order_mtime", "--order mtime", True),  # é»˜è®¤é€‰ä¸­
    ]

    # å®šä¹‰è¾“å…¥æ¡†é€‰é¡¹
    input_options = [
        ("è·¯å¾„", "path", "--path", "", "è¾“å…¥è¦å¤„ç†çš„è·¯å¾„ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤è·¯å¾„"),
    ]

    # é¢„è®¾é…ç½®
    preset_configs = {
        "æ ‡å‡†å¤šç”»å¸ˆ": {
            "description": "æ ‡å‡†å¤šç”»å¸ˆæ¨¡å¼ï¼Œä¼šæ·»åŠ ç”»å¸ˆå",
            "checkbox_options": ["keep_timestamp", "multi_mode", "clipboard"],
            "input_values": {"path": ""}
        },
        "æ ‡å‡†å•ç”»å¸ˆ": {
            "description": "æ ‡å‡†å•ç”»å¸ˆæ¨¡å¼ï¼Œä¼šæ·»åŠ ç”»å¸ˆå", 
            "checkbox_options": ["keep_timestamp", "single_mode", "clipboard"],
            "input_values": {"path": ""}
        },
        "æ— ç”»å¸ˆæ¨¡å¼": {
            "description": "ä¸æ·»åŠ ç”»å¸ˆåçš„é‡å‘½åæ¨¡å¼",
            "checkbox_options": ["no_artist", "keep_timestamp", "clipboard"],
            "input_values": {"path": ""}
        },
        "å®Œæ•´åºåˆ—": {
            "description": "æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹ï¼šUUID-JSON -> è‡ªåŠ¨æ–‡ä»¶å -> UUID-JSON",
            "checkbox_options": ["keep_timestamp", "clipboard", "auto_sequence"],
            "input_values": {"path": ""}
        },
        "UUIDæ›´æ–°": {
            "description": "é‡ç»„UUIDæ–‡ä»¶ç»“æ„å¹¶æ›´æ–°è®°å½•",
            "checkbox_options": ["reorganize", "update_records"],
            "input_values": {"path": ""}
        },
        "å®Œæ•´ç»´æŠ¤": {
            "description": "æ‰§è¡Œå®Œæ•´åºåˆ—å¹¶æ›´æ–°UUIDè®°å½•",
            "checkbox_options": ["keep_timestamp", "clipboard", "auto_sequence", "reorganize", "update_records"],
            "input_values": {"path": ""}
        },
        "YAMLè½¬æ¢": {
            "description": "è½¬æ¢ç°æœ‰YAMLæ–‡ä»¶åˆ°JSONæ ¼å¼",
            "checkbox_options": ["convert_yaml"],
            "input_values": {"path": ""}
        }
    }

    # åˆ›å»ºå¹¶è¿è¡Œé…ç½®ç•Œé¢
    app = create_config_app(
        program=__file__,
        checkbox_options=checkbox_options,
        input_options=input_options,
        title="UUID-JSON å·¥å…·",
        preset_configs=preset_configs
    )
    app.run()

def reorganize_uuid_files(uuid_directory=r'E:\1BACKUP\ehv\uuid'):
    """æ ¹æ®æœ€åä¿®æ”¹æ—¶é—´é‡æ–°ç»„ç»‡UUIDæ–‡ä»¶çš„ç›®å½•ç»“æ„"""
    logger.info("[#current_stats]ğŸ”„ å¼€å§‹é‡æ–°ç»„ç»‡UUIDæ–‡ä»¶...")
    
    json_record_path = os.path.join(uuid_directory, 'uuid_records.json')
    if not os.path.exists(json_record_path):
        logger.error("[#process]âŒ UUIDè®°å½•æ–‡ä»¶ä¸å­˜åœ¨")
        return
        
    try:
        with open(json_record_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
            
        total_records = len(records)
        processed = 0
        
        for uuid, data in records.items():
            if not data.get("timestamps"):
                continue
                
            latest_timestamp = max(data["timestamps"].keys())
            
            try:
                date = datetime.strptime(latest_timestamp, "%Y-%m-%d %H:%M:%S")
                year = str(date.year)
                month = f"{date.month:02d}"
                day = f"{date.day:02d}"
                
                year_dir = os.path.join(uuid_directory, year)
                month_dir = os.path.join(year_dir, month)
                day_dir = os.path.join(month_dir, day)
                target_path = os.path.join(day_dir, f"{uuid}.json")
                
                current_json_path = None
                for root, _, files in os.walk(uuid_directory):
                    if f"{uuid}.json" in files:
                        current_json_path = os.path.join(root, f"{uuid}.json")
                        break
                
                if current_json_path and current_json_path != target_path:
                    os.makedirs(day_dir, exist_ok=True)
                    shutil.move(current_json_path, target_path)
                    logger.info(f"[#process]âœ… å·²ç§»åŠ¨: {uuid}.json")
                
                processed += 1
                logger.info(f"[@current_progress]é‡ç»„è¿›åº¦ {processed}/{total_records} ({(processed/total_records*100):.1f}%)")
                    
            except ValueError as e:
                logger.error(f"[#process]âŒ UUID {uuid} çš„æ—¶é—´æˆ³æ ¼å¼æ— æ•ˆ: {latest_timestamp}")
                
    except Exception as e:
        logger.error(f"[#process]é‡ç»„UUIDæ–‡ä»¶å¤±è´¥: {e}")
    
    logger.info("[#current_stats]âœ¨ UUIDæ–‡ä»¶é‡ç»„å®Œæˆ")

def update_json_records(uuid_directory=r'E:\1BACKUP\ehv\uuid'):
    """æ›´æ–°JSONè®°å½•æ–‡ä»¶ï¼Œç¡®ä¿æ‰€æœ‰è®°å½•éƒ½è¢«ä¿å­˜"""
    logger.info("[#current_stats]ğŸ”„ å¼€å§‹æ›´æ–°JSONè®°å½•...")
    
    json_record_path = os.path.join(uuid_directory, 'uuid_records.json')
    
    # åŠ è½½ç°æœ‰è®°å½•ï¼Œç¡®ä¿åŸºç¡€ç»“æ„æ­£ç¡®
    try:
        existing_records = JsonHandler.load(json_record_path)
        if not isinstance(existing_records, dict):
            existing_records = {"record": {}}
        if "record" not in existing_records:
            existing_records["record"] = {}
    except Exception as e:
        logger.error(f"[#process]åŠ è½½è®°å½•æ–‡ä»¶å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°è®°å½•: {e}")
        existing_records = {"record": {}}
    
    total_files = 0
    processed = 0
    
    # é¦–å…ˆè®¡ç®—æ€»æ–‡ä»¶æ•°
    for root, _, files in os.walk(uuid_directory):
        total_files += sum(1 for file in files if file.endswith('.json') and file != 'uuid_records.json')
    
    # éå†ç›®å½•ç»“æ„æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    for root, _, files in os.walk(uuid_directory):
        for file in files:
            if file.endswith('.json') and file != 'uuid_records.json':
                uuid = os.path.splitext(file)[0]
                json_path = os.path.join(root, file)
                try:
                    file_data = JsonHandler.load(json_path)
                    if not file_data:
                        logger.warning(f"[#process]è·³è¿‡ç©ºæ–‡ä»¶: {file}")
                        continue
                        
                    if uuid not in existing_records["record"]:
                        existing_records["record"][uuid] = file_data
                        logger.info(f"[#process]âœ… æ·»åŠ æ–°è®°å½•: {uuid}")
                    else:
                        # ç¡®ä¿timestampså­—æ®µå­˜åœ¨
                        if "timestamps" not in existing_records["record"][uuid]:
                            existing_records["record"][uuid]["timestamps"] = {}
                        if "timestamps" in file_data:
                            existing_records["record"][uuid]["timestamps"].update(file_data["timestamps"])
                            logger.info(f"[#process]âœ… æ›´æ–°è®°å½•: {uuid}")
                        
                except Exception as e:
                    logger.error(f"[#process]å¤„ç†JSONæ–‡ä»¶å¤±è´¥ {json_path}: {e}")
                
                processed += 1
                logger.info(f"[@current_progress]æ›´æ–°è¿›åº¦ {processed}/{total_files} ({(processed/total_files*100):.1f}%)")
    
    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¿è¯å†™å…¥å®‰å…¨æ€§
    temp_path = f"{json_record_path}.tmp"
    try:
        with open(temp_path, 'w', encoding='utf-8') as f:
            json.dump(existing_records, f, ensure_ascii=False, indent=2)
        
        # åŸå­æ€§æ›¿æ¢
        if os.path.exists(json_record_path):
            os.replace(temp_path, json_record_path)
        else:
            os.rename(temp_path, json_record_path)
        logger.info("[#current_stats]âœ… JSONè®°å½•æ›´æ–°å®Œæˆ")
        
    except Exception as e:
        logger.error(f"[#process]âŒ JSONè®°å½•æ›´æ–°å¤±è´¥: {e}")
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass

def convert_yaml_to_json_structure():
    """å°†ç°æœ‰çš„YAMLæ–‡ä»¶ç»“æ„è½¬æ¢ä¸ºJSONç»“æ„"""
    logger.info("[#current_stats]ğŸ”„ å¼€å§‹è½¬æ¢YAMLåˆ°JSONç»“æ„...")
    
    uuid_directory = r'E:\1BACKUP\ehv\uuid'
    yaml_record_path = os.path.join(uuid_directory, 'uuid_records.yaml')
    json_record_path = os.path.join(uuid_directory, 'uuid_records.json')
    
    # è½¬æ¢ä¸»è®°å½•æ–‡ä»¶
    if os.path.exists(yaml_record_path):
        try:
            with open(yaml_record_path, 'r', encoding='utf-8') as f:
                yaml_data = yaml.safe_load(f)
                
            total_records = len(yaml_data)
            processed = 0
            
            json_records = {}
            for record in yaml_data:
                uuid = record.get('UUID')
                if not uuid:
                    continue
                    
                if uuid not in json_records:
                    json_records[uuid] = {"timestamps": {}}
                    
                timestamp = record.get('LastModified') or record.get('CreatedAt')
                if timestamp:
                    json_records[uuid]["timestamps"][timestamp] = {
                        "archive_name": record.get('ArchiveName', ''),
                        "artist_name": record.get('ArtistName', ''),
                        "relative_path": record.get('LastPath', '')
                    }
                
                processed += 1
                logger.info(f"[@current_progress]è½¬æ¢è¿›åº¦ {processed}/{total_records} ({(processed/total_records*100):.1f}%)")
            
            JsonHandler.save(json_record_path, json_records)
            logger.info("[#current_stats]âœ… ä¸»è®°å½•æ–‡ä»¶è½¬æ¢å®Œæˆ")
            
        except Exception as e:
            logger.error(f"[#process]è½¬æ¢ä¸»è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
    
    # è½¬æ¢ç›®å½•ä¸­çš„YAMLæ–‡ä»¶
    yaml_files = []
    for root, _, files in os.walk(uuid_directory):
        yaml_files.extend([os.path.join(root, f) for f in files if f.endswith('.yaml') and f != 'uuid_records.yaml'])
    
    total_files = len(yaml_files)
    processed = 0
    
    for yaml_path in yaml_files:
        try:
            with open(yaml_path, 'r', encoding='utf-8') as f:
                yaml_data = yaml.safe_load(f)
                
            json_path = os.path.join(os.path.dirname(yaml_path), f"{os.path.splitext(os.path.basename(yaml_path))[0]}.json")
            
            json_data = JsonHandler.convert_yaml_to_json(yaml_data)
            json_data["uuid"] = os.path.splitext(os.path.basename(yaml_path))[0]
            
            if JsonHandler.save(json_path, json_data):
                os.remove(yaml_path)
                logger.info(f"[#process]âœ… è½¬æ¢å®Œæˆ: {os.path.basename(yaml_path)}")
            
            processed += 1
            logger.info(f"[@current_progress]æ–‡ä»¶è½¬æ¢è¿›åº¦ {processed}/{total_files} ({(processed/total_files*100):.1f}%)")
            
        except Exception as e:
            logger.error(f"[#process]è½¬æ¢æ–‡ä»¶å¤±è´¥ {os.path.basename(yaml_path)}: {e}")
    
    logger.info("[#current_stats]âœ¨ YAMLåˆ°JSONè½¬æ¢å®Œæˆ")

class CommandManager:
    """å‘½ä»¤è¡Œå‚æ•°ç®¡ç†å™¨"""
    
    @staticmethod
    def init_parser():
        parser = argparse.ArgumentParser(description='å¤„ç†æ–‡ä»¶UUIDå’ŒJSONç”Ÿæˆ')
        parser.add_argument('-c', '--clipboard', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
        parser.add_argument('-m', '--mode', choices=['multi', 'single'], help='å¤„ç†æ¨¡å¼ï¼šmulti(å¤šäººæ¨¡å¼)æˆ–single(å•äººæ¨¡å¼)')
        parser.add_argument('--no-artist', action='store_true', help='æ— ç”»å¸ˆæ¨¡å¼ - ä¸æ·»åŠ ç”»å¸ˆå')
        parser.add_argument('--keep-timestamp', action='store_true', help='ä¿æŒæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´')
        parser.add_argument('--path', help='è¦å¤„ç†çš„è·¯å¾„')
        parser.add_argument('-a', '--auto-sequence', action='store_true', help='è‡ªåŠ¨æ‰§è¡Œå®Œæ•´åºåˆ—ï¼šUUID-JSON -> è‡ªåŠ¨æ–‡ä»¶å -> UUID-JSON')
        parser.add_argument('-r', '--reorganize', action='store_true', help='é‡æ–°ç»„ç»‡ UUID æ–‡ä»¶ç»“æ„')
        parser.add_argument('-u', '--update-records', action='store_true', help='æ›´æ–° UUID è®°å½•æ–‡ä»¶')
        parser.add_argument('--convert', action='store_true', help='è½¬æ¢YAMLåˆ°JSONç»“æ„')
        parser.add_argument('--order', choices=['path', 'mtime'], default='mtime',
                          help='å¤„ç†é¡ºåº: path(æŒ‰è·¯å¾„å‡åº) æˆ– mtime(æŒ‰ä¿®æ”¹æ—¶é—´å€’åº)')
        return parser

    @staticmethod
    def get_target_directory(args):
        if args.clipboard:
            try:
                target_directory = pyperclip.paste().strip().strip('"')
                if not os.path.exists(target_directory):
                    logger.error(f"[#process]å‰ªè´´æ¿ä¸­çš„è·¯å¾„æ— æ•ˆ: {target_directory}")
                    sys.exit(1)
                logger.info(f"[#current_stats]å·²ä»å‰ªè´´æ¿è¯»å–è·¯å¾„: {target_directory}")
            except Exception as e:
                logger.error(f"[#process]ä»å‰ªè´´æ¿è¯»å–è·¯å¾„å¤±è´¥: {e}")
                sys.exit(1)
        else:
            target_directory = args.path or r"E:\1EHV"
            logger.info(f"[#current_stats]ä½¿ç”¨è·¯å¾„: {target_directory}")
        return target_directory

class TaskExecutor:
    """ä»»åŠ¡æ‰§è¡Œå™¨"""
    
    def __init__(self, args, target_directory: str):
        self.args = args
        self.target_directory = target_directory
        self.max_workers = min(32, (multiprocessing.cpu_count() * 4) + 1)
        self.confirmed_artists = set()
        self.uuid_directory = r'E:\1BACKUP\ehv\uuid'
        self.archive_processor = ArchiveProcessor(
            self.target_directory, 
            self.uuid_directory,
            self.max_workers,
            order=args.order  # æ·»åŠ æ’åºå‚æ•°
        )
        self.uuid_record_manager = UuidRecordManager(self.uuid_directory)

    def _confirm_artists(self) -> None:
        """ç¡®è®¤ç”»å¸ˆä¿¡æ¯"""
        print("\næ­£åœ¨æ‰«æç”»å¸ˆä¿¡æ¯...")
        artists = set()
        
        # æ‰«ææ‰€æœ‰å‹ç¼©æ–‡ä»¶ä»¥è·å–ç”»å¸ˆä¿¡æ¯
        for root, _, files in os.walk(self.target_directory):
            for file in files:
                if file.endswith(('.zip', '.rar', '.7z')):
                    archive_path = os.path.join(root, file)
                    artist = PathHandler.get_artist_name(self.target_directory, archive_path, self.args.mode)
                    if artist:
                        artists.add(artist)
        
        # æ˜¾ç¤ºç”»å¸ˆä¿¡æ¯å¹¶ç­‰å¾…ç¡®è®¤
        if self.args.mode == 'single':
            if len(artists) > 1:
                print("\nâš ï¸ è­¦å‘Šï¼šåœ¨å•äººæ¨¡å¼ä¸‹æ£€æµ‹åˆ°å¤šä¸ªç”»å¸ˆåç§°ï¼š")
                for i, artist in enumerate(sorted(artists), 1):
                    print(f"{i}. {artist}")
                print("\nè¯·ç¡®è®¤è¿™æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Ÿå¦‚æœä¸ç¬¦åˆï¼Œè¯·æ£€æŸ¥ç›®å½•ç»“æ„ã€‚")
            elif len(artists) == 1:
                print(f"\næ£€æµ‹åˆ°ç”»å¸ˆ: {next(iter(artists))}")
            else:
                print("\nâš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°ç”»å¸ˆåç§°ï¼")
            
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
            
        else:  # å¤šäººæ¨¡å¼
            print(f"\nå…±æ£€æµ‹åˆ° {len(artists)} ä¸ªç”»å¸ˆç›®å½•ï¼š")
            for i, artist in enumerate(sorted(artists), 1):
                print(f"{i}. {artist}")
            
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
        
        self.confirmed_artists = artists

    def execute_tasks(self) -> None:
        """æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡"""
        # é¦–å…ˆç¡®è®¤ç”»å¸ˆä¿¡æ¯
        self._confirm_artists()
        
        # ç„¶ååˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        init_TextualLogger()
        
        logger.info(f"[#current_stats]å½“å‰æ¨¡å¼: {'å¤šäººæ¨¡å¼' if self.args.mode == 'multi' else 'å•äººæ¨¡å¼'}")

        if self.args.convert:
            self._execute_convert_task()
            return

        if self.args.reorganize:
            self._execute_reorganize_task()

        if self.args.update_records:
            self._execute_update_records_task()

        if self.args.auto_sequence:
            self._execute_auto_sequence()
        elif not self.args.reorganize and not self.args.update_records:
            self._execute_normal_process()

    def _execute_convert_task(self) -> None:
        """æ‰§è¡ŒYAMLè½¬JSONä»»åŠ¡"""
        self.uuid_record_manager.convert_yaml_to_json_structure()
        sys.exit(0)

    def _execute_reorganize_task(self) -> None:
        """æ‰§è¡Œé‡ç»„ä»»åŠ¡"""
        logger.info("[#current_stats]ğŸ“ å¼€å§‹é‡æ–°ç»„ç»‡ UUID æ–‡ä»¶...")
        self.uuid_record_manager.reorganize_uuid_files()

    def _execute_update_records_task(self) -> None:
        """æ‰§è¡Œæ›´æ–°è®°å½•ä»»åŠ¡"""
        logger.info("[#current_stats]ğŸ“ å¼€å§‹æ›´æ–° UUID è®°å½•...")
        self.uuid_record_manager.update_json_records()

    def _execute_auto_sequence(self) -> None:
        """ä¼˜åŒ–åçš„è‡ªåŠ¨åºåˆ—æ‰§è¡Œ"""
        # ç›´æ¥å¼€å§‹å¤„ç†ï¼Œä¸è¿›è¡Œé¢„çƒ­
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹åˆå¹¶å¤„ç†æµç¨‹...")
        self.archive_processor.process_archives()
        self._run_auto_filename_script()
        
        logger.info("[#current_stats]âœ¨ ä¼˜åŒ–åçš„å¤„ç†æµç¨‹å®Œæˆï¼")

    def _execute_normal_process(self) -> None:
        """æ‰§è¡Œæ™®é€šå¤„ç†æµç¨‹"""
        self.archive_processor.process_archives()

    def _process_uuid_json(self) -> None:
        """å¤„ç†UUID-JSONç›¸å…³ä»»åŠ¡"""
        # ç§»é™¤é¢„çƒ­è°ƒç”¨
        skip_limit_reached = self.archive_processor.process_archives()
        if skip_limit_reached:
            logger.info("[#current_stats]â© ç”±äºè¿ç»­è·³è¿‡æ¬¡æ•°è¾¾åˆ°é™åˆ¶ï¼Œæå‰è¿›å…¥ä¸‹ä¸€é˜¶æ®µ")

    def _run_auto_filename_script(self) -> None:
        """è¿è¡Œè‡ªåŠ¨æ–‡ä»¶åè„šæœ¬"""
        auto_filename_script = os.path.join(os.path.dirname(__file__), '011-è‡ªåŠ¨å”¯ä¸€æ–‡ä»¶å.py')
        if not os.path.exists(auto_filename_script):
            logger.error(f"[#process]æ‰¾ä¸åˆ°è‡ªåŠ¨æ–‡ä»¶åè„šæœ¬: {auto_filename_script}")
            return

        try:
            cmd = [sys.executable, auto_filename_script]
            if self.args.clipboard:
                cmd.extend(['-c'])
            if self.args.mode:
                cmd.extend(['-m', self.args.mode])

            startupinfo = None
            if os.name == 'nt':
                startupinfo = subprocess.STARTUPINFO()
                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True,
                encoding='gbk',
                errors='ignore',
                startupinfo=startupinfo
            )

            for line in result.stdout.splitlines():
                if line.strip():
                    logger.info(line)

            logger.info("[#current_stats]âœ… è‡ªåŠ¨æ–‡ä»¶åå¤„ç†å®Œæˆ")
        except subprocess.CalledProcessError as e:
            logger.error(f"[#process]è‡ªåŠ¨æ–‡ä»¶åå¤„ç†å¤±è´¥: {str(e)}")
            if e.output:
                logger.error(f"[#process]é”™è¯¯è¾“å‡º: {e.output}")

    def _validate_json_records(self) -> None:
        """éªŒè¯JSONè®°å½•æ–‡ä»¶"""
        json_record_path = os.path.join(self.uuid_directory, 'uuid_records.json')
        if os.path.exists(json_record_path):
            try:
                with open(json_record_path, 'r', encoding='utf-8') as f:
                    json.load(f)
                logger.info("[#current_stats]âœ… JSONè®°å½•æ–‡ä»¶éªŒè¯é€šè¿‡")
            except json.JSONDecodeError as e:
                logger.error(f"[#process]âŒ JSONè®°å½•æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
                sys.exit(1)
        else:
            logger.warning("[#process]âš ï¸ JSONè®°å½•æ–‡ä»¶ä¸å­˜åœ¨")

class UuidRecordManager:
    """UUIDè®°å½•ç®¡ç†ç±»"""
    
    def __init__(self, uuid_directory: str = r'E:\1BACKUP\ehv\uuid'):
        self.uuid_directory = uuid_directory
    
    def reorganize_uuid_files(self) -> None:
        """æ ¹æ®æœ€åä¿®æ”¹æ—¶é—´é‡æ–°ç»„ç»‡UUIDæ–‡ä»¶çš„ç›®å½•ç»“æ„"""
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹é‡æ–°ç»„ç»‡UUIDæ–‡ä»¶...")
        
        json_record_path = os.path.join(self.uuid_directory, 'uuid_records.json')
        if not os.path.exists(json_record_path):
            logger.error("[#process]âŒ UUIDè®°å½•æ–‡ä»¶ä¸å­˜åœ¨")
            return
            
        try:
            with open(json_record_path, 'r', encoding='utf-8') as f:
                records = json.load(f)
                
            total_records = len(records)
            processed = 0
            
            for uuid, data in records.items():
                if not data.get("timestamps"):
                    continue
                    
                latest_timestamp = max(data["timestamps"].keys())
                
                try:
                    date = datetime.strptime(latest_timestamp, "%Y-%m-%d %H:%M:%S")
                    year = str(date.year)
                    month = f"{date.month:02d}"
                    day = f"{date.day:02d}"
                    
                    year_dir = os.path.join(self.uuid_directory, year)
                    month_dir = os.path.join(year_dir, month)
                    day_dir = os.path.join(month_dir, day)
                    target_path = os.path.join(day_dir, f"{uuid}.json")
                    
                    current_json_path = None
                    for root, _, files in os.walk(self.uuid_directory):
                        if f"{uuid}.json" in files:
                            current_json_path = os.path.join(root, f"{uuid}.json")
                            break
                    
                    if current_json_path and current_json_path != target_path:
                        os.makedirs(day_dir, exist_ok=True)
                        shutil.move(current_json_path, target_path)
                        logger.info(f"[#process]âœ… å·²ç§»åŠ¨: {uuid}.json")
                    
                    processed += 1
                    logger.info(f"[@current_progress]é‡ç»„è¿›åº¦ {processed}/{total_records} ({(processed/total_records*100):.1f}%)")
                        
                except ValueError as e:
                    logger.error(f"[#process]âŒ UUID {uuid} çš„æ—¶é—´æˆ³æ ¼å¼æ— æ•ˆ: {latest_timestamp}")
                    
        except Exception as e:
            logger.error(f"[#process]é‡ç»„UUIDæ–‡ä»¶å¤±è´¥: {e}")
        
        logger.info("[#current_stats]âœ¨ UUIDæ–‡ä»¶é‡ç»„å®Œæˆ")
    
    def update_json_records(self) -> None:
        """æ›´æ–°JSONè®°å½•æ–‡ä»¶ï¼Œç¡®ä¿æ‰€æœ‰è®°å½•éƒ½è¢«ä¿å­˜"""
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹æ›´æ–°JSONè®°å½•...")
        
        json_record_path = os.path.join(self.uuid_directory, 'uuid_records.json')
        
        # åŠ è½½ç°æœ‰è®°å½•ï¼Œç¡®ä¿åŸºç¡€ç»“æ„æ­£ç¡®
        try:
            existing_records = JsonHandler.load(json_record_path)
            if not isinstance(existing_records, dict):
                existing_records = {"record": {}}
            if "record" not in existing_records:
                existing_records["record"] = {}
        except Exception as e:
            logger.error(f"[#process]åŠ è½½è®°å½•æ–‡ä»¶å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°è®°å½•: {e}")
            existing_records = {"record": {}}
        
        total_files = 0
        processed = 0
        
        # é¦–å…ˆè®¡ç®—æ€»æ–‡ä»¶æ•°
        for root, _, files in os.walk(self.uuid_directory):
            total_files += sum(1 for file in files if file.endswith('.json') and file != 'uuid_records.json')
        
        # éå†ç›®å½•ç»“æ„æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        for root, _, files in os.walk(self.uuid_directory):
            for file in files:
                if file.endswith('.json') and file != 'uuid_records.json':
                    uuid = os.path.splitext(file)[0]
                    json_path = os.path.join(root, file)
                    try:
                        file_data = JsonHandler.load(json_path)
                        if not file_data:
                            logger.warning(f"[#process]è·³è¿‡ç©ºæ–‡ä»¶: {file}")
                            continue
                            
                        # æ— è®ºæ˜¯å¦éœ€è¦æ›´æ–°ï¼Œéƒ½ç¡®ä¿è®°å½•å­˜åœ¨äºç¼“å­˜ä¸­
                        if uuid not in existing_records["record"]:
                            # æ–°è®°å½•ï¼Œç›´æ¥æ·»åŠ 
                            existing_records["record"][uuid] = file_data
                            logger.info(f"[#process]âœ… æ·»åŠ æ–°è®°å½•: {uuid}")
                        else:
                            # å·²å­˜åœ¨çš„è®°å½•ï¼Œåˆå¹¶æ—¶é—´æˆ³
                            if "timestamps" not in existing_records["record"][uuid]:
                                existing_records["record"][uuid]["timestamps"] = {}
                            
                            if "timestamps" in file_data:
                                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ—¶é—´æˆ³éœ€è¦æ›´æ–°
                                has_new_timestamps = False
                                for timestamp, data in file_data["timestamps"].items():
                                    if timestamp not in existing_records["record"][uuid]["timestamps"]:
                                        has_new_timestamps = True
                                        existing_records["record"][uuid]["timestamps"][timestamp] = data
                                
                                if has_new_timestamps:
                                    logger.info(f"[#process]âœ… æ›´æ–°è®°å½•: {uuid}")
                                else:
                                    logger.info(f"[#process]âœ“ è®°å½•å·²å­˜åœ¨ä¸”æ— éœ€æ›´æ–°: {uuid}")
                            else:
                                logger.info(f"[#process]âœ“ è®°å½•å·²å­˜åœ¨ä¸”æ— éœ€æ›´æ–°: {uuid}")
                            
                    except Exception as e:
                        logger.error(f"[#process]å¤„ç†JSONæ–‡ä»¶å¤±è´¥ {json_path}: {e}")
                    
                    processed += 1
                    logger.info(f"[@current_progress]æ›´æ–°è¿›åº¦ {processed}/{total_files} ({(processed/total_files*100):.1f}%)")
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¿è¯å†™å…¥å®‰å…¨æ€§
        temp_path = f"{json_record_path}.tmp"
        try:
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(existing_records, f, ensure_ascii=False, indent=2)
            
            # åŸå­æ€§æ›¿æ¢
            if os.path.exists(json_record_path):
                os.replace(temp_path, json_record_path)
            else:
                os.rename(temp_path, json_record_path)
            logger.info("[#current_stats]âœ… JSONè®°å½•æ›´æ–°å®Œæˆ")
            
        except Exception as e:
            logger.error(f"[#process]âŒ JSONè®°å½•æ›´æ–°å¤±è´¥: {e}")
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
    
    def convert_yaml_to_json_structure(self) -> None:
        """å°†ç°æœ‰çš„YAMLæ–‡ä»¶ç»“æ„è½¬æ¢ä¸ºJSONç»“æ„"""
        logger.info("[#current_stats]ğŸ”„ å¼€å§‹è½¬æ¢YAMLåˆ°JSONç»“æ„...")
        
        yaml_record_path = os.path.join(self.uuid_directory, 'uuid_records.yaml')
        json_record_path = os.path.join(self.uuid_directory, 'uuid_records.json')
        
        # è½¬æ¢ä¸»è®°å½•æ–‡ä»¶
        if os.path.exists(yaml_record_path):
            try:
                with open(yaml_record_path, 'r', encoding='utf-8') as f:
                    yaml_data = yaml.safe_load(f)
                    
                total_records = len(yaml_data)
                processed = 0
                
                json_records = {}
                for record in yaml_data:
                    uuid = record.get('UUID')
                    if not uuid:
                        continue
                        
                    if uuid not in json_records:
                        json_records[uuid] = {"timestamps": {}}
                        
                    timestamp = record.get('LastModified') or record.get('CreatedAt')
                    if timestamp:
                        json_records[uuid]["timestamps"][timestamp] = {
                            "archive_name": record.get('ArchiveName', ''),
                            "artist_name": record.get('ArtistName', ''),
                            "relative_path": record.get('LastPath', '')
                        }
                    
                    processed += 1
                    logger.info(f"[@current_progress]è½¬æ¢è¿›åº¦ {processed}/{total_records} ({(processed/total_records*100):.1f}%)")
                
                JsonHandler.save(json_record_path, json_records)
                logger.info("[#current_stats]âœ… ä¸»è®°å½•æ–‡ä»¶è½¬æ¢å®Œæˆ")
                
            except Exception as e:
                logger.error(f"[#process]è½¬æ¢ä¸»è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
        
        # è½¬æ¢ç›®å½•ä¸­çš„YAMLæ–‡ä»¶
        yaml_files = []
        for root, _, files in os.walk(self.uuid_directory):
            yaml_files.extend([os.path.join(root, f) for f in files if f.endswith('.yaml') and f != 'uuid_records.yaml'])
        
        total_files = len(yaml_files)
        processed = 0
        
        for yaml_path in yaml_files:
            try:
                with open(yaml_path, 'r', encoding='utf-8') as f:
                    yaml_data = yaml.safe_load(f)
                    
                json_path = os.path.join(os.path.dirname(yaml_path), f"{os.path.splitext(os.path.basename(yaml_path))[0]}.json")
                
                json_data = JsonHandler.convert_yaml_to_json(yaml_data)
                json_data["uuid"] = os.path.splitext(os.path.basename(yaml_path))[0]
                
                if JsonHandler.save(json_path, json_data):
                    os.remove(yaml_path)
                    logger.info(f"[#process]âœ… è½¬æ¢å®Œæˆ: {os.path.basename(yaml_path)}")
                
                processed += 1
                logger.info(f"[@current_progress]æ–‡ä»¶è½¬æ¢è¿›åº¦ {processed}/{total_files} ({(processed/total_files*100):.1f}%)")
                
            except Exception as e:
                logger.error(f"[#process]è½¬æ¢æ–‡ä»¶å¤±è´¥ {os.path.basename(yaml_path)}: {e}")
        
        logger.info("[#current_stats]âœ¨ YAMLåˆ°JSONè½¬æ¢å®Œæˆ")

if __name__ == '__main__':
    # åˆå§‹åŒ–å‘½ä»¤è¡Œè§£æå™¨
    parser = CommandManager.init_parser()
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œå¯åŠ¨TUIç•Œé¢
    if len(sys.argv) == 1:
        main()
        sys.exit(0)

    # è·å–ç›®æ ‡ç›®å½•
    target_directory = CommandManager.get_target_directory(args)

    # æ‰§è¡Œä»»åŠ¡
    executor = TaskExecutor(args, target_directory)
    executor.execute_tasks()
    