import os
import sys
import json
import hashlib
import argparse
import subprocess
from pathlib import Path
from typing import Optional, List
import logging
from datetime import datetime
# æ·»åŠ TextualLoggerå¯¼å…¥

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from nodes.tui.textual_logger import TextualLoggerManager
from nodes.pics.hash_process_config import get_latest_hash_file_path, process_artist_folder, process_duplicates
from nodes.record.logger_config import setup_logger
from nodes.record.logger_config import setup_logger
from nodes.tui.mode_manager import create_mode_manager
from nodes.file_ops.backup_handler import BackupHandler
from nodes.file_ops.archive_handler import ArchiveHandler
from nodes.pics.image_filter import ImageFilter
from nodes.io.input_handler import InputHandler
from nodes.io.config_handler import ConfigHandler
from nodes.io.path_handler import PathHandler, ExtractMode
# åœ¨å…¨å±€é…ç½®éƒ¨åˆ†æ·»åŠ ä»¥ä¸‹å†…å®¹
# ================= æ—¥å¿—é…ç½® =================
config = {
    'script_name': 'recruit_remove',
}
logger, config_info = setup_logger(config)

# å‚æ•°é…ç½®
DEFAULT_PARAMS = {
    'ref_hamming_distance': 16,  # ä¸å¤–éƒ¨å‚è€ƒæ–‡ä»¶æ¯”è¾ƒçš„æ±‰æ˜è·ç¦»é˜ˆå€¼
    'hamming_distance': 0,  # å†…éƒ¨å»é‡çš„æ±‰æ˜è·ç¦»é˜ˆå€¼
    'self_redup': False,  # æ˜¯å¦å¯ç”¨è‡ªèº«å»é‡å¤
    'remove_duplicates': True,  # æ˜¯å¦å¯ç”¨é‡å¤å›¾ç‰‡è¿‡æ»¤
    'hash_size': 10,  # å“ˆå¸Œå€¼å¤§å°
    'filter_white_enabled': False,  # æ˜¯å¦å¯ç”¨ç™½å›¾è¿‡æ»¤
    'recruit_folder': r'E:\1EHV\[01æ‚]\zzzå»å›¾',  # ç”»å¸ˆæ–‡ä»¶å¤¹
    'exclude-paths': ['ç”»é›†', 'cg', 'åŠ¨ç”»', 'å›¾é›†'],  # å·²ç»å­˜åœ¨ï¼Œæ— éœ€ä¿®æ”¹
}

# TextualLoggerå¸ƒå±€é…ç½®
TEXTUAL_LAYOUT = {
    "current_stats": {
        "ratio": 2,
        "title": "ğŸ“Š æ€»ä½“è¿›åº¦",
        "style": "lightyellow"
    },
    "current_progress": {
        "ratio": 2,
        "title": "ğŸ”„ å½“å‰è¿›åº¦",
        "style": "lightcyan"
    },
    "process_log": {
        "ratio": 3,
        "title": "ğŸ“ å¤„ç†æ—¥å¿—",
        "style": "lightpink"
    },
    "update_log": {
        "ratio": 3,
        "title": "â„¹ï¸ æ›´æ–°æ—¥å¿—",
        "style": "lightblue"
    },
}

# å¸¸é‡è®¾ç½®
WORKER_COUNT = 2  # çº¿ç¨‹æ•°
FORCE_UPDATE = False  # æ˜¯å¦å¼ºåˆ¶æ›´æ–°å“ˆå¸Œå€¼

def init_TextualLogger():
    TextualLoggerManager.set_layout(TEXTUAL_LAYOUT, config_info['log_file'])

class RecruitRemoveFilter:
    """æ‹›å‹Ÿå›¾ç‰‡è¿‡æ»¤å™¨"""
    
    def __init__(self, hash_file: str = None, cover_count: int = 3, hamming_threshold: int = 12):
        """åˆå§‹åŒ–è¿‡æ»¤å™¨"""
        self.image_filter = ImageFilter(hash_file, cover_count, hamming_threshold)
        
    def _robust_cleanup(self, temp_dir: str) -> None:
        """æ›´å¥å£®çš„æ–‡ä»¶æ¸…ç†æ–¹æ³•ï¼Œå¤„ç†æ–‡ä»¶è¢«å ç”¨çš„æƒ…å†µ"""
        if not os.path.exists(temp_dir):
            return

        def on_rm_error(func, path, exc_info):
            try:
                os.chmod(path, stat.S_IWRITE)
                os.unlink(path)
            except Exception as e:
                logger.warning(f"[#process_log]æ— æ³•åˆ é™¤ {path}: {e}")

        try:
            # å°è¯•æ ‡å‡†åˆ é™¤
            shutil.rmtree(temp_dir, onerror=on_rm_error)
        except Exception as e:
            logger.warning(f"[#process_log]æ ‡å‡†åˆ é™¤å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åˆ é™¤: {temp_dir}")
            try:
                # ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤å¼ºåˆ¶åˆ é™¤ï¼ˆWindowsï¼‰
                if platform.system() == 'Windows':
                    subprocess.run(f'rmdir /s /q "{temp_dir}"', shell=True, check=True)
                else:  # Linux/MacOS
                    subprocess.run(f'rm -rf "{temp_dir}"', shell=True, check=True)
            except subprocess.CalledProcessError as e:
                logger.error(f"[#update_log]å¼ºåˆ¶åˆ é™¤å¤±è´¥: {temp_dir}")
                raise

    def process_archive(self, zip_path: str, extract_mode: str = ExtractMode.ALL, extract_params: dict = None) -> bool:
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…"""
        TextualLoggerManager.set_layout(TEXTUAL_LAYOUT, config_info['log_file'])
        
        # æ£€æŸ¥è¾“å…¥è·¯å¾„æ˜¯å¦ä¸ºç›®å½•
        if os.path.isdir(zip_path):
            # éå†ç›®å½•æŸ¥æ‰¾zipæ–‡ä»¶
            zip_files = []
            for root, _, files in os.walk(zip_path):
                for file in files:
                    if file.lower().endswith('.zip'):
                        # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
                        skip = False
                        for exclude_path in DEFAULT_PARAMS['exclude-paths']:
                            if exclude_path.lower() in root.lower() or exclude_path.lower() in file.lower():
                                logger.info(f"[#process_log]è·³è¿‡é»‘åå•è·¯å¾„: {os.path.join(root, file)}")
                                skip = True
                                break
                        if not skip:
                            zip_files.append(os.path.join(root, file))
            
            if not zip_files:
                logger.info(f"[#process_log]åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ°å¯å¤„ç†çš„å‹ç¼©åŒ…: {zip_path}")
                return False
                
            # å¤„ç†æ‰¾åˆ°çš„æ¯ä¸ªå‹ç¼©åŒ…
            success_count = 0
            for zip_file in zip_files:
                if self.process_archive(zip_file, extract_mode, extract_params):
                    success_count += 1
            return success_count > 0
        
        # å¦‚æœæ˜¯å•ä¸ªæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºzipæ–‡ä»¶
        if not zip_path.lower().endswith('.zip'):
            logger.info(f"[#process_log]ä¸æ˜¯æœ‰æ•ˆçš„å‹ç¼©åŒ…æ–‡ä»¶: {zip_path}")
            return False
            
        # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
        for exclude_path in DEFAULT_PARAMS['exclude-paths']:
            if exclude_path.lower() in zip_path.lower():
                logger.info(f"[#process_log]è·³è¿‡é»‘åå•è·¯å¾„: {zip_path}")
                return False
        
        logger.info(f"[#process_log]å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {zip_path}")
        
        # åˆ—å‡ºå‹ç¼©åŒ…å†…å®¹
        files = ArchiveHandler.list_archive_contents(zip_path)
        if not files:
            logger.info("[#process_log]æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return False
            
        # è·å–è¦è§£å‹çš„æ–‡ä»¶ç´¢å¼•
        extract_params = extract_params or {}
        selected_indices = ExtractMode.get_selected_indices(extract_mode, len(files), extract_params)
        if not selected_indices:
            logger.error("[#process_log]æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶è¿›è¡Œè§£å‹")
            return False
            
        # è§£å‹é€‰å®šæ–‡ä»¶
        selected_files = [files[i] for i in selected_indices]
        success, temp_dir = ArchiveHandler.extract_files(zip_path, selected_files)
        if not success:
            return False
            
        try:
            # è·å–è§£å‹åçš„å›¾ç‰‡æ–‡ä»¶
            image_files = []
            for root, _, files in os.walk(temp_dir):
                for file in files:
                    if PathHandler.get_file_extension(file) in {'.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.avif', '.heic', '.heif', '.jxl'}:
                        image_files.append(PathHandler.join_paths(root, file))
                        
            # å¤„ç†å›¾ç‰‡ - å¯ç”¨é‡å¤å›¾ç‰‡è¿‡æ»¤
            to_delete, removal_reasons = self.image_filter.process_images(
                image_files,
                enable_duplicate_filter=True,   # å¯ç”¨é‡å¤å›¾ç‰‡è¿‡æ»¤
                duplicate_filter_mode='quality'  # ä½¿ç”¨è´¨é‡è¿‡æ»¤æ¨¡å¼
            )
            
            if not to_delete:
                logger.info("[#process_log]æ²¡æœ‰éœ€è¦åˆ é™¤çš„å›¾ç‰‡")
                self._robust_cleanup(temp_dir)
                return False
                
            # å¤‡ä»½è¦åˆ é™¤çš„æ–‡ä»¶
            backup_results = BackupHandler.backup_removed_files(zip_path, to_delete, removal_reasons)
            
            # ä»å‹ç¼©åŒ…ä¸­åˆ é™¤æ–‡ä»¶
            files_to_delete = []
            for file_path in to_delete:
                # è·å–æ–‡ä»¶åœ¨å‹ç¼©åŒ…ä¸­çš„ç›¸å¯¹è·¯å¾„
                rel_path = os.path.relpath(file_path, temp_dir)
                files_to_delete.append(rel_path)
                
            # ä½¿ç”¨7zåˆ é™¤æ–‡ä»¶
            delete_list_file = os.path.join(temp_dir, '@delete.txt')
            with open(delete_list_file, 'w', encoding='utf-8') as f:
                for file_path in files_to_delete:
                    f.write(file_path + '\n')
                    
            # åœ¨æ‰§è¡Œåˆ é™¤æ“ä½œå‰å¤‡ä»½åŸå§‹å‹ç¼©åŒ…
            backup_success, backup_path = BackupHandler.backup_source_file(zip_path)
            if backup_success:
                logger.info(f"[#process_log]âœ… æºæ–‡ä»¶å¤‡ä»½æˆåŠŸ: {backup_path}")
            else:
                logger.warning(f"[#process_log]âš ï¸ æºæ–‡ä»¶å¤‡ä»½å¤±è´¥: {backup_path}")

            # ä½¿ç”¨7zåˆ é™¤æ–‡ä»¶
            cmd = ['7z', 'd', zip_path, f'@{delete_list_file}']
            result = subprocess.run(cmd, capture_output=True, text=True)
            os.remove(delete_list_file)
            
            if result.returncode != 0:
                logger.error(f"[#process_log]ä»å‹ç¼©åŒ…åˆ é™¤æ–‡ä»¶å¤±è´¥: {result.stderr}")
                self._robust_cleanup(temp_dir)
                return False
                
            logger.info(f"[#process_log]æˆåŠŸå¤„ç†å‹ç¼©åŒ…: {zip_path}")
            logger.info("[#current_progress]æ­£åœ¨åˆ†æå›¾ç‰‡ç›¸ä¼¼åº¦...")
            self._robust_cleanup(temp_dir)
            return True
            
        except Exception as e:
            logger.error(f"[#process_log]å¤„ç†å‹ç¼©åŒ…å¤±è´¥ {zip_path}: {e}")
            self._robust_cleanup(temp_dir)
            return False

def process_single_path(path: Path, workers: int = 4, force_update: bool = False, params: dict = None) -> bool:
    """å¤„ç†å•ä¸ªè·¯å¾„
    
    Args:
        path: è¾“å…¥è·¯å¾„
        workers: çº¿ç¨‹æ•°
        force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
        params: å‚æ•°å­—å…¸ï¼ŒåŒ…å«å¤„ç†å‚æ•°
        
    Returns:
        bool: æ˜¯å¦å¤„ç†æˆåŠŸ
    """
    try:
        logging.info(f"[#process_log]\nğŸ”„ å¤„ç†è·¯å¾„: {path}")
        
        recruit_folder=Path(params['recruit_folder']).resolve()
        # å¤„ç†ç”»å¸ˆæ–‡ä»¶å¤¹ï¼Œç”Ÿæˆå“ˆå¸Œæ–‡ä»¶
        hash_file = process_artist_folder(recruit_folder, workers, force_update)
        if not hash_file:
            return False
            
        logging.info(f"[#update_log]âœ… ç”Ÿæˆå“ˆå¸Œæ–‡ä»¶: {hash_file}")
        
        # å¤„ç†é‡å¤æ–‡ä»¶
        logging.info(f"[#process_log]\nğŸ”„ å¤„ç†é‡å¤æ–‡ä»¶ {path}")
        
        # åˆ›å»ºè¿‡æ»¤å™¨å®ä¾‹å¹¶å¤„ç†æ–‡ä»¶
        filter_instance = RecruitRemoveFilter(
            hash_file=hash_file,
            cover_count=3,  # é»˜è®¤å¤„ç†å‰3å¼ 
            hamming_threshold=params.get('ref_hamming_distance', 16)
        )
        
        # è®¾ç½®è§£å‹å‚æ•°ï¼Œé»˜è®¤å¤„ç†å‰3å¼ å’Œå5å¼ 
        extract_params = {
            'first_n': 3,  # å‰3å¼ 
            'last_n': 5   # å5å¼ 
        }
        
        # å¤„ç†æ–‡ä»¶
        success = filter_instance.process_archive(
            str(path),
            extract_mode=ExtractMode.RANGE,
            extract_params=extract_params
        )
        
        if success:
            logging.info(f"[#update_log]âœ… å¤„ç†å®Œæˆ: {path}")
            return True
        return False
        
    except Exception as e:
        logging.info(f"[#process_log]âŒ å¤„ç†è·¯å¾„æ—¶å‡ºé”™: {path}: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    # è·å–è·¯å¾„åˆ—è¡¨
    print("è¯·è¾“å…¥è¦å¤„ç†çš„è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
    paths = []
    while True:
        path = input().strip().replace('"', '')
        if not path:
            break
        paths.append(Path(path))
    if not paths:
        print("[#process_log]âŒ æœªè¾“å…¥ä»»ä½•è·¯å¾„")
        return
        
    print("[#process_log]\nğŸš€ å¼€å§‹å¤„ç†...")
    
    # å‡†å¤‡å‚æ•°
    params = DEFAULT_PARAMS.copy()
    recruit_folder = Path(params['recruit_folder']).resolve()
    
    # å¤„ç†ç”»å¸ˆæ–‡ä»¶å¤¹ï¼Œç”Ÿæˆå“ˆå¸Œæ–‡ä»¶
    hash_file = process_artist_folder(recruit_folder, WORKER_COUNT, FORCE_UPDATE)
    if not hash_file:
        logging.info("[#process_log]âŒ æ— æ³•ç”Ÿæˆå“ˆå¸Œæ–‡ä»¶")
        return
    
    success_count = 0
    total_count = len(paths)
    
    for i, path in enumerate(paths, 1):
        logging.info(f"[#process_log]\n=== å¤„ç†ç¬¬ {i}/{total_count} ä¸ªè·¯å¾„ ===")
        logging.info(f"[#process_log]è·¯å¾„: {path}")
        
        # æ›´æ–°è¿›åº¦
        progress = int((i - 1) / total_count * 100)
        logging.debug(f"[#current_progress]å½“å‰è¿›åº¦: [{('=' * int(progress/5))}] {progress}%")
        logging.info(f"[#current_stats]æ€»è·¯å¾„æ•°: {total_count} å·²å¤„ç†: {i-1} æˆåŠŸ: {success_count} æ€»è¿›åº¦: [{('=' * int(progress/5))}] {progress}%")
        
        # å¤„ç†é‡å¤æ–‡ä»¶
        try:
            # åˆ›å»ºè¿‡æ»¤å™¨å®ä¾‹å¹¶å¤„ç†æ–‡ä»¶
            filter_instance = RecruitRemoveFilter(
                hash_file=hash_file,
                cover_count=3,  # é»˜è®¤å¤„ç†å‰3å¼ 
                hamming_threshold=params.get('ref_hamming_distance', 16)
            )
            
            # è®¾ç½®è§£å‹å‚æ•°ï¼Œé»˜è®¤å¤„ç†å‰3å¼ å’Œå5å¼ 
            extract_params = {
                'first_n': 3,  # å‰3å¼ 
                'last_n': 5   # å5å¼ 
            }
            
            # å¤„ç†æ–‡ä»¶
            success = filter_instance.process_archive(
                str(path),
                extract_mode=ExtractMode.RANGE,
                extract_params=extract_params
            )
            
            if success:
                success_count += 1
        except Exception as e:
            logging.info(f"[#process_log]âŒ å¤„ç†å¤±è´¥: {path}: {e}")
        
        # æ›´æ–°æœ€ç»ˆè¿›åº¦
        progress = int(i / total_count * 100)
        logging.debug(f"[#current_progress]å½“å‰è¿›åº¦: [{('=' * int(progress/5))}] {progress}%")
        logging.info(f"[#current_stats]æ€»è·¯å¾„æ•°: {total_count}\nå·²å¤„ç†: {i}\næˆåŠŸ: {success_count}\næ€»è¿›åº¦: [{('=' * int(progress/5))}] {progress}%")
            
    logging.info(f"[#update_log]\nâœ… æ‰€æœ‰å¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{total_count}")

if __name__ == "__main__":
    main()