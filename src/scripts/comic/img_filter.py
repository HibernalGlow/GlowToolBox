from nodes.config.import_bundles import *

# å¯¼å…¥æ—¥å¿—é…ç½®
from nodes.logs.logger_config import setup_logger

config = {
    'script_name': 'comic_img_filter',
    'console_enabled': False
}
logger = setup_logger(config)
# åˆå§‹åŒ– TextualLoggerManager
HAS_TUI = True
USE_DEBUGGER = False

TEXTUAL_LAYOUT = {
    "cur_stats": {
        "ratio": 1,
        "title": "ğŸ“Š æ€»ä½“è¿›åº¦",
        "style": "lightyellow"
    },
    "cur_progress": {
        "ratio": 1,
        "title": "ğŸ”„ å½“å‰è¿›åº¦",
        "style": "lightcyan"
    },
    "file_ops": {  # æ–°å¢æ–‡ä»¶æ“ä½œé¢æ¿
        "ratio": 2,
        "title": "ğŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightpink"
    },
    "hash_calc": {  # æ–°å¢å“ˆå¸Œæ“ä½œé¢æ¿
        "ratio": 3,
        "title": "ğŸ”¢ å“ˆå¸Œè®¡ç®—",
        "style": "lightblue"
    },
    "update_log": {
        "ratio": 1,
        "title": "ğŸ”§ ç³»ç»Ÿæ¶ˆæ¯",
        "style": "lightwhite"
    }
}


# å…¨å±€é…ç½®
GLOBAL_HASH_CACHE = os.path.expanduser(r"E:\1EHV\image_hashes_global.json")
HASH_COLLECTION_FILE = os.path.expanduser(r"E:\1EHV\image_hashes_collection.json")  # ä¿®æ”¹ä¸ºcollection
HASH_FILES_LIST = os.path.expanduser(r"E:\1EHV\hash_files_list.txt")

# TUIå¸ƒå±€é…ç½®
def initialize_textual_logger():
    """åˆå§‹åŒ–æ—¥å¿—å¸ƒå±€ï¼Œç¡®ä¿åœ¨æ‰€æœ‰æ¨¡å¼ä¸‹éƒ½èƒ½æ­£ç¡®åˆå§‹åŒ–"""
    try:
        TextualLoggerManager.set_layout(TEXTUAL_LAYOUT)
        logger.info("[#update_log]âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")  # æ·»åŠ é¢æ¿æ ‡è¯†
    except Exception as e:
        print(f"âŒ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

class PathManager:
    """
    ç±»æè¿°
    """
    @staticmethod
    def create_temp_directory(file_path):
        """
        ä¸ºæ¯ä¸ªå‹ç¼©åŒ…åˆ›å»ºå”¯ä¸€çš„ä¸´æ—¶ç›®å½•ï¼Œä½¿ç”¨å‹ç¼©åŒ…åŸå+æ—¶é—´æˆ³
        
        Args:
            file_path: æºæ–‡ä»¶è·¯å¾„ï¼ˆå‹ç¼©åŒ…è·¯å¾„ï¼‰
        """
        original_dir = os.path.dirname(file_path)
        file_name = os.path.splitext(os.path.basename(file_path))[0]
        timestamp = time.strftime("%Y%m%d_%H%M%S", time.localtime())
        temp_dir = os.path.join(original_dir, f'{file_name}_{timestamp}')
        os.makedirs(temp_dir, exist_ok=True)
        logger.info( f'[#file_ops]ä¸´æ—¶ç›®å½•: {temp_dir}')
        return temp_dir

    @staticmethod
    def cleanup_temp_files(temp_dir, new_zip_path, backup_file_path):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•ï¼Œä½†ä¸å¤„ç†å¤‡ä»½æ–‡ä»¶"""
        try:
            if temp_dir and os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
                logger.info( f'[#file_ops]å·²åˆ é™¤ä¸´æ—¶ç›®å½•: {temp_dir}')
            if new_zip_path and os.path.exists(new_zip_path):
                os.remove(new_zip_path)
                logger.info( f'[#file_ops]å·²åˆ é™¤ä¸´æ—¶å‹ç¼©åŒ…: {new_zip_path}')
            # ä¸å¤„ç†å¤‡ä»½æ–‡ä»¶ï¼Œè®©BackupHandler.handle_bak_fileæ¥å¤„ç†
        except Exception as e:
            logger.info( f'[#file_ops]âŒ æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}')

class ImageProcessor:
    """
    ç±»æè¿°
    """
    def __init__(self):
        """åˆå§‹åŒ–å›¾ç‰‡å¤„ç†å™¨"""
        self.grayscale_detector = GrayscaleDetector()
        self.global_hashes = {}  # åˆå§‹åŒ–ä¸ºç©ºå­—å…¸
        self.temp_hashes = {}  # ä¸´æ—¶å­˜å‚¨å½“å‰å‹ç¼©åŒ…çš„å“ˆå¸Œå€¼

    def set_global_hashes(self, hashes):
        """è®¾ç½®å…¨å±€å“ˆå¸Œç¼“å­˜"""
        self.global_hashes = hashes

    @staticmethod
    def calculate_phash(image_path_or_data):
        """ä½¿ç”¨æ„ŸçŸ¥å“ˆå¸Œç®—æ³•è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼
        
        Args:
            image_path_or_data: å¯ä»¥æ˜¯å›¾ç‰‡è·¯å¾„(str/Path)æˆ–BytesIOå¯¹è±¡
            
        Returns:
            str: 16è¿›åˆ¶æ ¼å¼çš„æ„ŸçŸ¥å“ˆå¸Œå€¼ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            hash_value = ImageHashCalculator.calculate_phash(image_path_or_data)
            if isinstance(image_path_or_data, (str, Path)):
                logger.info( f"[#hash_calc]è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼: {os.path.basename(str(image_path_or_data))} -> {hash_value}")
            return hash_value
        except Exception as e:
            logger.info(f"[#hash_calc]è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼å¤±è´¥: {e}")
            return None

    def process_images_in_directory(self, temp_dir, params):
        """å¤„ç†ç›®å½•ä¸­çš„å›¾ç‰‡"""
        try:
            # æ¸…ç©ºä¸´æ—¶å“ˆå¸Œå­˜å‚¨
            self.temp_hashes.clear()
            
            image_files = ArchiveExtractor.get_image_files(temp_dir)
            if not image_files:
                logger.info( f"âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
                return (set(), set())
            removed_files = set()
            duplicate_files = set()
            lock = threading.Lock()
            existing_file_names = set()
            with ThreadPoolExecutor(max_workers=params['max_workers']) as executor:
                futures = []
                for file_path in image_files:
                    rel_path = os.path.relpath(file_path, os.path.dirname(file_path))
                    future = executor.submit(self.process_single_image, file_path, rel_path, existing_file_names, params, lock)
                    futures.append((future, file_path))
                image_hashes = []
                for future, file_path in futures:
                    try:
                        img_hash, img_data, rel_path, reason = future.result()
                        if reason in ['small_image', 'white_image']:
                            removed_files.add(file_path)
                        elif img_hash is not None:
                            image_hashes.append((img_hash, img_data, file_path, reason))
                    except Exception as e:
                        logger.info( f"âŒ å¤„ç†å›¾ç‰‡å¤±è´¥ {file_path}: {e}")
            if params['remove_duplicates'] and image_hashes:
                unique_images, _, removal_reasons = DuplicateDetector.remove_duplicates_in_memory(image_hashes, params)
                processed_files = {img[2] for img in unique_images}
                for img_hash, _, file_path, _ in image_hashes:
                    if file_path not in processed_files:
                        duplicate_files.add(file_path)
                        
                # # å¤„ç†å®Œæˆåï¼Œå°†ä¸´æ—¶å“ˆå¸Œæ›´æ–°åˆ°å…¨å±€å“ˆå¸Œ
                # if self.temp_hashes:
                #     with lock:
                #         self.global_hashes.update(self.temp_hashes)
                #         logger.info(f"[#hash_calc]å·²æ‰¹é‡æ·»åŠ  {len(self.temp_hashes)} ä¸ªå“ˆå¸Œåˆ°å…¨å±€ç¼“å­˜")
                #         # æ¸…ç©ºä¸´æ—¶å­˜å‚¨
                #         self.temp_hashes.clear()
                        
            return (removed_files, duplicate_files)
        except Exception as e:
            logger.info( f"âŒ å¤„ç†ç›®å½•ä¸­çš„å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return (set(), set())

    def process_single_image(self, file_path, rel_path, existing_file_names, params, lock): 
        """å¤„ç†å•ä¸ªå›¾ç‰‡æ–‡ä»¶"""
        try:
            if not file_path or not os.path.exists(file_path):
                logger.info(f"[#file_ops]âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return (None, None, None, 'file_not_found')
                            
            try:
                with open(file_path, 'rb') as f:
                    file_data = f.read()
                    logger.info(f"[#file_ops]ğŸ“·è¯»å›¾: {file_path}")  # æ·»åŠ é¢æ¿æ ‡è¯†
            except (IOError, OSError) as e:
                logger.info(f"[#file_ops]âŒ å›¾ç‰‡æ–‡ä»¶æŸåæˆ–æ— æ³•è¯»å– {rel_path}: {e}")
                return (None, None, None, 'corrupted_image')
            except Exception as e:
                logger.info(f"[#file_ops]âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {rel_path}: {e}")
                return (None, None, None, 'read_error')
                
            if file_path.lower().endswith(('png', 'webp', 'jxl', 'avif', 'jpg', 'jpeg', 'bmp', 'tiff', 'tif', 'heic', 'heif', 'bmp')):
                # === ç‹¬ç«‹å¤„ç†æ­¥éª¤ ===
                processed_data = file_data
                removal_reason = None

                # æ­¥éª¤1: å°å›¾æ£€æµ‹ (ç‹¬ç«‹åˆ¤æ–­)
                if params.get('filter_height_enabled', False):
                    logger.info(f"[#image_processing]ğŸ–¼ï¸ æ­£åœ¨æ£€æµ‹å°å›¾: {os.path.basename(file_path)}")
                    processed_data, removal_reason = self.detect_small_image(processed_data, params)
                    if removal_reason:
                        return (None, file_data, file_path, removal_reason)

                # æ­¥éª¤2: ç™½å›¾æ£€æµ‹ (ç‹¬ç«‹åˆ¤æ–­)
                if params.get('remove_grayscale', False):
                    logger.info(f"[#image_processing]ğŸ¨ æ­£åœ¨æ£€æµ‹ç™½å›¾: {os.path.basename(file_path)}")
                    processed_data, removal_reason = self.detect_grayscale_image(processed_data)
                    if removal_reason:
                        return (None, file_data, file_path, removal_reason)

                # æ­¥éª¤3: é‡å¤æ£€æµ‹ - ç›´æ¥è®¡ç®—å“ˆå¸Œ,ä¸æ£€æŸ¥å…¨å±€åŒ¹é…
                if params.get('remove_duplicates', False):
                    img_hash = self.handle_duplicate_detection(file_path, rel_path, params, lock, processed_data)
                    if not img_hash:
                        return (None, file_data, file_path, 'hash_error')
                    return (img_hash, file_data, file_path, None)

                return (None, file_data, file_path, None)
            else:
                return (None, file_data, file_path, 'non_image_file')
        except Exception as e:
            logger.info(f"[#file_ops]âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {rel_path}: {e}")
            return (None, None, None, 'processing_error')

       # ä¿®æ”¹åçš„ç‹¬ç«‹å°å›¾æ£€æµ‹æ–¹æ³•
    def detect_small_image(self, image_data, params):
        """ç‹¬ç«‹çš„å°å›¾æ£€æµ‹"""
        try:
            # å¦‚æœæ˜¯PILå›¾åƒå¯¹è±¡ï¼Œå…ˆè½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
            if isinstance(image_data, Image.Image):
                img_byte_arr = BytesIO()
                image_data.save(img_byte_arr, format=image_data.format)
                img_data = img_byte_arr.getvalue()
            else:
                img_data = image_data
                
            img = Image.open(BytesIO(img_data))
            width, height = img.size
            if height < params.get('min_size', 631):
                logger.info(f"[#image_processing]ğŸ–¼ï¸ å›¾ç‰‡å°ºå¯¸: {width}x{height} å°äºæœ€å°å°ºå¯¸")
                return None, 'small_image'
            logger.info(f"[#image_processing]ğŸ–¼ï¸ å›¾ç‰‡å°ºå¯¸: {width}x{height} å¤§äºæœ€å°å°ºå¯¸")
            return img_data, None
        except Exception as e:
            return None, 'size_detection_error'

    def detect_grayscale_image(self, image_data):
        """ç‹¬ç«‹çš„ç™½å›¾æ£€æµ‹"""
        white_keywords = ['pure_white', 'white', 'pure_black', 'grayscale']
        try:
            result = self.grayscale_detector.analyze_image(image_data)
            if result is None:
                logger.info( f"ç°åº¦åˆ†æè¿”å›None")
                return (None, 'grayscale_detection_error')
                
            # è¯¦ç»†è®°å½•åˆ†æç»“æœ
            # logger.info( f"ç°åº¦åˆ†æç»“æœ: {result}")
            
            if hasattr(result, 'removal_reason') and result.removal_reason:
                logger.info( f"æ£€æµ‹åˆ°ç§»é™¤åŸå› : {result.removal_reason}")
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å…³é”®å­—
                matched_keywords = [keyword for keyword in white_keywords 
                                  if keyword in result.removal_reason]
                if matched_keywords:
                    logger.info( f"åŒ¹é…åˆ°ç™½å›¾å…³é”®å­—: {matched_keywords}")
                    return (None, 'white_image')
                    
                # å¦‚æœæœ‰removal_reasonä½†ä¸åŒ¹é…å…³é”®å­—ï¼Œè®°å½•è¿™ç§æƒ…å†µ
                logger.info( f"æœªåŒ¹é…å…³é”®å­—çš„ç§»é™¤åŸå› : {result.removal_reason}")
                return (None, result.removal_reason)
                
            return (image_data, None)
            
        except ValueError as ve:
            logger.info( f"ç°åº¦æ£€æµ‹å‘ç”ŸValueError: {str(ve)}")
            return (None, 'grayscale_detection_error')
        except Exception as e:
            logger.info( f"ç°åº¦æ£€æµ‹å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None, 'grayscale_detection_error'

    def handle_duplicate_detection(self, file_path, rel_path, params, lock, image_data):
        """å¤„ç†é‡å¤æ£€æµ‹ - åªè®¡ç®—å“ˆå¸Œ"""
        try:
            # è®¡ç®—æ–°çš„å“ˆå¸Œå€¼
            img_hash = ImageHashCalculator.calculate_phash(image_data)
            if img_hash:
                # è·å–å‹ç¼©åŒ…è·¯å¾„å¹¶æ„å»ºURI
                zip_path = params.get('zip_path')
                if zip_path:
                    img_uri = PathURIGenerator.generate(f"{zip_path}!{rel_path}")
                    # æ·»åŠ å“ˆå¸Œæ“ä½œé¢æ¿æ ‡è¯†
                    logger.info(f"[#hash_calc]è®¡ç®—å“ˆå¸Œå€¼: {img_uri} -> {img_hash['hash']}")  
            return img_hash
            
        except Exception as e:
            # é”™è¯¯æ—¥å¿—ä¹ŸæŒ‡å‘å“ˆå¸Œæ“ä½œé¢æ¿
            logger.info(f"[#hash_calc]âŒ è®¡ç®—å“ˆå¸Œå€¼å¤±è´¥: {str(e)}")  
            return None

class DuplicateDetector:
    """
    ç±»æè¿°
    """
    @staticmethod
    def _compare_with_reference_hashes(image_hashes, ref_hashes, hash_to_uri, params):
        """ä¸å‚è€ƒå“ˆå¸Œè¿›è¡Œæ¯”è¾ƒçš„å…¬å…±é€»è¾‘"""
        remaining_images = []
        hash_duplicates = 0
        removal_reasons = {}

        for i, (hash1, img_data1, file_path1, reason) in enumerate(image_hashes):
            if hash1 is None:
                continue

            # ä¸å‚è€ƒå“ˆå¸Œå€¼æ¯”è¾ƒ
            found, similar_hash, similar_uri = HashFileHandler.find_similar_hash(
                hash1, ref_hashes, hash_to_uri, params['ref_hamming_distance']
            )

            if found:
                hash_duplicates += 1
                StatisticsManager.update_counts(hash_duplicates=1)
                removal_reasons[file_path1] = 'hash_duplicate'

                # è®°å½•ç›¸ä¼¼æ€§
                hamming_distance = ImageHashCalculator.calculate_hamming_distance(hash1, similar_hash)
                # æ·»åŠ å“ˆå¸Œæ“ä½œé¢æ¿æ ‡è¯†
                logger.info(f"[#hash_calc]æ±‰æ˜è·ç¦»: {hamming_distance}<{params['ref_hamming_distance']}")  
                HashFileHandler.record_similarity(file_path1, similar_uri, hamming_distance)
                # ä½¿ç”¨æ–°çš„æ—¥å¿—æ ¼å¼
                logger.info(f"[#hash_calc]å¤„ç†æ–‡ä»¶: {os.path.basename(file_path1)}")
                logger.info(f"[#hash_calc]å‘ç°å“ˆå¸Œé‡å¤ï¼Œå°†åˆ é™¤: {os.path.basename(file_path1)}")  # ä¿®æ”¹é¢æ¿æ ‡è¯†
            else:
                remaining_images.append((hash1, img_data1, file_path1, reason))

        return remaining_images, hash_duplicates, removal_reasons

    @staticmethod
    def _process_internal_duplicates(remaining_images, hamming_threshold, removal_reasons):  # æ·»åŠ removal_reasonså‚æ•°
        """å¤„ç†å†…éƒ¨é‡å¤çš„å…¬å…±é€»è¾‘"""
        final_images = []
        processed_indices = set()
        normal_duplicates = 0
        internal_removal_reasons = {}  # æ–°å¢å†…éƒ¨removal_reasons

        # æ„å»ºå†…éƒ¨å“ˆå¸Œé›†åˆ
        internal_hashes = []
        hash_to_image = {}
        for i, (hash1, img_data1, file_path1, reason) in enumerate(remaining_images):
            # ç»Ÿä¸€å“ˆå¸Œæ ¼å¼ä¸ºå­—ç¬¦ä¸²
            hash_str = hash1['hash'] if isinstance(hash1, dict) else hash1
            internal_hashes.append(hash_str)
            hash_to_image[hash_str] = (i, img_data1, file_path1, reason)

        # å¯¹å“ˆå¸Œå€¼è¿›è¡Œæ’åº
        internal_hashes.sort()

        # å¯¹æ¯ä¸ªå›¾ç‰‡è¿›è¡Œæ¯”è¾ƒ
        for i, (hash1, img_data1, file_path1, reason) in enumerate(remaining_images):
            if i in processed_indices:
                continue
            
            # ç»Ÿä¸€å“ˆå¸Œæ ¼å¼ä¸ºå­—ç¬¦ä¸² ç¬¬ä¸€ä¸ªå“ˆå¸Œå€¼æ˜¯å­—å…¸æ ¼å¼ï¼ˆå› ä¸ºæ˜¯æ–°è®¡ç®—çš„ï¼‰ï¼Œè€Œå…¶ä»–çš„æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå› ä¸ºæ˜¯ä»ç¼“å­˜åŠ è½½çš„ï¼‰ã€‚
            hash_str = hash1['hash'] if isinstance(hash1, dict) else hash1
            logger.info(f"[#cur_progress]åˆ†ææ–‡ä»¶: {os.path.basename(file_path1)}")
            similar_images = [(i, hash_str, img_data1, file_path1)]  # ä½¿ç”¨ç»Ÿä¸€çš„hash_str
            target_int = int(hash_str, 16)  # ä½¿ç”¨ç»Ÿä¸€çš„hash_str
            max_diff = 2 ** hamming_threshold

            # éå†æ‰€æœ‰å“ˆå¸Œå€¼è¿›è¡Œæ¯”è¾ƒ
            for current_hash in internal_hashes:
                if current_hash == hash_str:  # ä½¿ç”¨ç»Ÿä¸€çš„hash_stræ¯”è¾ƒ
                    continue

                current_idx, current_data, current_path, current_reason = hash_to_image[current_hash]
                if current_idx in processed_indices:
                    continue

                hamming_distance = ImageHashCalculator.calculate_hamming_distance(hash_str, current_hash)  # ä½¿ç”¨ç»Ÿä¸€çš„hash_str
                if hamming_distance <= hamming_threshold:
                    similar_images.append((current_idx, current_hash, current_data, current_path))  # ç§»é™¤current_reason

            # å¤„ç†ç›¸ä¼¼å›¾ç‰‡ç»„
            if len(similar_images) > 1:
                image_sizes = []
                for sim_img in similar_images:
                    idx, hash_val, img_data, file_path = sim_img  # ç°åœ¨æ‰€æœ‰å…ƒç»„éƒ½æ˜¯4å…ƒç»„
                    image_sizes.append((len(img_data), idx, img_data, file_path))
                
                image_sizes.sort(reverse=True)

                kept_idx = image_sizes[0][1]
                kept_image = next(x for x in similar_images if x[0] == kept_idx)
                final_images.append(remaining_images[kept_idx])
                processed_indices.add(kept_idx)

                # è®°å½•ç›¸ä¼¼æ€§å…³ç³»
                for size, idx, _, file_path in image_sizes[1:]:
                    processed_indices.add(idx)
                    normal_duplicates += 1
                    StatisticsManager.update_counts(normal_duplicates=1)
                    
                    # è·å–è¦æ¯”è¾ƒçš„ä¸¤ä¸ªå“ˆå¸Œå€¼
                    current_hash = remaining_images[idx][0]
                    kept_hash = remaining_images[kept_idx][0]
                    
                    # ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                    current_hash_str = current_hash['hash'] if isinstance(current_hash, dict) else current_hash
                    kept_hash_str = kept_hash['hash'] if isinstance(kept_hash, dict) else kept_hash
                    
                    # è®¡ç®—æ±‰æ˜è·ç¦»
                    hamming_distance = ImageHashCalculator.calculate_hamming_distance(
                        current_hash_str,
                        kept_hash_str
                    )
                    
                    HashFileHandler.record_similarity(file_path, kept_image[3], hamming_distance)
                    internal_removal_reasons[file_path] = 'normal_duplicate'
                    logger.info(f"[#hash_calc]å‘ç°é‡å¤å›¾ç‰‡ï¼Œå°†åˆ é™¤: {os.path.basename(file_path)}, è·ç¦»: {hamming_distance}")  
                    logger.info(f"[#hash_calc]é‡å¤è¯¦æƒ… - æº: {os.path.basename(kept_image[3])}, è·ç¦»: {hamming_distance}")
            else:
                final_images.append((hash1, img_data1, file_path1, reason))
                processed_indices.add(i)

        # æ›´æ–°ä¸»removal_reasons
        removal_reasons.update(internal_removal_reasons)
        return final_images, normal_duplicates, internal_removal_reasons

    @staticmethod
    def remove_duplicates_in_memory(image_hashes, params):
        """å¤„ç†é‡å¤å›¾ç‰‡"""
        unique_images = []
        hash_duplicates = 0
        normal_duplicates = 0
        skipped_images = {'hash_error': 0, 'small_images': 0, 'white_images': 0}
        removal_reasons = {}  # åˆå§‹åŒ–removal_reasonså­—å…¸

        # é¢„å¤„ç†ï¼šç»Ÿè®¡è·³è¿‡çš„å›¾ç‰‡
        for img in image_hashes:
            if img[3] == 'small_image':
                skipped_images['small_images'] += 1
                StatisticsManager.update_counts(small_images=1)
                removal_reasons[img[2]] = 'small_image'
            elif img[3] == 'white_image':
                skipped_images['white_images'] += 1
                StatisticsManager.update_counts(white_images=1)
                removal_reasons[img[2]] = 'white_image'
            elif img[0] is None:
                skipped_images['hash_error'] += 1

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        StatisticsManager.update_counts(
            hash_duplicates=hash_duplicates,
            normal_duplicates=normal_duplicates,
            small_images=skipped_images['small_images'],
            white_images=skipped_images['white_images']
        )

        # åŠ è½½å¤–éƒ¨å“ˆå¸Œæ–‡ä»¶
        ref_hashes, hash_to_uri = HashFileHandler.load_hash_file(params.get('hash_file'))

        # ç¬¬ä¸€æ­¥ï¼šä¸å‚è€ƒå“ˆå¸Œæ¯”è¾ƒï¼ˆä»…å½“æä¾›äº†å“ˆå¸Œæ–‡ä»¶æ—¶ï¼‰
        remaining_images = image_hashes
        hash_reasons = {}
        if ref_hashes:
            logger.info(f"[#hash_calc]å¼€å§‹å¤„ç†å¤–éƒ¨å“ˆå¸Œæ–‡ä»¶ï¼Œé•¿åº¦: {len(ref_hashes)}")
            remaining_images, hash_duplicates, hash_reasons = DuplicateDetector._compare_with_reference_hashes(
                image_hashes, ref_hashes, hash_to_uri, params
            )
            removal_reasons.update(hash_reasons)

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†å†…éƒ¨é‡å¤
        # æ²¡æœ‰å“ˆå¸Œæ–‡ä»¶æ—¶,æˆ–è€…æœ‰å“ˆå¸Œæ–‡ä»¶ä¸”å¯ç”¨äº†è‡ªèº«å»é‡æ—¶,è¿›è¡Œå†…éƒ¨å»é‡
        if not ref_hashes or params.get('self_redup', False):
            # ä½¿ç”¨hamming_distanceè¿›è¡Œå†…éƒ¨å»é‡
            internal_hamming_distance = params['hamming_distance']
            logger.info(f"[#hash_calc]å¼€å§‹å¤„ç†å†…éƒ¨é‡å¤å›¾ç‰‡ (ä½¿ç”¨hamming_distance: {internal_hamming_distance})")
            final_images, normal_duplicates, internal_reasons = DuplicateDetector._process_internal_duplicates(
                remaining_images, 
                internal_hamming_distance,
                removal_reasons
            )
            removal_reasons.update(internal_reasons)
        else:
            final_images = [(h, d, p, r) for h, d, p, r in remaining_images]

        # è®°å½•æ—¥å¿—
        logger.info( f'æ€»å…±åˆ é™¤å“ˆå¸Œé‡å¤å›¾ç‰‡: {hash_duplicates}')
        logger.info( f'æ€»å…±åˆ é™¤æ™®é€šé‡å¤å›¾ç‰‡: {normal_duplicates}')
        logger.info( f"æ€»å…±åˆ é™¤å°å›¾: {skipped_images['small_images']}")
        logger.info( f"æ€»å…±åˆ é™¤ç™½å›¾: {skipped_images['white_images']}")
        logger.info( f"æ€»å…±è·³è¿‡å“ˆå¸Œé”™è¯¯: {skipped_images['hash_error']}")

        return (final_images, skipped_images, removal_reasons)

class DirectoryHandler:
    """
    ç±»æè¿°
    """
    @staticmethod
    def remove_empty_directories(path, exclude_keywords=[]):
        """
        åˆ é™¤æŒ‡å®šè·¯å¾„ä¸‹çš„æ‰€æœ‰ç©ºæ–‡ä»¶å¤¹
        
        Args:
            path (str): ç›®æ ‡è·¯å¾„
            exclude_keywords (list): æ’é™¤å…³é”®è¯åˆ—è¡¨
        """
        removed_count = 0
        for root, dirs, _ in os.walk(path, topdown=False):
            if any((keyword in root for keyword in exclude_keywords)):
                continue
            for dir_name in dirs:
                folder_path = os.path.join(root, dir_name)
                try:
                    if not os.listdir(folder_path):
                        os.rmdir(folder_path)
                        removed_count += 1
                        logger.info(f'[#file_ops]å·²åˆ é™¤ç©ºæ–‡ä»¶å¤¹: {folder_path}')
                except Exception as e:
                    logger.info( f"âŒ åˆ é™¤ç©ºæ–‡ä»¶å¤¹å¤±è´¥ {folder_path}: {e}")
        if removed_count > 0:
            logger.info( f'å…±åˆ é™¤ {removed_count} ä¸ªç©ºæ–‡ä»¶å¤¹')
        return removed_count

class ArchiveExtractor:
    """
    ç±»æè¿°
    """
    @staticmethod
    def get_image_files(directory):
        """è·å–ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶"""
        image_files = []
        image_extensions = ('.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.bmp')
        
        for root, _, files in os.walk(directory):
            for file in files:
                if file.lower().endswith(image_extensions):
                    image_files.append(os.path.join(root, file))
                    
        return image_files



    @staticmethod
    def prepare_archive(file_path):
        """å‡†å¤‡å‹ç¼©åŒ…å¤„ç†ç¯å¢ƒ"""
        temp_dir = PathManager.create_temp_directory(file_path)
        backup_file_path = file_path + '.bak'
        new_zip_path = os.path.join(os.path.dirname(file_path), f'{os.path.splitext(os.path.basename(file_path))[0]}.new.zip')
        try:
            shutil.copy(file_path, backup_file_path)
            logger.info( f'åˆ›å»ºå¤‡ä»½: {backup_file_path}')
            cmd = ['7z', 'x', file_path, f'-o{temp_dir}', '-y']
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.info( f"âŒ è§£å‹å¤±è´¥: {file_path}\né”™è¯¯: {result.stderr}")
                PathManager.cleanup_temp_files(temp_dir, new_zip_path, backup_file_path)
                return (None, None, None)
            return (temp_dir, backup_file_path, new_zip_path)
        except Exception as e:
            logger.info( f"âŒ å‡†å¤‡ç¯å¢ƒå¤±è´¥ {file_path}: {e}")
            PathManager.cleanup_temp_files(temp_dir, new_zip_path, backup_file_path)
            return (None, None, None)

class ArchiveCompressor:
    """
    ç±»æè¿°
    """
    @staticmethod
    def run_7z_command(command, zip_path, operation='', additional_args=None):
        """
        æ‰§7zå‘½ä»¤çš„é€šå‡½æ•°
        
        Args:
            command: ä¸»å‘½ä»¤ (å¦‚ 'a', 'x', 'l' ç­‰)
            zip_path: å‹ç¼©åŒ…è·¯å¾„
            operation: æ“ä½œæè¿°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            additional_args: é¢å¤–çš„å‘½ä»¤è¡Œå‚æ•°
        """
        try:
            cmd = ['7z', command, zip_path]
            if additional_args:
                cmd.extend(additional_args)
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                logger.info( f'æˆåŠŸæ‰§è¡Œ7z {operation}: {zip_path}')
                return (True, result.stdout)
            else:
                logger.info( f"âŒ 7z {operation}å¤±è´¥: {zip_path}\né”™è¯¯: {result.stderr}")
                return (False, result.stderr)
        except Exception as e:
            logger.info( f"âŒ æ‰§è¡Œ7zå‘½ä»¤å‡ºé”™: {e}")
            return (False, str(e))



class ArchiveProcessor:
    """
    ç±»æè¿°
    """
    @staticmethod
    def merge_archives(paths, params):
        """
        åˆå¹¶å‹ç¼©åŒ…ä¸ºä¸€ä¸ªä¸´æ—¶å‹ç¼©åŒ…è¿›è¡Œå¤„ç†
        
        Args:
            paths: å‹ç¼©åŒ…è·¯å¾„åˆ—è¡¨æˆ–æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨
            params: å‚æ•°å­—å…¸
        
        Returns:
            (temp_dir, merged_zip_path, archive_paths): ä¸´æ—¶ç›®å½•ã€åˆå¹¶åçš„å‹ç¼©åŒ…è·¯å¾„å’ŒåŸå§‹å‹ç¼©åŒ…è·¯å¾„åˆ—è¡¨
        """
        temp_dir = None
        try:
            archive_paths = []
            for path in paths:
                if os.path.isdir(path):
                    for root, _, files in os.walk(path):
                        archive_paths.extend((os.path.join(root, file) for file in files if file.lower().endswith('.zip')))
                elif path.lower().endswith('.zip'):
                    archive_paths.append(path)

            # æ–°å¢å¤‡ä»½æ­¥éª¤ï¼šä¸ºæ¯ä¸ªè¦åˆå¹¶çš„å‹ç¼©åŒ…åˆ›å»ºå¤‡ä»½
            for zip_path in archive_paths:
                backup_path = zip_path + '.bak'
                try:
                    if not os.path.exists(backup_path):
                        shutil.copy2(zip_path, backup_path)
                        logger.info(f"[#file_ops]å·²åˆ›å»ºåˆå¹¶å‰å¤‡ä»½: {backup_path}")
                except Exception as e:
                    logger.error(f"[#file_ops]åˆ›å»ºåˆå¹¶å‰å¤‡ä»½å¤±è´¥ {zip_path}: {e}")

            if not archive_paths:
                logger.info( f"âŒ æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„å‹ç¼©åŒ…")
                return (None, None, None)
                
            directories = {os.path.dirname(path) for path in archive_paths}
            if len(directories) > 1:
                logger.info( f"âŒ æ‰€é€‰å‹ç¼©åŒ…ä¸åœ¨åŒä¸€ç›®å½•")
                return (None, None, None)
            base_dir = list(directories)[0]
            timestamp = int(time.time() * 1000)
            temp_dir = os.path.join(base_dir, f'temp_merge_{timestamp}')
            os.makedirs(temp_dir, exist_ok=True)
            for zip_path in archive_paths:
                logger.info( f'è§£å‹: {zip_path}')
                archive_name = os.path.splitext(os.path.basename(zip_path))[0]
                archive_temp_dir = os.path.join(temp_dir, archive_name)
                os.makedirs(archive_temp_dir, exist_ok=True)
                success, error = ArchiveCompressor.run_7z_command('x', zip_path, 'è§£å‹æ–‡ä»¶', [f'-o{archive_temp_dir}', '-y'])
                if not success:
                    logger.info( f"âŒ è§£å‹å¤±è´¥: {zip_path}\né”™è¯¯: {error}")
                    PathManager.cleanup_temp_files(temp_dir, None, None)
                    return (None, None, None)
            merged_zip_path = os.path.join(base_dir, f'merged_{timestamp}.zip')
            logger.info( 'åˆ›å»ºåˆå¹¶å‹ç¼©åŒ…')
            success, error = ArchiveCompressor.run_7z_command('a', merged_zip_path, 'åˆ›å»ºåˆå¹¶å‹ç¼©åŒ…', ['-tzip', os.path.join(temp_dir, '*')])
            if not success:
                logger.info( f"âŒ åˆ›å»ºåˆå¹¶å‹ç¼©åŒ…å¤±è´¥: {error}")
                PathManager.cleanup_temp_files(temp_dir, None, None)
                return (None, None, None)
            return (temp_dir, merged_zip_path, archive_paths)
        except Exception as e:
            logger.info( f"âŒ åˆå¹¶å‹ç¼©åŒ…æ—¶å‡ºé”™: {e}")
            if temp_dir and os.path.exists(temp_dir):
                PathManager.cleanup_temp_files(temp_dir, None, None)
            return (None, None, None)

    @staticmethod
    def process_single_archive(file_path, params):
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…æ–‡ä»¶"""
        try:
            logger.info( f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            
            if not os.path.exists(file_path):
                logger.info( f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return []
                
            cmd = ['7z', 'l', file_path]
            result = subprocess.run(cmd, capture_output=True, encoding='utf-8', errors='ignore')
            if result.returncode != 0:
                logger.info( f"âŒ å‹ç¼©åŒ…å¯èƒ½æŸå: {file_path}")
                return []
                
            if result.stdout is None:
                logger.info( f"âŒ æ— æ³•è¯»å–å‹ç¼©åŒ…å†…å®¹: {file_path}")
                return []
                
            has_images = any((line.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.jxl')) 
                            for line in result.stdout.splitlines()))
            if not has_images:
                logger.info( f"âš ï¸ è·³è¿‡æ— å›¾ç‰‡çš„å‹ç¼©åŒ…: {file_path}")
                return []
            processed_archives = []
            if not params['ignore_processed_log']:
                if ProcessedLogHandler.has_processed_log(file_path):
                    logger.info( f"âš ï¸ æ–‡ä»¶å·²æœ‰å¤„ç†è®°å½•: {file_path}")
                    return processed_archives
                    
            logger.info( "å¼€å§‹å¤„ç†å‹ç¼©åŒ…å†…å®¹...")
            processed_archives.extend(ArchiveProcessor.process_archive_in_memory(file_path, params))
            
            if processed_archives:
                # æ›´æ–°é‡å¤ä¿¡æ¯é¢æ¿
                info = processed_archives[-1]
                logger.info( 
                    f"å¤„ç†ç»“æœ:\n"
                    f"- å“ˆå¸Œé‡å¤: {info.get('hash_duplicates_removed', 0)} å¼ \n"
                    f"- æ™®é€šé‡å¤: {info.get('normal_duplicates_removed', 0)} å¼ \n"
                    f"- å°å›¾: {info.get('small_images_removed', 0)} å¼ \n"
                    f"- ç™½å›¾: {info.get('white_images_removed', 0)} å¼ \n"
                    f"- å‡å°‘å¤§å°: {info['size_reduction_mb']:.2f} MB"
                )
                
                # æ›´æ–°è¿›åº¦é¢æ¿
                logger.info( f"âœ… æˆåŠŸå¤„ç†: {os.path.basename(file_path)}")
                
                    
                if params['add_processed_log_enabled']:
                    processed_info = {
                        'hash_duplicates_removed': info.get('hash_duplicates_removed', 0),
                        'normal_duplicates_removed': info.get('normal_duplicates_removed', 0),
                        'small_images_removed': info.get('small_images_removed', 0),
                        'white_images_removed': info.get('white_images_removed', 0)
                    }
                    ProcessedLogHandler.add_processed_log(file_path, processed_info)
                    logger.info( "å·²æ·»åŠ å¤„ç†æ—¥å¿—")
            else:
                logger.info( f"âš ï¸ å‹ç¼©åŒ…å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰éœ€è¦å¤„ç†çš„å†…å®¹: {file_path}")
                
            backup_file_path = file_path + '.bak'
            if os.path.exists(backup_file_path):
                BackupHandler.handle_bak_file(backup_file_path, params)
                logger.info( "å·²å¤„ç†å¤‡ä»½æ–‡ä»¶")
                
            return processed_archives
            
        except UnicodeDecodeError as e:
            logger.info( f"âŒ å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºç°ç¼–ç é”™è¯¯ {file_path}: {e}")
            return []
        except Exception as e:
            logger.info( f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {file_path}\n{str(e)}")
            return []

    @staticmethod
    def split_merged_archive(processed_zip, original_archives, temp_dir, params):
        """
        å°†å¤„ç†åçš„åˆå¹¶å‹ç¼©åŒ…æ‹†åˆ†å›åŸå§‹å‹ç¼©åŒ…
        
        Args:
            processed_zip: å¤„ç†åçš„åˆå¹¶å‹ç¼©åŒ…è·¯å¾„
            original_archives: åŸå§‹å‹ç¼©åŒ…è·¯å¾„åˆ—è¡¨
            temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„
            params: å‚æ•°å­—å…¸
        """
        try:
            logger.info( 'å¼€å§‹æ‹†åˆ†å¤„ç†åçš„å‹ç¼©åŒ…')
            extract_dir = os.path.join(temp_dir, 'processed')
            os.makedirs(extract_dir, exist_ok=True)
            success, error = ArchiveCompressor.run_7z_command('x', processed_zip, 'è§£å‹å¤„ç†åçš„å‹ç¼©åŒ…', [f'-o{extract_dir}', '-y'])
            if not success:
                logger.info( f"âŒ è§£å‹å¤„ç†åçš„å‹ç¼©åŒ…å¤±è´¥: {error}")
                return False
            for original_zip in original_archives:
                archive_name = os.path.splitext(os.path.basename(original_zip))[0]
                source_dir = os.path.join(extract_dir, archive_name)
                if not os.path.exists(source_dir):
                    logger.info( f"âš ï¸ æ‰¾ä¸åˆ°å¯¹åº”çš„ç›®å½•: {source_dir}")
                    continue
                new_zip = original_zip + '.new'
                success, error = ArchiveCompressor.run_7z_command('a', new_zip, 'åˆ›å»ºæ–°å‹ç¼©åŒ…', ['-tzip', os.path.join(source_dir, '*')])
                if success:
                    try:
                        if params.get('backup_removed_files_enabled', True):
                            send2trash(original_zip)
                        else:
                            os.remove(original_zip)
                        os.rename(new_zip, original_zip)
                        logger.info( f'æˆåŠŸæ›´æ–°å‹ç¼©åŒ…: {original_zip}')
                    except Exception as e:
                        logger.info( f"âŒ æ›¿æ¢å‹ç¼©åŒ…å¤±è´¥ {original_zip}: {e}")
                else:
                    logger.info( f"âŒ åˆ›å»ºæ–°å‹ç¼©åŒ…å¤±è´¥ {new_zip}: {error}")
            return True
        except Exception as e:
            logger.info( f"âŒ æ‹†åˆ†å‹ç¼©åŒ…æ—¶å‡ºé”™: {e}")
            return False

    @staticmethod
    def handle_size_comparison(file_path, new_zip_path, backup_file_path):
        """
        æ¯”è¾ƒæ–°æ—§æ–‡ä»¶å¤§å°å¹¶å¤„ç†æ›¿æ¢
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            new_zip_path: æ–°å‹ç¼©åŒ…è·¯å¾„
            backup_file_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„
        
        Returns:
            (success, size_change): å¤„ç†æ˜¯å¦æˆåŠŸå’Œæ–‡ä»¶å¤§å°å˜åŒ–(MB)
        """
        try:
            if not os.path.exists(new_zip_path):
                logger.info( f"âŒ æ–°å‹ç¼©åŒ…ä¸å­˜åœ¨: {new_zip_path}")
                return (False, 0)
            original_size = os.path.getsize(file_path)
            new_size = os.path.getsize(new_zip_path)
            REDUNDANCY_SIZE = 1 * 1024 * 1024
            if new_size >= original_size + REDUNDANCY_SIZE:
                logger.info( f"âš ï¸ æ–°å‹ç¼©åŒ… ({new_size / 1024 / 1024:.2f}MB) æœªæ¯”åŸå§‹æ–‡ä»¶ ({original_size / 1024 / 1024:.2f}MB) å°è¶…è¿‡1MBï¼Œè¿˜åŸå¤‡ä»½")
                os.remove(new_zip_path)
                if os.path.exists(backup_file_path):
                    os.replace(backup_file_path, file_path)
                return (False, 0)
            os.replace(new_zip_path, file_path)
            size_change = (original_size - new_size) / (1024 * 1024)
            logger.info( f'æ›´æ–°å‹ç¼©åŒ…: {file_path} (å‡å°‘ {size_change:.2f}MB)')
            return (True, size_change)
        except Exception as e:
            logger.info( f"âŒ æ¯”è¾ƒæ–‡ä»¶å¤§å°æ—¶å‡ºé”™: {e}")
            if os.path.exists(backup_file_path):
                os.replace(backup_file_path, file_path)
            if os.path.exists(new_zip_path):
                os.remove(new_zip_path)
            return (False, 0)

    @staticmethod
    def process_archive_in_memory(file_path, params):
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…çš„ä¸»å‡½æ•°"""
        processed_archives = []
        temp_dir = None
        backup_file_path = None
        new_zip_path = None
        try:
            logger.info(f"[#file_ops]å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {file_path}")

            temp_dir, backup_file_path, new_zip_path = ArchiveExtractor.prepare_archive(file_path)
            if not temp_dir:
                logger.info(f"[#file_ops]âŒ å‡†å¤‡ç¯å¢ƒå¤±è´¥: {file_path}")
                return []
                
            logger.info(f"[#file_ops]ç¯å¢ƒå‡†å¤‡å®Œæˆ")
            
            image_files = ArchiveExtractor.get_image_files(temp_dir)
            if not image_files:
                logger.info(f"[#file_ops]âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
                PathManager.cleanup_temp_files(temp_dir, new_zip_path, backup_file_path)
                return []
                
            
            removed_files = set()
            duplicate_files = set()
            removal_reasons = {}  # åˆå§‹åŒ–removal_reasons
            lock = threading.Lock()
            existing_file_names = set()
            image_processor = ImageProcessor()
            # image_processor.set_global_hashes(global_hashes)  # è®¾ç½®å…¨å±€å“ˆå¸Œ
            
            # æ·»åŠ zip_pathåˆ°params
            params['zip_path'] = file_path
            
            # åœ¨å¤„ç†å›¾ç‰‡æ—¶æ˜¾ç¤ºè¿›åº¦
            with ThreadPoolExecutor(max_workers=params['max_workers']) as executor:
                futures = []
                total_files = len(image_files)
                processed_files = 0
                
                for img_path in image_files:
                    rel_path = os.path.relpath(img_path, temp_dir)
                    future = executor.submit(
                        image_processor.process_single_image, 
                        img_path, 
                        rel_path, 
                        existing_file_names, 
                        params, 
                        lock
                    )
                    futures.append((future, img_path))
                    
                image_hashes = []
                for future, img_path in futures:
                    try:
                        img_hash, img_data, _, reason = future.result()
                        processed_files += 1
                        percentage = (processed_files / total_files) * 100
                        logger.info(f"[@cur_progress] å¤„ç†å›¾ç‰‡ ({processed_files}/{total_files}) {percentage:.1f}%")
                        
                        if reason in ['small_image', 'white_image']:
                            removed_files.add(img_path)
                            removal_reasons[img_path] = reason
                        elif img_hash is not None and params['remove_duplicates']:
                            image_hashes.append((img_hash, img_data, img_path, reason))
                            
                    except Exception as e:
                        logger.info(f"[#hash_calc]âŒ å¤„ç†å›¾ç‰‡å¤±è´¥ {img_path}: {e}")
                        processed_files += 1
                        percentage = (processed_files / total_files) * 100
                        logger.info(f"[@hash_calc] å¤„ç†å›¾ç‰‡ ({processed_files}/{total_files}) {percentage:.1f}%")

            if params['remove_duplicates'] and image_hashes:
                unique_images, _, dup_removal_reasons = DuplicateDetector.remove_duplicates_in_memory(image_hashes, params)
                removal_reasons.update(dup_removal_reasons)  # åˆå¹¶åˆ é™¤åŸå› 
                processed_files = {img[2] for img in unique_images}
                for img_hash, _, img_path, _ in image_hashes:
                    if img_path not in processed_files:
                        duplicate_files.add(img_path)
                        
                # å¤„ç†å®Œæˆåï¼Œå°†ä¸´æ—¶å“ˆå¸Œæ›´æ–°åˆ°å…¨å±€å“ˆå¸Œ
                # if image_processor.temp_hashes:
                #     with lock:
                #         global_hashes.update(image_processor.temp_hashes)
                #         logger.info(f"[#hash_calc]å·²æ‰¹é‡æ·»åŠ  {len(image_processor.temp_hashes)} ä¸ªå“ˆå¸Œåˆ°å…¨å±€ç¼“å­˜")
                #         # æ¸…ç©ºä¸´æ—¶å­˜å‚¨
                #         image_processor.temp_hashes.clear()

            # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
            # ImageHashCalculator.save_global_hashes(global_hashes)  # æ³¨é‡Šæ‰åŸæ¥çš„å…¨å±€ä¿å­˜
            
            # ä¸ºå½“å‰å‹ç¼©åŒ…ä¿å­˜å“ˆå¸Œæ–‡ä»¶
            zip_path = params.get('zip_path')
            if zip_path:
                zip_dir = os.path.dirname(zip_path)
                zip_name = os.path.splitext(os.path.basename(zip_path))[0]                
                # æ„å»ºå‹ç¼©åŒ…ç‰¹å®šçš„å“ˆå¸Œå­—å…¸
                zip_hashes = {}
                for img_hash, _, img_path, _ in image_hashes:
                    if img_hash:
                        rel_path = os.path.relpath(img_path, temp_dir)
                        img_uri = PathURIGenerator.generate(f"{zip_path}!{rel_path}")
                        # ç»Ÿä¸€å“ˆå¸Œå€¼æ ¼å¼ï¼šå¦‚æœæ˜¯å­—å…¸åˆ™æå–hashå­—æ®µ
                        hash_value = img_hash['hash'] if isinstance(img_hash, dict) else img_hash
                        zip_hashes[img_uri] = {"hash": hash_value}  # ç›´æ¥å­˜å‚¨ä¸ºæ–°æ ¼å¼
                
                # ä¿å­˜åˆ°collectionæ–‡ä»¶
                try:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(HASH_COLLECTION_FILE), exist_ok=True)
                    
                    # è¯»å–ç°æœ‰collection
                    collection_data = {
                        "_hash_params": "hash_size=10;hash_version=1",
                        "dry_run": False,
                        "hashes": {}
                    }
                    
                    if os.path.exists(HASH_COLLECTION_FILE):
                        try:
                            with open(HASH_COLLECTION_FILE, 'r', encoding='utf-8') as f:
                                file_content = f.read().strip()
                                if not file_content:  # æ–‡ä»¶ä¸ºç©º
                                    logger.info(f"[#hash_calc]Collectionæ–‡ä»¶ä¸ºç©ºï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                                    collection_data = {
                                        "_hash_params": "hash_size=10;hash_version=1",
                                        "dry_run": False,
                                        "hashes": {}
                                    }
                                else:
                                    try:
                                        loaded_data = json.loads(file_content)
                                        if not isinstance(loaded_data, dict):
                                            raise ValueError("JSONæ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œä¸æ˜¯å­—å…¸æ ¼å¼")
                                            
                                        # ä¿ç•™åŸæœ‰çš„å…ƒæ•°æ®
                                        collection_data = {
                                            "_hash_params": loaded_data.get("_hash_params", "hash_size=10;hash_version=1"),
                                            "dry_run": loaded_data.get("dry_run", False),
                                            "hashes": {}
                                        }
                                        
                                        # å¤„ç†å“ˆå¸Œæ•°æ®
                                        if "hashes" in loaded_data and isinstance(loaded_data["hashes"], dict):
                                            collection_data["hashes"] = loaded_data["hashes"]
                                        else:
                                            # å°è¯•å¤„ç†æ—§æ ¼å¼
                                            for uri, hash_value in loaded_data.items():
                                                if uri not in ["_hash_params", "dry_run"]:
                                                    if isinstance(hash_value, str):
                                                        collection_data["hashes"][uri] = {"hash": hash_value}
                                                    elif isinstance(hash_value, dict) and "hash" in hash_value:
                                                        collection_data["hashes"][uri] = hash_value
                                                        
                                        logger.info(f"[#hash_calc]æˆåŠŸè¯»å–Collectionæ–‡ä»¶ï¼ŒåŒ…å« {len(collection_data['hashes'])} ä¸ªå“ˆå¸Œå€¼")
                                        
                                    except json.JSONDecodeError as je:
                                        # æ£€æŸ¥æ–‡ä»¶å†…å®¹ï¼Œè¾“å‡ºæ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                                        logger.error(f"[#hash_calc]JSONè§£æé”™è¯¯: {str(je)}")
                                        logger.error(f"[#hash_calc]æ–‡ä»¶å†…å®¹é¢„è§ˆ: {file_content[:200]}...")
                                        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚å¤„ç†
                                        
                        except (json.JSONDecodeError, ValueError) as e:
                            # åªæœ‰åœ¨ç¡®å®æ˜¯JSONæ ¼å¼é”™è¯¯æ—¶æ‰åˆ›å»ºå¤‡ä»½
                            error_time = int(time.time())
                            backup_path = f"{HASH_COLLECTION_FILE}.error_{error_time}"
                            shutil.copy2(HASH_COLLECTION_FILE, backup_path)
                            logger.error(f"[#hash_calc]Collectionæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå·²å¤‡ä»½åˆ°: {backup_path}")
                            logger.error(f"[#hash_calc]é”™è¯¯è¯¦æƒ…: {str(e)}")
                            # åˆ›å»ºæ–°çš„collectionæ•°æ®ç»“æ„
                            collection_data = {
                                "_hash_params": "hash_size=10;hash_version=1",
                                "dry_run": False,
                                "hashes": {}
                            }
                        except Exception as e:
                            logger.error(f"[#hash_calc]è¯»å–Collectionæ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
                            raise  # å¯¹äºå…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œå‘ä¸ŠæŠ›å‡º
                    else:
                        logger.info(f"[#hash_calc]Collectionæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                    
                    # æ›´æ–°collectionï¼ˆåˆå¹¶æ–°çš„å“ˆå¸Œå€¼ï¼‰
                    collection_data["hashes"].update(zip_hashes)
                    
                    # åœ¨å†™å…¥ä¹‹å‰éªŒè¯æ•°æ®ç»“æ„
                    if not isinstance(collection_data, dict) or "hashes" not in collection_data:
                        raise ValueError("Collectionæ•°æ®ç»“æ„æ— æ•ˆ")
                    
                    # ä¿å­˜æ›´æ–°åçš„collection
                    temp_file = f"{HASH_COLLECTION_FILE}.temp"
                    try:
                        with open(temp_file, 'w', encoding='utf-8') as f:
                            json.dump(collection_data, f, ensure_ascii=False, indent=2)
                        # å¦‚æœå†™å…¥æˆåŠŸï¼Œæ›¿æ¢åŸæ–‡ä»¶
                        os.replace(temp_file, HASH_COLLECTION_FILE)
                        logger.info(f"[#hash_calc]å·²æ›´æ–° {len(zip_hashes)} ä¸ªå“ˆå¸Œåˆ°collectionæ–‡ä»¶")
                    except Exception as e:
                        if os.path.exists(temp_file):
                            os.remove(temp_file)
                        raise
                except Exception as e:
                    logger.error(f"[#file_ops]ä¿å­˜collectionæ–‡ä»¶å¤±è´¥: {str(e)}")
                    # å°è¯•å¤‡ä»½æŸåçš„æ–‡ä»¶
                    if os.path.exists(HASH_COLLECTION_FILE):
                        backup_path = HASH_COLLECTION_FILE + '.bak'
                        try:
                            shutil.copy2(HASH_COLLECTION_FILE, backup_path)
                            logger.info(f"[#hash_calc]å·²å¤‡ä»½å¯èƒ½æŸåçš„collectionæ–‡ä»¶åˆ°: {backup_path}")
                        except Exception as backup_error:
                            logger.error(f"[#hash_calc]å¤‡ä»½collectionæ–‡ä»¶å¤±è´¥: {str(backup_error)}")
            
            if not ArchiveProcessor.cleanup_and_compress(temp_dir, removed_files, duplicate_files, new_zip_path, params, removal_reasons):
                logger.info( f"âŒ æ¸…ç†å’Œå‹ç¼©å¤±è´¥: {file_path}")
                if os.path.exists(backup_file_path):
                    os.replace(backup_file_path, file_path)
                return []
            if not os.path.exists(new_zip_path):
                logger.info( f"âŒ æ–°å‹ç¼©åŒ…ä¸å­˜åœ¨: {new_zip_path}")
                if os.path.exists(backup_file_path):
                    os.replace(backup_file_path, file_path)
                return []
            original_size = os.path.getsize(file_path)
            new_size = os.path.getsize(new_zip_path)
            REDUNDANCY_SIZE = 1 * 1024 * 1024
            if new_size >= original_size + REDUNDANCY_SIZE:
                logger.info( f"âš ï¸ æ–°å‹ç¼©åŒ… ({new_size / 1024 / 1024:.2f}MB) ä¸å°äºåŸå§‹æ–‡ä»¶ ({original_size / 1024 / 1024:.2f}MB)ï¼Œè¿˜åŸå¤‡ä»½")
                os.remove(new_zip_path)
                if os.path.exists(backup_file_path):
                    os.replace(backup_file_path, file_path)
                return []
            # æ›¿æ¢åŸå§‹æ–‡ä»¶
            os.replace(new_zip_path, file_path)
            # è®© BackupHandler.handle_bak_file æ¥å¤„ç†å¤‡ä»½æ–‡ä»¶ï¼Œä¸åœ¨è¿™é‡Œç›´æ¥åˆ é™¤
            BackupHandler.handle_bak_file(backup_file_path, params)
            
            result = {
                'file_path': file_path,
                'hash_duplicates_removed': len([f for f in duplicate_files if removal_reasons.get(f) == 'hash_duplicate']),
                'normal_duplicates_removed': len([f for f in duplicate_files if removal_reasons.get(f) == 'normal_duplicate']),
                'small_images_removed': len([f for f in removed_files if removal_reasons.get(f) == 'small_image']),
                'white_images_removed': len([f for f in removed_files if removal_reasons.get(f) == 'white_image']),
                'size_reduction_mb': (original_size - new_size) / (1024 * 1024)
            }
            processed_archives.append(result)
        except Exception as e:
            logger.info( f"âŒ å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™ {file_path}: {e}")
            if backup_file_path and os.path.exists(backup_file_path):
                os.replace(backup_file_path, file_path)
            return []
        finally:
            PathManager.cleanup_temp_files(temp_dir, new_zip_path, backup_file_path)
        return processed_archives

    @staticmethod
    def cleanup_and_compress(temp_dir, removed_files, duplicate_files, new_zip_path, params, removal_reasons):
        """æ¸…ç†æ–‡ä»¶å¹¶åˆ›å»ºæ–°å‹ç¼©åŒ…"""
        try:
            if removed_files is None:
                removed_files = set()
            if duplicate_files is None:
                duplicate_files = set()
            if not isinstance(removed_files, set) or not isinstance(duplicate_files, set):
                logger.info( f"âŒ æ— æ•ˆçš„å‚æ•°ç±»å‹: removed_files={type(removed_files)}, duplicate_files={type(duplicate_files)}")
                return False
            BackupHandler.backup_removed_files(new_zip_path, removed_files, duplicate_files, params, removal_reasons)
            all_files_to_remove = removed_files | duplicate_files
            removed_count = 0
            for file_path in all_files_to_remove:
                try:
                    if file_path and os.path.exists(file_path):
                        os.remove(file_path)
                        removed_count += 1
                        logger.info( f'å·²åˆ é™¤æ–‡ä»¶: {file_path}')
                except Exception as e:
                    logger.info( f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue
            if removed_count > 0:
                logger.info( f'å·²åˆ é™¤ {removed_count} ä¸ªæ–‡ä»¶')
            empty_dirs_removed = DirectoryHandler.remove_empty_directories(temp_dir)
            if empty_dirs_removed > 0:
                logger.info( f'å·²åˆ é™¤ {empty_dirs_removed} ä¸ªç©ºæ–‡ä»¶å¤¹')
            if not os.path.exists(temp_dir) or not any(os.scandir(temp_dir)):
                logger.info( f'ä¸´æ—¶ç›®å½•ä¸ºç©ºæˆ–ä¸å­˜åœ¨: {temp_dir}')
                temp_empty_file = os.path.join(temp_dir, '.empty')
                os.makedirs(temp_dir, exist_ok=True)
                with open(temp_empty_file, 'w') as f:
                    pass
                success, error = ArchiveCompressor.run_7z_command('a', new_zip_path, 'åˆ›å»ºç©ºå‹ç¼©åŒ…', ['-tzip', temp_empty_file])
                os.remove(temp_empty_file)
                if success and os.path.exists(new_zip_path):
                    logger.info( f'æˆåŠŸåˆ›å»ºç©ºå‹ç¼©åŒ…: {new_zip_path}')
                    return True
                else:
                    logger.info( f"âŒ åˆ›å»ºç©ºå‹ç¼©åŒ…å¤±è´¥: {error}")
                    return False
            success, error = ArchiveCompressor.run_7z_command('a', new_zip_path, 'åˆ›å»ºæ–°å‹ç¼©åŒ…', ['-tzip', os.path.join(temp_dir, '*')])
            if success:
                if not os.path.exists(new_zip_path):
                    logger.info( f"âŒ å‹ç¼©åŒ…åˆ›å»ºå¤±è´¥: {new_zip_path}")
                    return False
                logger.info( f'æˆåŠŸåˆ›å»ºæ–°å‹ç¼©åŒ…: {new_zip_path}')
                return True
            else:
                logger.info( f"âŒ åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥: {error}")
                return False
        except Exception as e:
            logger.info( f"âŒ æ¸…ç†å’Œå‹ç¼©æ—¶å‡ºé”™: {e}")
            return False

class ProcessedLogHandler:
    """
    ç±»æè¿°
    """
    @staticmethod
    def has_processed_log(zip_path):
        command = ['7z', 'l', zip_path]
        result = subprocess.run(command, capture_output=True, text=True)
        if result.returncode == 0:
            if 'processed.log' in result.stdout:
                ProcessedLogHandler.save_processed_file(zip_path)
                return True
        else:
            logger.info( f"âŒ Failed to list contents of {zip_path}: {result.stderr}")
        return False

    @staticmethod
    def add_processed_log(zip_path, processed_info):
        """
        å°†å¤„ç†æ—¥å¿—æ·»åŠ åˆ°å‹ç¼©åŒ…ä¸­
        
        Args:
            zip_path: å‹ç¼©åŒ…è·¯å¾„
            processed_info: å¤„ç†ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«:
                - hash_duplicates_removed: å“ˆå¸Œé‡å¤æ•°é‡
                - normal_duplicates_removed: æ™®é€šé‡å¤æ•°é‡
                - small_images_removed: å°å›¾æ•°é‡
                - white_images_removed: ç™½å›¾æ•°é‡
        """
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = os.path.join(os.path.dirname(zip_path), 'temp_log')
            os.makedirs(temp_dir, exist_ok=True)
            
            # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
            log_file_path = os.path.join(temp_dir, 'processed.log')
            with open(log_file_path, 'w', encoding='utf-8') as log_file:
                # åŸºæœ¬å¤„ç†ä¿¡æ¯
                log_file.write(f'{os.path.basename(zip_path)} - å¤„ç†æ—¶é—´: {datetime.now()} - å¤„ç†æƒ…å†µ:\n')
                log_file.write(f" - åˆ é™¤çš„å“ˆå¸Œé‡å¤å›¾ç‰‡: {processed_info.get('hash_duplicates_removed', 0)}\n")
                log_file.write(f" - åˆ é™¤çš„æ™®é€šé‡å¤å›¾ç‰‡: {processed_info.get('normal_duplicates_removed', 0)}\n")
                log_file.write(f" - åˆ é™¤çš„å°å›¾æ•°é‡: {processed_info.get('small_images_removed', 0)}\n")
                log_file.write(f" - åˆ é™¤çš„ç™½å›¾æ•°é‡: {processed_info.get('white_images_removed', 0)}\n\n")
                
                # æ·»åŠ ç›¸ä¼¼æ€§è®°å½•
                similarity_records = HashFileHandler.get_similarity_records()
                if similarity_records:
                    log_file.write("ç›¸ä¼¼æ€§è®°å½•:\n")
                    for record in similarity_records:
                        log_file.write(f" - æ–‡ä»¶: {os.path.basename(record['file_path'])}\n")
                        log_file.write(f"   ç›¸ä¼¼äº: {record['similar_uri']}\n")
                        log_file.write(f"   æ±‰æ˜è·ç¦»: {record['hamming_distance']}\n")
                        log_file.write(f"   è®°å½•æ—¶é—´: {record['timestamp']}\n")
                    log_file.write("\n")
            
            # å°†æ—¥å¿—æ–‡ä»¶æ·»åŠ åˆ°å‹ç¼©åŒ…
            command = ['7z', 'a', zip_path, log_file_path]
            result = subprocess.run(command, capture_output=True, text=True)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
            os.remove(log_file_path)
            os.rmdir(temp_dir)
            
            # æ¸…ç©ºç›¸ä¼¼æ€§è®°å½•ï¼Œä¸ºä¸‹ä¸€ä¸ªæ–‡ä»¶åšå‡†å¤‡
            HashFileHandler.clear_similarity_records()
            
            if result.returncode == 0:
                logger.info( f'æˆåŠŸæ·»åŠ å¤„ç†æ—¥å¿—åˆ°å‹ç¼©åŒ…: {zip_path}')
            else:
                logger.info( f"âŒ æ·»åŠ æ—¥å¿—åˆ°å‹ç¼©åŒ…å¤±è´¥: {result.stderr}")
                
        except Exception as e:
            logger.info( f"âŒ æ·»åŠ æ—¥å¿—åˆ°å‹ç¼©åŒ…æ—¶å‡ºé”™: {e}")
            # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(log_file_path):
                os.remove(log_file_path)
            if os.path.exists(temp_dir):
                os.rmdir(temp_dir)

class BackupHandler:
    """
    ç±»æè¿°
    """
    @staticmethod
    def handle_bak_file(bak_path, params=None):
        """
        æ ¹æ®æŒ‡å®šæ¨¡å¼å¤„ç†bakæ–‡ä»¶
        
        Args:
            bak_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„
            params: å‚æ•°å­—å…¸æˆ–Namespaceå¯¹è±¡ï¼ŒåŒ…å«:
                - bak_mode: å¤‡ä»½æ–‡ä»¶å¤„ç†æ¨¡å¼ ('keep', 'recycle', 'delete')
                - backup_removed_files_enabled: æ˜¯å¦ä½¿ç”¨å›æ”¶ç«™
        """
        try:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if params is None:
                params = {}
            
            # è·å–æ¨¡å¼ï¼Œæ”¯æŒå­—å…¸å’ŒNamespaceå¯¹è±¡ï¼Œé»˜è®¤ä¸ºkeep
            mode = params.bak_mode if hasattr(params, 'bak_mode') else params.get('bak_mode', 'keep')
            
            if mode == 'keep':
                logger.info( f'ä¿ç•™å¤‡ä»½æ–‡ä»¶: {bak_path}')
                return
                
            if not os.path.exists(bak_path):
                logger.info( f'å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {bak_path}')
                return
                
            # è·å–æ˜¯å¦ä½¿ç”¨å›æ”¶ç«™ï¼Œæ”¯æŒå­—å…¸å’ŒNamespaceå¯¹è±¡

            # å¦‚æœæ˜¯å›æ”¶ç«™æ¨¡å¼ï¼Œæˆ–è€…å¯ç”¨äº†å¤‡ä»½æ–‡ä»¶
            if mode == 'recycle':
                try:
                    send2trash(bak_path)
                    logger.info( f'å·²å°†å¤‡ä»½æ–‡ä»¶ç§»è‡³å›æ”¶ç«™: {bak_path}')
                except Exception as e:
                    logger.info( f"âŒ ç§»åŠ¨å¤‡ä»½æ–‡ä»¶åˆ°å›æ”¶ç«™å¤±è´¥ {bak_path}: {e}")
            # åªæœ‰åœ¨æ˜ç¡®æŒ‡å®šåˆ é™¤æ¨¡å¼æ—¶æ‰ç›´æ¥åˆ é™¤
            elif mode == 'delete':
                try:
                    os.remove(bak_path)
                    logger.info( f'å·²åˆ é™¤å¤‡ä»½æ–‡ä»¶: {bak_path}')
                except Exception as e:
                    logger.info( f"âŒ åˆ é™¤å¤‡ä»½æ–‡ä»¶å¤±è´¥ {bak_path}: {e}")
        except Exception as e:
            logger.info( f"âŒ å¤„ç†å¤‡ä»½æ–‡ä»¶æ—¶å‡ºé”™ {bak_path}: {e}")



    @staticmethod
    def backup_removed_files(zip_path, removed_files, duplicate_files, params, removal_reasons):
        """
        å°†åˆ é™¤çš„æ–‡ä»¶å¤‡ä»½åˆ°trashæ–‡ä»¶å¤¹ä¸­ï¼Œä¿æŒåŸå§‹ç›®å½•ç»“æ„
        
        Args:
            zip_path: åŸå§‹å‹ç¼©åŒ…è·¯å¾„
            removed_files: è¢«åˆ é™¤çš„å°å›¾/ç™½å›¾æ–‡ä»¶é›†åˆ
            duplicate_files: è¢«åˆ é™¤çš„é‡å¤å›¾ç‰‡æ–‡ä»¶é›†åˆ
            params: å‚æ•°å­—å…¸
            removal_reasons: æ–‡ä»¶åˆ é™¤åŸå› çš„å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶è·¯å¾„ï¼Œå€¼ä¸ºåˆ é™¤åŸå› 
        """
        try:
            if not params.get('backup_removed_files_enabled', True):
                logger.info( 'è·³è¿‡å¤‡ä»½åˆ é™¤çš„æ–‡ä»¶')
                return
            if not removed_files and (not duplicate_files):
                return
                
            zip_name = os.path.splitext(os.path.basename(zip_path))[0]
            trash_dir = os.path.join(os.path.dirname(zip_path), f'{zip_name}.trash')
            os.makedirs(trash_dir, exist_ok=True)
            
            # åˆ†ç±»å¤‡ä»½ä¸åŒç±»å‹çš„æ–‡ä»¶
            for file_path in removed_files | duplicate_files:
                try:
                    # æ ¹æ®è®°å½•çš„åˆ é™¤åŸå› ç¡®å®šå­ç›®å½•
                    reason = removal_reasons.get(file_path)
                    if reason == 'hash_duplicate':
                        subdir = 'hash_duplicates'
                    elif reason == 'normal_duplicate':
                        subdir = 'normal_duplicates'
                    elif reason == 'small_image':
                        subdir = 'small_images'
                    elif reason == 'white_image':
                        subdir = 'white_images'
                    else:
                        subdir = 'other'
                    
                    # åˆ›å»ºç›®æ ‡è·¯å¾„å¹¶å¤åˆ¶æ–‡ä»¶
                    rel_path = os.path.relpath(file_path, os.path.dirname(zip_path))
                    dest_path = os.path.join(trash_dir, subdir, rel_path)
                    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                    shutil.copy2(file_path, dest_path)
                    logger.info( f"å·²å¤‡ä»½åˆ° {subdir}: {rel_path}")
                    
                except Exception as e:
                    logger.info( f"âŒ å¤‡ä»½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue
                
            logger.info( f'å·²å¤‡ä»½åˆ é™¤çš„æ–‡ä»¶åˆ°: {trash_dir}')
            
        except Exception as e:
            logger.info( f"âŒ å¤‡ä»½åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {e}")

class ContentFilter:
    """
    ç±»æè¿°
    """

    @staticmethod
    def should_process_file(file_path, params):
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦éœ€è¦å¤„ç†"""
        logger.info( f'\nå¼€å§‹æ£€æŸ¥æ–‡ä»¶æ˜¯å¦éœ€è¦å¤„ç†: {file_path}')
        if params['exclude_paths']:
            for exclude_path in params['exclude_paths']:
                if exclude_path in file_path:
                    logger.info( f'æ–‡ä»¶åœ¨æ’é™¤è·¯å¾„ä¸­ (æ’é™¤å…³é”®è¯: {exclude_path})')
                    return False
        logger.info( 'æ–‡ä»¶é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼Œå°†è¿›è¡Œå¤„ç†')
        return True
class ProcessManager:
    """
    ç±»æè¿°
    """
    @staticmethod
    def generate_summary_report(processed_archives):
        """ç”Ÿæˆå¤„ç†æ‘˜è¦å¹¶æ˜¾ç¤ºåˆ°é¢æ¿"""
        if not processed_archives:
            logger.info( 'æ²¡æœ‰å¤„ç†ä»»ä½•å‹ç¼©åŒ…ã€‚')
            return
            
        # ä½¿ç”¨StatisticsManagerä¸­çš„ç»Ÿè®¡æ•°æ®
        summary = [
            "ğŸ“Š å¤„ç†å®Œæˆæ‘˜è¦",
            f"æ€»å…±å¤„ç†: {len(processed_archives)} ä¸ªå‹ç¼©åŒ…",
            f"åˆ é™¤å“ˆå¸Œé‡å¤å›¾ç‰‡: {StatisticsManager.hash_duplicates_count} å¼ ",
            f"åˆ é™¤æ™®é€šé‡å¤å›¾ç‰‡: {StatisticsManager.normal_duplicates_count} å¼ ",
            f"åˆ é™¤å°å›¾: {StatisticsManager.small_images_count} å¼ ",
            f"åˆ é™¤ç™½å›¾: {StatisticsManager.white_images_count} å¼ ",
            f"æ€»å…±å‡å°‘: {sum(archive['size_reduction_mb'] for archive in processed_archives):.2f} MB",
            "\nè¯¦ç»†ä¿¡æ¯:"
        ]
        
        # æŒ‰ç›®å½•ç»„ç»‡å¤„ç†ç»“æœ
        common_path_prefix = os.path.commonpath([archive['file_path'] for archive in processed_archives])
        tree_structure = {}
        for archive in processed_archives:
            relative_path = os.path.relpath(archive['file_path'], common_path_prefix)
            path_parts = relative_path.split(os.sep)
            current_level = tree_structure
            for part in path_parts:
                current_level = current_level.setdefault(part, {})
            current_level['_summary'] = (
                f"å“ˆå¸Œé‡å¤: {archive.get('hash_duplicates_removed', 0)} å¼ , "
                f"æ™®é€šé‡å¤: {archive.get('normal_duplicates_removed', 0)} å¼ , "
                f"å°å›¾: {archive.get('small_images_removed', 0)} å¼ , "
                f"ç™½å›¾: {archive.get('white_images_removed', 0)} å¼ , "
                f"å‡å°‘: {archive['size_reduction_mb']:.2f} MB"
            )
        
        # ç”Ÿæˆæ ‘å½¢ç»“æ„çš„è¯¦ç»†ä¿¡æ¯
        def build_tree_text(level, indent=''):
            tree_text = []
            for name, content in level.items():
                if name == '_summary':
                    tree_text.append(f'{indent}{content}')
                else:
                    tree_text.append(f'{indent}â”œâ”€ {name}')
                    tree_text.extend(build_tree_text(content, indent + 'â”‚   '))
            return tree_text
        
        # æ·»åŠ æ ‘å½¢ç»“æ„åˆ°æ‘˜è¦
        summary.extend(build_tree_text(tree_structure))
        
        # æ›´æ–°åˆ°é¢æ¿
        logger.info( '\n'.join(summary))
        logger.info( "âœ… å¤„ç†å®Œæˆï¼Œå·²ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š")


    @staticmethod
    def process_normal_archives(directories, args):
        """å¤„ç†æ™®é€šæ¨¡å¼çš„å‹ç¼©åŒ…æˆ–ç›®å½•"""
        for directory in directories:
            if os.path.exists(directory):
                params = InputHandler.prepare_params(args)
                ProcessManager.process_all_archives([directory], params)
                if args.bak_mode != 'keep':
                    for root, _, files in os.walk(directory):
                        for file in files:
                            if file.endswith('.bak'):
                                bak_path = os.path.join(root, file)
                                BackupHandler.handle_bak_file(bak_path, args)
                DirectoryHandler.remove_empty_directories(directory)
            else:
                logger.info( f"è¾“å…¥çš„è·¯å¾„ä¸å­˜åœ¨: {directory}")

    @staticmethod
    def process_merged_archives(directories, args):
        """å¤„ç†åˆå¹¶æ¨¡å¼çš„å‹ç¼©åŒ…"""
        temp_dir, merged_zip, archive_paths = ArchiveProcessor.merge_archives(directories, args)
        if temp_dir and merged_zip:
            try:
                params = InputHandler.prepare_params(args)
                ProcessManager.process_all_archives([merged_zip], params)
                if ArchiveProcessor.split_merged_archive(merged_zip, archive_paths, temp_dir, params):
                    logger.info( 'æˆåŠŸå®Œæˆå‹ç¼©åŒ…çš„åˆå¹¶å¤„ç†å’Œæ‹†åˆ†')
                else:
                    logger.info( 'æ‹†åˆ†å‹ç¼©åŒ…å¤±è´¥')
            finally:
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
                if os.path.exists(merged_zip):
                    os.remove(merged_zip)

    @staticmethod
    def print_config(args, max_workers):
        """æ‰“å°å½“å‰é…ç½®ä¿¡æ¯"""
        # æ¸…å±
        os.system('cls' if os.name == 'nt' else 'clear')
        
        # ä½¿ç”¨log_panelè¾“å‡ºé…ç½®ä¿¡æ¯
        config_info = [
            '\n=== å½“å‰é…ç½®ä¿¡æ¯ ===',
            'å¯ç”¨çš„åŠŸèƒ½:',
            f"  - å°å›¾è¿‡æ»¤: {('æ˜¯' if args.remove_small else 'å¦')}"
        ]
        
        if args.remove_small:
            config_info.append(f'    æœ€å°å°ºå¯¸: {args.min_size}x{args.min_size} åƒç´ ')
            
        config_info.extend([
            f"  - é»‘ç™½å›¾è¿‡æ»¤: {('æ˜¯' if args.remove_grayscale else 'å¦')}"
        ])
        

            
        config_info.extend([
            f"  - é‡å¤å›¾ç‰‡è¿‡æ»¤: {('æ˜¯' if args.remove_duplicates else 'å¦')}"
        ])
        
        if args.remove_duplicates:
            config_info.extend([
                f'    å†…éƒ¨å»é‡æ±‰æ˜è·ç¦»é˜ˆå€¼: {args.hamming_distance}',
                f'    å¤–éƒ¨å‚è€ƒæ±‰æ˜è·ç¦»é˜ˆå€¼: {args.ref_hamming_distance}'
            ])
            
        config_info.extend([
            f"  - åˆå¹¶å‹ç¼©åŒ…å¤„ç†: {('æ˜¯' if args.merge_archives else 'å¦')}",
            f"ä»å‰ªè´´æ¿è¯»å–: {('æ˜¯' if args.clipboard else 'å¦')}",
            f'å¤‡ä»½æ–‡ä»¶å¤„ç†æ¨¡å¼: {args.bak_mode}',
            f'çº¿ç¨‹æ•°: {max_workers}',
            '==================\n'
        ])
        
        initialize_textual_logger()
        logger.info( '\n'.join(config_info))

    @staticmethod
    def process_all_archives(directories, params):
        """
        ä¸»å¤„ç†å‡½æ•°
        
        Args:
            directories: è¦å¤„ç†çš„ç›®å½•åˆ—è¡¨
            params: å‚æ•°å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„å¤„ç†å‚æ•°
        """

            
        processed_archives = []
        logger.info( "å¼€å§‹å¤„ç†æ‹–å…¥çš„ç›®å½•æˆ–æ–‡ä»¶")
        
        # è®¡ç®—æ€»æ–‡ä»¶æ•°
        total_zip_files = sum((1 for directory in directories 
                             for root, _, files in os.walk(directory) 
                             for file in files if file.lower().endswith('zip')))
        
        # æ›´æ–°æ€»ä½“è¿›åº¦é¢æ¿
        logger.info( 
            f"æ€»æ–‡ä»¶æ•°: {total_zip_files}\n"
            f"å·²å¤„ç†: 0\n"
            f"æˆåŠŸ: 0\n"
            f"è­¦å‘Š: 0\n"
            f"é”™è¯¯: 0"
        )

        # è®¾ç½®æ€»æ•°
        StatisticsManager.set_total(total_zip_files)
            
        for directory in directories:
            archives = ProcessManager.process_directory(directory, params)
            processed_archives.extend(archives)
                
            # æ›´æ–°æ€»ä½“è¿›åº¦
            success_count = sum(1 for a in archives if (a.get('duplicates_removed', 0) > 0 or 
                                                      a.get('small_images_removed', 0) > 0 or 
                                                      a.get('white_images_removed', 0) > 0))
            warning_count = sum(1 for a in archives if (a.get('duplicates_removed', 0) == 0 and 
                                                      a.get('small_images_removed', 0) == 0 and 
                                                      a.get('white_images_removed', 0) == 0))
            error_count = StatisticsManager.processed_count - len(archives)
                
            logger.info( 
                f"æ€»æ–‡ä»¶æ•°: {total_zip_files}\n"
                f"å·²å¤„ç†: {StatisticsManager.processed_count}\n"
                f"æˆåŠŸ: {success_count}\n"
                f"è­¦å‘Š: {warning_count}\n"
                f"é”™è¯¯: {error_count}"
            )
        
        ProcessManager.generate_summary_report(processed_archives)
        logger.info( "æ‰€æœ‰ç›®å½•å¤„ç†å®Œæˆ")
        return processed_archives

    @staticmethod
    def process_directory(directory, params):
        """å¤„ç†å•ä¸ªç›®å½•"""
        try:
            logger.info( f"\nå¼€å§‹å¤„ç†ç›®å½•: {directory}")
            processed_archives = []
            if os.path.isfile(directory):
                logger.info( f"å¤„ç†å•ä¸ªæ–‡ä»¶: {directory}")
                if directory.lower().endswith('zip'):
                    if ContentFilter.should_process_file(directory, params):
                        logger.info( f"å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {directory}")
                        archives = ProcessManager.process_single_archive(directory, params)
                        processed_archives.extend(archives)
                    else:
                        logger.info( f"è·³è¿‡æ–‡ä»¶ï¼ˆæ ¹æ®è¿‡æ»¤è§„åˆ™ï¼‰: {directory}")
                    StatisticsManager.increment()
                else:
                    logger.info( f"è·³è¿‡ézipæ–‡ä»¶: {directory}")
            elif os.path.isdir(directory):
                logger.info( f"æ‰«æç›®å½•ä¸­çš„æ–‡ä»¶: {directory}")
                files_to_process = []
                for root, _, files in os.walk(directory):
                    logger.debug( f"æ‰«æå­ç›®å½•: {root}")
                    for file in files:
                        if file.lower().endswith('zip'):
                            file_path = os.path.join(root, file)
                            logger.info( f"å‘ç°zipæ–‡ä»¶: {file_path}")
                            if ContentFilter.should_process_file(file_path, params):
                                logger.info( f"æ·»åŠ åˆ°å¤„ç†åˆ—è¡¨: {file_path}")
                                files_to_process.append(file_path)
                            else:
                                logger.info( f"è·³è¿‡æ–‡ä»¶ï¼ˆæ ¹æ®è¿‡æ»¤è§„åˆ™ï¼‰: {file_path}")
                                StatisticsManager.increment()
                logger.info( f"æ‰«æå®Œæˆ: æ‰¾åˆ° {len(files_to_process)} ä¸ªè¦å¤„ç†çš„æ–‡ä»¶")
                for file_path in files_to_process:
                    try:
                        logger.info( f"\næ­£åœ¨å¤„ç†å‹ç¼©åŒ…: {file_path}")
                        archives = ProcessManager.process_single_archive(file_path, params)
                        if archives:
                            logger.info( f"æˆåŠŸå¤„ç†å‹ç¼©åŒ…: {file_path}")
                        else:
                            logger.info( f"å‹ç¼©åŒ…å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰å˜åŒ–: {file_path}")
                        processed_archives.extend(archives)
                    except Exception as e:
                        logger.info( f"å¤„ç†å‹ç¼©åŒ…å‡ºé”™: {file_path}\né”™è¯¯: {e}")
                    finally:
                        StatisticsManager.increment()
            if os.path.isdir(directory):
                exclude_keywords = params.get('exclude_paths', [])
            return processed_archives
        except Exception as e:
            logger.info( f"å¤„ç†ç›®å½•æ—¶å‘ç”Ÿå¼‚å¸¸: {directory}\n{str(e)}")
            return []

    @staticmethod
    def process_single_archive(file_path, params):
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…æ–‡ä»¶"""
        try:
            logger.info( f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            
            if not os.path.exists(file_path):
                logger.info( f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return []
                
            cmd = ['7z', 'l', file_path]
            result = subprocess.run(cmd, capture_output=True, encoding='utf-8', errors='ignore')
            if result.returncode != 0:
                logger.info( f"âŒ å‹ç¼©åŒ…å¯èƒ½æŸå: {file_path}")
                return []
                
            if result.stdout is None:
                logger.info( f"âŒ æ— æ³•è¯»å–å‹ç¼©åŒ…å†…å®¹: {file_path}")
                return []
                
            has_images = any((line.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.jxl')) 
                            for line in result.stdout.splitlines()))
            if not has_images:
                logger.info( f"âš ï¸ è·³è¿‡æ— å›¾ç‰‡çš„å‹ç¼©åŒ…: {file_path}")
                return []
            processed_archives = []
            if not params['ignore_processed_log']:
                if ProcessedLogHandler.has_processed_log(file_path):
                    logger.info( f"âš ï¸ æ–‡ä»¶å·²æœ‰å¤„ç†è®°å½•: {file_path}")
                    return processed_archives
                    
            logger.info( "å¼€å§‹å¤„ç†å‹ç¼©åŒ…å†…å®¹...")
            processed_archives.extend(ArchiveProcessor.process_archive_in_memory(file_path, params))
            
            if processed_archives:
                # æ›´æ–°é‡å¤ä¿¡æ¯é¢æ¿
                info = processed_archives[-1]
                logger.info( 
                    f"å¤„ç†ç»“æœ:\n"
                    f"- å“ˆå¸Œé‡å¤: {info.get('hash_duplicates_removed', 0)} å¼ \n"
                    f"- æ™®é€šé‡å¤: {info.get('normal_duplicates_removed', 0)} å¼ \n"
                    f"- å°å›¾: {info.get('small_images_removed', 0)} å¼ \n"
                    f"- ç™½å›¾: {info.get('white_images_removed', 0)} å¼ \n"
                    f"- å‡å°‘å¤§å°: {info['size_reduction_mb']:.2f} MB"
                )
                
                # æ›´æ–°è¿›åº¦é¢æ¿
                logger.info( f"âœ… æˆåŠŸå¤„ç†: {os.path.basename(file_path)}")
                
                    
                if params['add_processed_log_enabled']:
                    processed_info = {
                        'hash_duplicates_removed': info.get('hash_duplicates_removed', 0),
                        'normal_duplicates_removed': info.get('normal_duplicates_removed', 0),
                        'small_images_removed': info.get('small_images_removed', 0),
                        'white_images_removed': info.get('white_images_removed', 0)
                    }
                    ProcessedLogHandler.add_processed_log(file_path, processed_info)
                    logger.info( "å·²æ·»åŠ å¤„ç†æ—¥å¿—")
            else:
                logger.info( f"âš ï¸ å‹ç¼©åŒ…å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰éœ€è¦å¤„ç†çš„å†…å®¹: {file_path}")
                
            backup_file_path = file_path + '.bak'
            if os.path.exists(backup_file_path):
                BackupHandler.handle_bak_file(backup_file_path, params)
                logger.info( "å·²å¤„ç†å¤‡ä»½æ–‡ä»¶")
                
            return processed_archives
            
        except UnicodeDecodeError as e:
            logger.info( f"âŒ å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºç°ç¼–ç é”™è¯¯ {file_path}: {e}")
            return []
        except Exception as e:
            logger.info( f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {file_path}\n{str(e)}")
            return []

    @staticmethod
    def get_max_workers():
        """è·å–æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°"""
        return max_workers  # è¿”å›å…¨å±€é…ç½®çš„max_workerså€¼

class StatisticsManager:
    """Statistics"""
    processed_count = 0
    total_count = 0
    hash_duplicates_count = 0  # å“ˆå¸Œæ–‡ä»¶å»é‡çš„æ•°é‡
    normal_duplicates_count = 0  # æ™®é€šå»é‡çš„æ•°é‡
    small_images_count = 0  # å°å›¾æ•°é‡
    white_images_count = 0  # ç™½å›¾æ•°é‡

    @staticmethod
    def update_progress():
        """æ›´æ–°è¿›åº¦æ˜¾ç¤º"""
        if StatisticsManager.total_count > 0:
            percentage = (StatisticsManager.processed_count / StatisticsManager.total_count) * 100
            # æ›´æ–°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
            stats_str = (
                f"å¤„ç†è¿›åº¦: {StatisticsManager.processed_count}/{StatisticsManager.total_count}\n"
                f"å“ˆå¸Œå»é‡: {StatisticsManager.hash_duplicates_count} å¼ \n"
                f"æ™®é€šå»é‡: {StatisticsManager.normal_duplicates_count} å¼ \n"
                f"å°å›¾: {StatisticsManager.small_images_count} å¼ \n"
                f"ç™½å›¾: {StatisticsManager.white_images_count} å¼ "
            )
            logger.info(f"[#cur_stats]{stats_str}")
            
            # ä½¿ç”¨è¿›åº¦æ¡é¢æ¿æ˜¾ç¤ºæ€»ä½“è¿›åº¦
            logger.info(f"[@cur_stats] æ€»ä½“è¿›åº¦ {percentage:.1f}%")

    @staticmethod
    def increment():
        """å¢åŠ å¤„ç†è®¡æ•°å¹¶æ›´æ–°è¿›åº¦"""
        StatisticsManager.processed_count += 1
        StatisticsManager.update_progress()

    @staticmethod
    def set_total(total):
        """è®¾ç½®æ€»æ•°å¹¶é‡ç½®æ‰€æœ‰è®¡æ•°"""
        StatisticsManager.total_count = total
        StatisticsManager.processed_count = 0
        StatisticsManager.hash_duplicates_count = 0
        StatisticsManager.normal_duplicates_count = 0
        StatisticsManager.small_images_count = 0
        StatisticsManager.white_images_count = 0
        StatisticsManager.update_progress()


    @staticmethod
    def update_counts(hash_duplicates=0, normal_duplicates=0, small_images=0, white_images=0):
        """æ›´æ–°å„ç±»å‹æ–‡ä»¶çš„è®¡æ•°"""
        StatisticsManager.hash_duplicates_count += hash_duplicates
        StatisticsManager.normal_duplicates_count += normal_duplicates
        StatisticsManager.small_images_count += small_images
        StatisticsManager.white_images_count += white_images
        StatisticsManager.update_progress()



# é…ç½®å‚æ•°
verbose_logging = True
use_direct_path_mode = True
filter_height_enabled = True
remove_grayscale = True
add_processed_log_enabled = True
ignore_processed_log = True
min_size = 631
# max_workers = min(4, os.cpu_count() or 4)
max_workers = 4
backup_removed_files_enabled = True
use_clipboard = False


# åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨

class HashFileHandler:
    """å¤„ç†å“ˆå¸Œæ–‡ä»¶çš„ç±»"""
    
    # ç”¨äºä¸´æ—¶å­˜å‚¨ç›¸ä¼¼æ€§è®°å½•çš„ç±»å˜é‡
    similarity_records = []
    
    @staticmethod
    def clear_similarity_records():
        """æ¸…ç©ºç›¸ä¼¼æ€§è®°å½•"""
        HashFileHandler.similarity_records = []
    
    @staticmethod
    def record_similarity(file_path, similar_uri, hamming_distance):
        """è®°å½•ç›¸ä¼¼æ–‡ä»¶çš„å¯¹åº”å…³ç³»åˆ°å†…å­˜ä¸­
        
        Args:
            file_path: å½“å‰å¤„ç†çš„æ–‡ä»¶è·¯å¾„
            similar_uri: ç›¸ä¼¼æ–‡ä»¶çš„URI
            hamming_distance: æ±‰æ˜è·ç¦»
        """
        try:
            # æ·»åŠ ç›¸ä¼¼æ€§ä¿¡æ¯
            similarity_info = {
                'file_path': file_path,
                'similar_uri': similar_uri,
                'hamming_distance': hamming_distance,
                'timestamp': datetime.now().isoformat()
            }
            
            HashFileHandler.similarity_records.append(similarity_info)
            logger.info( f"[#update_log]å·²è®°å½•ç›¸ä¼¼æ€§: {file_path} -> {similar_uri} (è·ç¦»: {hamming_distance})")
            # æ·»åŠ å“ˆå¸Œæ“ä½œé¢æ¿æ ‡è¯†
            
        except Exception as e:
            logger.info(f"[#update_log]- è®°å½•ç›¸ä¼¼æ€§æ—¶å‡ºé”™: {str(e)}")

    @staticmethod
    def get_similarity_records():
        """è·å–æ‰€æœ‰ç›¸ä¼¼æ€§è®°å½•"""
        return HashFileHandler.similarity_records

    @staticmethod
    def load_hash_file(hash_file_path):
        """åŠ è½½å“ˆå¸Œæ–‡ä»¶å¹¶å¯¹å“ˆå¸Œå€¼è¿›è¡Œé¢„å¤„ç†
        
        Args:
            hash_file_path: å“ˆå¸Œæ–‡ä»¶è·¯å¾„
            
        Returns:
            tuple: (å“ˆå¸Œå€¼åˆ—è¡¨, å“ˆå¸Œå€¼åˆ°URIçš„æ˜ å°„å­—å…¸)
        """
        try:
            if not hash_file_path:
                logger.info("[#file_ops]æœªæä¾›å“ˆå¸Œæ–‡ä»¶è·¯å¾„")
                return [], {}
                
            logger.info(f"[#file_ops]å°è¯•åŠ è½½å“ˆå¸Œæ–‡ä»¶: {hash_file_path}")
            
            if not os.path.exists(hash_file_path):
                logger.info(f"[#file_ops]å“ˆå¸Œæ–‡ä»¶ä¸å­˜åœ¨: {hash_file_path}")
                return [], {}
                
            with open(hash_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            logger.info(f"[#update_log]âœ… æˆåŠŸè¯»å–å“ˆå¸Œæ–‡ä»¶: {hash_file_path}")
            
            # æå–æ‰€æœ‰å“ˆå¸Œå€¼å¹¶å»ºç«‹æ˜ å°„å…³ç³»
            hash_to_uri = {}
            hash_values = []
            
            # é¦–å…ˆå°è¯•æ–°æ ¼å¼ (hasheså­—æ®µ)
            hashes_data = data.get('hashes', {})
            if not hashes_data:
                # å¦‚æœæ²¡æœ‰hasheså­—æ®µ,å°è¯•æ—§æ ¼å¼ (resultså­—æ®µ)
                hashes_data = data.get('results', {})
            
            total_count = len(hashes_data)
            loaded_count = 0
            
            for uri, info in hashes_data.items():
                # å¤„ç†ä¸åŒæ ¼å¼çš„å“ˆå¸Œå€¼
                if isinstance(info, dict):
                    # æ–°æ ¼å¼: {'hash': 'xxx'} æˆ–æ—§æ ¼å¼: {'hash_value': 'xxx'}
                    hash_str = str(info.get('hash') or info.get('hash_value', ''))
                elif isinstance(info, str):
                    # ç›´æ¥æ˜¯å“ˆå¸Œå­—ç¬¦ä¸²
                    hash_str = info
                else:
                    continue
                    
                # éªŒè¯å“ˆå¸Œå€¼
                if not hash_str:
                    continue
                    
                # ç»Ÿä¸€ä½¿ç”¨å°å†™
                hash_str = hash_str.lower()
                hash_values.append(hash_str)
                hash_to_uri[hash_str] = uri
                
                loaded_count += 1
                if loaded_count % 1000 == 0:  # æ¯1000ä¸ªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    percentage = (loaded_count / total_count) * 100
                    logger.info(f"[@hash_calc] åŠ è½½å“ˆå¸Œæ–‡ä»¶ {percentage:.1f}%")
            
            # åˆå¹¶æ—¥å¿—è¾“å‡º
            logger.info(f"[#hash_calc]åŠ è½½å“ˆå¸Œæ–‡ä»¶å®Œæˆ")
            logger.info(f"[#update_log]âœ… å“ˆå¸Œæ–‡ä»¶åŠ è½½å®Œæˆ - æ€»æ•°: {len(hash_values)}ä¸ª")
            logger.info(f"[#hash_calc]å“ˆå¸Œå€¼æ•°é‡: {len(hash_values)}")
            logger.info(f"[#hash_calc]URIæ˜ å°„æ•°é‡: {len(hash_to_uri)}")
            
            return hash_values, hash_to_uri
                
        except Exception as e:
            logger.error(f"[#hash_calc]âŒ åŠ è½½å“ˆå¸Œæ–‡ä»¶å¤±è´¥: {str(e)}")
            return [], {}

    @staticmethod
    def find_similar_hash(target_hash, ref_hashes, hash_to_uri, hamming_distance_threshold):
        """éå†æ‰€æœ‰å“ˆå¸Œå€¼è¿›è¡Œå®Œæ•´æ¯”è¾ƒ
        
        Args:
            target_hash: ç›®æ ‡å“ˆå¸Œå€¼ï¼ˆå¯ä»¥æ˜¯å­—å…¸æ ¼å¼æˆ–å­—ç¬¦ä¸²æ ¼å¼ï¼‰
            ref_hashes: å‚è€ƒå“ˆå¸Œå€¼åˆ—è¡¨
            hash_to_uri: å“ˆå¸Œå€¼åˆ°URIçš„æ˜ å°„å­—å…¸
            hamming_distance_threshold: æ±‰æ˜è·ç¦»é˜ˆå€¼
            
        Returns:
            tuple: (æ˜¯å¦æ‰¾åˆ°ç›¸ä¼¼å€¼, ç›¸ä¼¼å“ˆå¸Œå€¼, å¯¹åº”çš„URI)
        """
        try:
            # å¦‚æœæ²¡æœ‰å¤–éƒ¨å“ˆå¸Œæ–‡ä»¶ï¼Œç›´æ¥è¿”å›æœªæ‰¾åˆ°
            if not ref_hashes:
                return False, None, None

            # ç»Ÿä¸€è·å–å“ˆå¸Œå€¼å­—ç¬¦ä¸²
            def get_hash_str(hash_obj):
                if isinstance(hash_obj, dict):
                    return str(hash_obj.get('hash') or hash_obj.get('phash') or hash_obj.get('hash_value', '')).lower()
                return str(hash_obj).lower()
                
            # æå–ç›®æ ‡å“ˆå¸Œå€¼
            target_hash_str = get_hash_str(target_hash)
            target_url = target_hash.get('url', '') if isinstance(target_hash, dict) else ''
            
            # è®°å½•æ¯”è¾ƒè¿‡ç¨‹
            logger.debug(f"[#hash_calc]å¼€å§‹æŸ¥æ‰¾ç›¸ä¼¼å“ˆå¸Œå€¼: {target_hash_str}" + (f" (æ¥è‡ª: {target_url})" if target_url else ""))
            
            compared_count = 0
            max_diff = 2 ** hamming_distance_threshold  # æœ€å¤§å¯èƒ½çš„å·®å¼‚å€¼
            
            # éå†æ‰€æœ‰å“ˆå¸Œå€¼è¿›è¡Œæ¯”è¾ƒ
            for current_hash in ref_hashes:
                # æå–å½“å‰å“ˆå¸Œå€¼
                current_hash_str = get_hash_str(current_hash)
                current_url = current_hash.get('url', '') if isinstance(current_hash, dict) else ''
                
                # è®¡ç®—æ±‰æ˜è·ç¦»
                hamming_distance = ImageHashCalculator.calculate_hamming_distance(target_hash_str, current_hash_str)
                
                compared_count += 1
                
                if hamming_distance <= hamming_distance_threshold:
                    # æ‰¾åˆ°ç›¸ä¼¼çš„å“ˆå¸Œå€¼
                    result_msg = f"[#hash_calc]æ‰¾åˆ°ç›¸ä¼¼å“ˆå¸Œå€¼: {current_hash_str}"
                    if current_url:
                        result_msg += f" (æ¥è‡ª: {current_url})"
                    result_msg += f", æ±‰æ˜è·ç¦»: {hamming_distance}, URI: {hash_to_uri[current_hash_str]}"
                    logger.info(result_msg)
                    return True, current_hash_str, hash_to_uri[current_hash_str]
            
            return False, None, None
            
        except Exception as e:
            logger.info(f"[#hash_calc]æŸ¥æ‰¾ç›¸ä¼¼å“ˆå¸Œå€¼æ—¶å‡ºé”™: {str(e)}")
            return False, None, None

class InputHandler:
    """è¾“å…¥å¤„ç†ç±»"""
    @staticmethod
    def parse_arguments(args=None):
        parser = argparse.ArgumentParser(description='å›¾ç‰‡å‹ç¼©åŒ…å»é‡å·¥å…·')
        # æ·»åŠ æ’é™¤è·¯å¾„å‚æ•°
        parser.add_argument('--exclude-paths', '-ep',
                          nargs='*',
                          default=[],
                          help='è¦æ’é™¤çš„è·¯å¾„å…³é”®è¯åˆ—è¡¨')
        feature_group = parser.add_argument_group('åŠŸèƒ½å¼€å…³')
        feature_group.add_argument('--remove-small', '-rs', action='store_true', help='å¯ç”¨å°å›¾è¿‡æ»¤')
        feature_group.add_argument('--remove-grayscale', '-rg', action='store_true', help='å¯ç”¨é»‘ç™½å›¾è¿‡æ»¤')
        feature_group.add_argument('--remove-duplicates', '-rd', action='store_true', help='å¯ç”¨é‡å¤å›¾ç‰‡è¿‡æ»¤')
        feature_group.add_argument('--merge-archives', '-ma', action='store_true', help='åˆå¹¶åŒä¸€æ–‡ä»¶å¤¹ä¸‹çš„å¤šä¸ªå‹ç¼©åŒ…è¿›è¡Œå¤„ç†')
        feature_group.add_argument('--no-trash', '-nt', action='store_true', help='ä¸ä¿ç•™trashæ–‡ä»¶å¤¹ï¼Œç›´æ¥åˆ é™¤åˆ°å›æ”¶ç«™')
        feature_group.add_argument('--hash-file', '-hf', type=str, help='æŒ‡å®šå“ˆå¸Œæ–‡ä»¶è·¯å¾„,ç”¨äºè·¨å‹ç¼©åŒ…å»é‡')
        feature_group.add_argument('--self-redup', '-sr', action='store_true', help='å¯ç”¨è‡ªèº«å»é‡å¤(å½“ä½¿ç”¨å“ˆå¸Œæ–‡ä»¶æ—¶é»˜è®¤ä¸å¯ç”¨)')
        feature_group.add_argument('path', nargs='*', help='è¦å¤„ç†çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
        small_group = parser.add_argument_group('å°å›¾è¿‡æ»¤å‚æ•°')
        small_group.add_argument('--min-size', '-ms', type=int, default=631, help='æœ€å°å›¾ç‰‡å°ºå¯¸ï¼ˆå®½åº¦å’Œé«˜åº¦ï¼‰ï¼Œé»˜è®¤ä¸º631')
        duplicate_group = parser.add_argument_group('é‡å¤å›¾ç‰‡è¿‡æ»¤å‚æ•°')
        duplicate_group.add_argument('--hamming_distance', '-hd', type=int, default=0, help='å†…éƒ¨å»é‡çš„æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼Œæ•°å€¼è¶Šå¤§åˆ¤å®šè¶Šå®½æ¾ï¼Œé»˜è®¤ä¸º2')
        duplicate_group.add_argument('--ref_hamming_distance', '-rhd', type=int, default=12, help='ä¸å¤–éƒ¨å‚è€ƒæ–‡ä»¶æ¯”è¾ƒçš„æ±‰æ˜è·ç¦»é˜ˆå€¼ï¼Œé»˜è®¤ä¸º12')
        
        parser.add_argument('--clipboard', '-c', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
        parser.add_argument('--bak-mode', '-bm', choices=['recycle', 'delete', 'keep'], default='keep', help='bakæ–‡ä»¶å¤„ç†æ¨¡å¼ï¼šrecycle=ç§»åˆ°å›æ”¶ç«™ï¼ˆé»˜è®¤ï¼‰ï¼Œdelete=ç›´æ¥åˆ é™¤ï¼Œkeep=ä¿ç•™')
        parser.add_argument('--max-workers', '-mw', type=int, default=4, help='æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4')

        return parser.parse_args(args)  # æ·»åŠ å‚æ•°ä¼ é€’


    @staticmethod
    def prepare_params(args):
        """
        ç»Ÿä¸€å‡†å¤‡å‚æ•°å­—å…¸
        
        Args:
            args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
            
        Returns:
            dict: åŒ…å«æ‰€æœ‰å¤„ç†å‚æ•°çš„å­—å…¸
        """
        return {
            'min_size': args.min_size,
            'hamming_distance': args.hamming_distance,  # è¿™é‡Œä½¿ç”¨è¿å­—ç¬¦å½¢å¼çš„å‚æ•°å
            'ref_hamming_distance': args.ref_hamming_distance,  # è¿™é‡Œä½¿ç”¨è¿å­—ç¬¦å½¢å¼çš„å‚æ•°å
            'filter_height_enabled': args.remove_small,
            'remove_grayscale': args.remove_grayscale,
            'ignore_processed_log': ignore_processed_log,
            'add_processed_log_enabled': add_processed_log_enabled,
            'max_workers': args.max_workers,
            'bak_mode': args.bak_mode,
            'remove_duplicates': args.remove_duplicates,
            'hash_file': args.hash_file,
            'self_redup': args.self_redup,
            'exclude_paths': args.exclude_paths if args.exclude_paths else []
        }

    @staticmethod
    def get_input_paths(args):
        """è·å–è¾“å…¥è·¯å¾„"""
        directories = []
        
        # é¦–å…ˆæ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°ä¸­çš„è·¯å¾„
        if args.path:
            directories.extend(args.path)
            
        # å¦‚æœæ²¡æœ‰è·¯å¾„ä¸”å¯ç”¨äº†å‰ªè´´æ¿ï¼Œåˆ™ä»å‰ªè´´æ¿è¯»å–
        if not directories and args.clipboard:
            directories = InputHandler.get_paths_from_clipboard()
            
        # å¦‚æœä»ç„¶æ²¡æœ‰è·¯å¾„ï¼Œåˆ™ä½¿ç”¨Rich Loggerçš„è¾“å…¥åŠŸèƒ½
        if not directories:
            try:
                print("è¯·è¾“å…¥è¦å¤„ç†çš„æ–‡ä»¶å¤¹æˆ–å‹ç¼©åŒ…è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
                while True:
                    line = input().strip()
                    if not line:
                        break
                    path = line.strip().strip('"').strip("'")
                    if os.path.exists(path):
                        directories.append(path)
                        # print(f"âœ… å·²æ·»åŠ æœ‰æ•ˆè·¯å¾„: {path}")
                    else:
                        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
                
            except Exception as e:
                print(f"è·å–è·¯å¾„å¤±è´¥: {e}")
                
        return directories

    @staticmethod
    def get_paths_from_clipboard():
        """ä»å‰ªè´´æ¿è¯»å–å¤šè¡Œè·¯å¾„"""
        try:
            clipboard_content = pyperclip.paste()
            if not clipboard_content:
                return []
            paths = [path.strip().strip('"') for path in clipboard_content.splitlines() if path.strip()]
            valid_paths = [path for path in paths if os.path.exists(path)]
            if valid_paths:
                logger.info( f'ä»å‰ªè´´æ¿è¯»å–åˆ° {len(valid_paths)} ä¸ªæœ‰æ•ˆè·¯å¾„')
            else:
                logger.info( 'å‰ªè´´æ¿ä¸­æ²¡æœ‰æœ‰æ•ˆè·¯å¾„')
            return valid_paths
        except ImportError:
            logger.info( 'æœªå®‰è£… pyperclip æ¨¡å—ï¼Œæ— æ³•è¯»å–å‰ªè´´æ¿')
            return []
        except Exception as e:
            logger.info( f'è¯»å–å‰ªè´´æ¿æ—¶å‡ºé”™: {e}')
            return []

    @staticmethod
    def validate_args(args):
        """éªŒè¯å‚æ•°æ˜¯å¦æœ‰æ•ˆ"""
        if not any([args.remove_small, args.remove_grayscale, args.remove_duplicates]):
            logger.info( 'è­¦å‘Š: æœªå¯ç”¨ä»»ä½•è¿‡æ»¤åŠŸèƒ½ï¼Œå°†ä¸ä¼šå¯¹å›¾ç‰‡è¿›è¡Œå¤„ç†')
        return True

class Application:
    """
    ç±»æè¿°
    """
    def main(self):
        """ä¸»å‡½æ•°"""
        try:
            # æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„
            sys.path.append(os.path.dirname(os.path.dirname(__file__)))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
            if len(sys.argv) > 1:
                # å‘½ä»¤è¡Œæ¨¡å¼å¤„ç†
                args = InputHandler.parse_arguments()
                if not InputHandler.validate_args(args):
                    sys.exit(1)
                self._process_with_args(args)
                return

            # TUIæ¨¡å¼å¤„ç†
            if not HAS_TUI:
                print("æ— æ³•å¯¼å…¥TUIé…ç½®æ¨¡å—,å°†ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼")
                args = InputHandler.parse_arguments()
                if not InputHandler.validate_args(args):
                    sys.exit(1)
                self._process_with_args(args)
                return

            # åˆ›å»ºé…ç½®é€‰é¡¹å’Œé¢„è®¾
            checkbox_options, input_options, preset_configs = self._create_ui_config()

            # åˆ›å»ºé…ç½®ç•Œé¢
            app = create_config_app(
                program=os.path.abspath(__file__),
                checkbox_options=checkbox_options,
                input_options=input_options,
                title="å›¾ç‰‡å‹ç¼©åŒ…å»é‡å·¥å…·",
                preset_configs=preset_configs,
                on_run=False  # æ–°å¢å›è°ƒå‡½æ•°
            )
            
            # è¿è¡Œé…ç½®ç•Œé¢
            app.run()

        except Exception as e:
            logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(f"é”™è¯¯ä¿¡æ¯: {e}")
            sys.exit(1)

    def _process_with_args(self, args):
        """ç»Ÿä¸€å¤„ç†å‚æ•°æ‰§è¡Œ"""
        directories = InputHandler.get_input_paths(args)
        if not directories:
            print('æœªæä¾›ä»»ä½•è¾“å…¥è·¯å¾„')
            return
        # initialize_textual_logger()
        ProcessManager.print_config(args, ProcessManager.get_max_workers())
        if args.remove_duplicates:
            
            global global_hashes
        if args.merge_archives:
            ProcessManager.process_merged_archives(directories, args)
        else:
            ProcessManager.process_normal_archives(directories, args)

    def _handle_tui_run(self, params: dict):
        """TUIæ¨¡å¼å›è°ƒå¤„ç†"""
        # è½¬æ¢å‚æ•°ä¸ºå‘½ä»¤è¡Œæ ¼å¼
        args_list = []
        
        # æ·»åŠ é€‰é¡¹å‚æ•°
        for arg, enabled in params['options'].items():
            if enabled:
                args_list.append(arg)
        
        # æ·»åŠ è¾“å…¥å‚æ•°
        for arg, value in params['inputs'].items():
            if value:  # åªæ·»åŠ æœ‰å€¼çš„å‚æ•°
                args_list.extend([arg, value])
        
        # æ·»åŠ è·¯å¾„å‚æ•°
        if params.get('paths'):
            args_list.extend(params['paths'])
        
        # è§£æå‚æ•°
        args = InputHandler.parse_arguments(args_list)
        if not InputHandler.validate_args(args):
            sys.exit(1)
        
        # ç»Ÿä¸€æ‰§è¡Œå¤„ç†
        self._process_with_args(args)

    def _create_ui_config(self):
        """åˆ›å»ºTUIé…ç½®é€‰é¡¹å’Œé¢„è®¾"""
        checkbox_options = [
            ("å°å›¾è¿‡æ»¤", "remove_small", "--remove-small"),
            ("é»‘ç™½å›¾è¿‡æ»¤", "remove_grayscale", "--remove-grayscale"), 
            ("é‡å¤å›¾ç‰‡è¿‡æ»¤", "remove_duplicates", "--remove-duplicates"),
            ("åˆå¹¶å‹ç¼©åŒ…å¤„ç†", "merge_archives", "--merge-archives"),
            ("è‡ªèº«å»é‡å¤", "self_redup", "--self-redup"),
        ]

        input_options = [
            ("æœ€å°å›¾ç‰‡å°ºå¯¸", "min_size", "--min-size", "631", "è¾“å…¥æ•°å­—(é»˜è®¤631)"),
            ("æ±‰æ˜è·ç¦»", "hamming_distance", "--hamming_distance", "12", "è¾“å…¥æ±‰æ˜è·ç¦»çš„æ•°å­—"),
            ("å†…éƒ¨å»é‡çš„æ±‰æ˜è·ç¦»é˜ˆå€¼", "ref_hamming_distance", "--ref-hamming_distance", "12", "è¾“å…¥å†…éƒ¨å»é‡çš„æ±‰æ˜è·ç¦»é˜ˆå€¼"),
            ("å“ˆå¸Œæ–‡ä»¶è·¯å¾„", "hash_file", "--hash-file", "", "è¾“å…¥å“ˆå¸Œæ–‡ä»¶è·¯å¾„(å¯é€‰)"),
        ]

        preset_configs = {
            "å»å°å›¾æ¨¡å¼": {
                "description": "ä»…å»é™¤å°å°ºå¯¸å›¾ç‰‡",
                "checkbox_options": ["remove_small",  "clipboard"],
                "input_values": {
                    "min_size": "631"
                }
            },
            "å»é‡å¤æ¨¡å¼": {
                "description": "ä»…å»é™¤é‡å¤å›¾ç‰‡",
                "checkbox_options": ["remove_duplicates", "clipboard"],
                "input_values": {
                    "hamming_distance": "12"
                }
            },
            "å»é»‘ç™½æ¨¡å¼": {
                "description": "ä»…å»é™¤é»‘ç™½/ç™½å›¾",
                "checkbox_options": ["remove_grayscale", "clipboard"],
            },
            "åˆå¹¶å¤„ç†æ¨¡å¼": {
                "description": "åˆå¹¶å‹ç¼©åŒ…å¤„ç†(å»é‡+å»å°å›¾+å»é»‘ç™½)",
                "checkbox_options": ["merge_archives", "remove_small", "remove_duplicates", "remove_grayscale", "clipboard"],
                "input_values": {
                    "min_size": "631",
                    "hamming_distance": "12"

                }
            }
        }

        return checkbox_options, input_options, preset_configs


class DebuggerHandler:
    LAST_CONFIG_FILE = "last_debug_config.json"

    @staticmethod
    def save_last_config(mode_choice, final_args):
        """ä¿å­˜æœ€åä¸€æ¬¡ä½¿ç”¨çš„é…ç½®"""
        try:
            config = {
                "mode": mode_choice,
                "args": final_args
            }
            with open(DebuggerHandler.LAST_CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.info(f"[#update_log]ä¿å­˜é…ç½®å¤±è´¥: {e}")

    @staticmethod
    def load_last_config():
        """åŠ è½½ä¸Šæ¬¡ä½¿ç”¨çš„é…ç½®"""
        try:
            if os.path.exists(DebuggerHandler.LAST_CONFIG_FILE):
                with open(DebuggerHandler.LAST_CONFIG_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.info(f"[#update_log]åŠ è½½é…ç½®å¤±è´¥: {e}")
        return None

    @staticmethod
    def get_debugger_options():
        """äº¤äº’å¼è°ƒè¯•æ¨¡å¼èœå•"""
        # åŸºç¡€æ¨¡å¼é€‰é¡¹
        base_modes = {
            "1": {"name": "å»å°å›¾æ¨¡å¼", "base_args": ["-rs"], "default_params": {"ms": "631"}},
            "2": {"name": "å»é‡å¤æ¨¡å¼", "base_args": ["-rd"], "default_params": {"hd": "12", "rhd": "12"}},
            "3": {"name": "å»é»‘ç™½æ¨¡å¼", "base_args": ["-rg"]},
            "4": {"name": "åˆå¹¶å¤„ç†æ¨¡å¼", "base_args": ["-ma", "-rs", "-rd", "-rg"], 
                  "default_params": {"ms": "631", "hd": "12", "rhd": "12"}}
        }
        
        # å¯é…ç½®å‚æ•°é€‰é¡¹
        param_options = {
            "ms": {"name": "æœ€å°å°ºå¯¸", "arg": "-ms", "default": "631", "type": int},
            "hd": {"name": "æ±‰æ˜è·ç¦»", "arg": "-hd", "default": "12", "type": int},
            "rhd": {"name": "å‚è€ƒæ±‰æ˜è·ç¦»", "arg": "-rhd", "default": "12", "type": int},
            "bm": {"name": "å¤‡ä»½æ¨¡å¼", "arg": "-bm", "default": "keep", "choices": ["keep", "recycle", "delete"]},
            "c": {"name": "ä»å‰ªè´´æ¿è¯»å–", "arg": "-c", "is_flag": True},
            "mw": {"name": "æœ€å¤§çº¿ç¨‹æ•°", "arg": "-mw", "default": "4", "type": int}
        }

        # åŠ è½½ä¸Šæ¬¡é…ç½®
        last_config = DebuggerHandler.load_last_config()
        
        while True:
            print("\n=== è°ƒè¯•æ¨¡å¼é€‰é¡¹ ===")
            print("\nåŸºç¡€æ¨¡å¼:")
            for key, mode in base_modes.items():
                print(f"{key}. {mode['name']}")
            
            if last_config:
                print("\nä¸Šæ¬¡é…ç½®:")
                print(f"æ¨¡å¼: {base_modes[last_config['mode']]['name']}")
                print("å‚æ•°:", " ".join(last_config['args']))
                print("\né€‰é¡¹:")
                print("L. ä½¿ç”¨ä¸Šæ¬¡é…ç½®")
                print("N. ä½¿ç”¨æ–°é…ç½®")
                choice = input("\nè¯·é€‰æ‹© (L/N æˆ–ç›´æ¥é€‰æ‹©æ¨¡å¼ 1-4): ").strip().upper()
                
                if choice == 'L':
                    return last_config['args']
                elif choice == 'N':
                    pass  # ç»§ç»­ä½¿ç”¨æ–°é…ç½®
                elif not choice:
                    return []
                elif choice in base_modes:
                    mode_choice = choice
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡è¯•")
                    continue
            else:
                # è·å–åŸºç¡€æ¨¡å¼é€‰æ‹©
                mode_choice = input("\nè¯·é€‰æ‹©åŸºç¡€æ¨¡å¼(1-4): ").strip()
                if not mode_choice:
                    return []
                
                if mode_choice not in base_modes:
                    print("âŒ æ— æ•ˆçš„æ¨¡å¼é€‰æ‹©ï¼Œè¯·é‡è¯•")
                    continue
            
            selected_mode = base_modes[mode_choice]
            final_args = selected_mode["base_args"].copy()
            
            # æ·»åŠ é»˜è®¤å‚æ•°
            if "default_params" in selected_mode:
                for param_key, default_value in selected_mode["default_params"].items():
                    if param_key in param_options:
                        param = param_options[param_key]
                        final_args.append(f"{param['arg']}={default_value}")
            
            # æ˜¾ç¤ºå½“å‰é…ç½®
            print("\nå½“å‰é…ç½®:")
            for arg in final_args:
                print(f"  {arg}")
            
            # è¯¢é—®æ˜¯å¦éœ€è¦ä¿®æ”¹å‚æ•°
            while True:
                print("\nå¯é€‰æ“ä½œ:")
                print("1. ä¿®æ”¹å‚æ•°")
                print("2. æ·»åŠ å‚æ•°")
                print("3. å¼€å§‹æ‰§è¡Œ")
                print("4. é‡æ–°é€‰æ‹©æ¨¡å¼")
                print("0. é€€å‡ºç¨‹åº")
                
                op_choice = input("\nè¯·é€‰æ‹©æ“ä½œ(0-4): ").strip()
                
                if op_choice == "0":
                    return []
                elif op_choice == "1":
                    # æ˜¾ç¤ºå½“å‰æ‰€æœ‰å‚æ•°
                    print("\nå½“å‰å‚æ•°:")
                    for i, arg in enumerate(final_args, 1):
                        print(f"{i}. {arg}")
                    param_idx = input("è¯·é€‰æ‹©è¦ä¿®æ”¹çš„å‚æ•°åºå·: ").strip()
                    try:
                        idx = int(param_idx) - 1
                        if 0 <= idx < len(final_args):
                            new_value = input(f"è¯·è¾“å…¥æ–°çš„å€¼: ").strip()
                            if '=' in final_args[idx]:
                                arg_name = final_args[idx].split('=')[0]
                                final_args[idx] = f"{arg_name}={new_value}"
                            else:
                                final_args[idx] = new_value
                    except ValueError:
                        print("âŒ æ— æ•ˆçš„è¾“å…¥")
                elif op_choice == "2":
                    # æ˜¾ç¤ºå¯æ·»åŠ çš„å‚æ•°
                    print("\nå¯æ·»åŠ çš„å‚æ•°:")
                    for key, param in param_options.items():
                        if param.get("is_flag"):
                            print(f"  {key}. {param['name']} (å¼€å…³å‚æ•°)")
                        elif "choices" in param:
                            print(f"  {key}. {param['name']} (å¯é€‰å€¼: {'/'.join(param['choices'])})")
                        else:
                            print(f"  {key}. {param['name']}")
                    
                    param_key = input("è¯·è¾“å…¥è¦æ·»åŠ çš„å‚æ•°ä»£å·: ").strip()
                    if param_key in param_options:
                        param = param_options[param_key]
                        if param.get("is_flag"):
                            final_args.append(param["arg"])
                        else:
                            value = input(f"è¯·è¾“å…¥{param['name']}çš„å€¼: ").strip()
                            if "choices" in param and value not in param["choices"]:
                                print(f"âŒ æ— æ•ˆçš„å€¼ï¼Œå¯é€‰å€¼: {'/'.join(param['choices'])}")
                                continue
                            if "type" in param:
                                try:
                                    value = param["type"](value)
                                except ValueError:
                                    print("âŒ æ— æ•ˆçš„æ•°å€¼")
                                    continue
                            final_args.append(f"{param['arg']}={value}")
                elif op_choice == "3":
                    print("\næœ€ç»ˆå‚æ•°:", " ".join(final_args))
                    # ä¿å­˜å½“å‰é…ç½®
                    DebuggerHandler.save_last_config(mode_choice, final_args)
                    return final_args
                elif op_choice == "4":
                    break
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©")
            
        return []
    
if __name__ == '__main__':
    if USE_DEBUGGER:
        selected_options = DebuggerHandler.get_debugger_options()
        if selected_options:
            # ç§»é™¤å¤šä½™çš„--no-tuiå‚æ•°
            args = InputHandler.parse_arguments(selected_options)  # åˆ é™¤+ ['--no-tui']
            Application()._process_with_args(args)
        else:
            print("æœªé€‰æ‹©ä»»ä½•åŠŸèƒ½ï¼Œç¨‹åºé€€å‡ºã€‚")
            sys.exit(0)
    else:
        Application().main()
 