import os
import shutil
import zipfile
import subprocess
import json
from datetime import datetime
import concurrent.futures
import send2trash  # æ·»åŠ send2trashåº“ç”¨äºå°†æ–‡ä»¶ç§»åŠ¨åˆ°å›æ”¶ç«™
from nodes.record.logger_config import setup_logger
from nodes.tui.textual_logger import TextualLoggerManager
from nodes.error.error_handler import handle_file_operation

# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¸ƒå±€é…ç½®
TEXTUAL_LAYOUT = {
    "current_stats": {
        "ratio": 2,
        "title": "ğŸ“Š æ€»ä½“ç»Ÿè®¡",
        "style": "lightyellow"
    },
    "current_progress": {
        "ratio": 4,
        "title": "ğŸ”„ æ–‡ä»¶å¤„ç†",
        "style": "lightcyan"
    },
    "process_log": {
        "ratio": 1,
        "title": "ğŸ“ å¤„ç†æ—¥å¿—",
        "style": "lightmagenta"
    },
    "update_log": {
        "ratio": 1,
        "title": "â„¹ï¸ çŠ¶æ€æ›´æ–°",
        "style": "lightblue"
    }
}

config = {
    'script_name': 'upscale_bus',
    "console_enabled": False,
}
logger, config_info = setup_logger(config)
TextualLoggerManager.set_layout(TEXTUAL_LAYOUT,log_file=config_info['log_file'])


def remove_empty_directories(directory):
    """åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ç©ºæ–‡ä»¶å¤¹"""
    removed_count = 0
    for root, dirs, _ in os.walk(directory, topdown=False):
        for dir_name in dirs:
            dir_path = os.path.join(root, dir_name)
            try:
                if not os.listdir(dir_path):  # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©º
                    os.rmdir(dir_path)
                    removed_count += 1
                    logger.info(f"[#process_log]å·²åˆ é™¤ç©ºæ–‡ä»¶å¤¹: {dir_path}")
            except Exception as e:
                logger.info(f"[#process_log]åˆ é™¤ç©ºæ–‡ä»¶å¤¹å¤±è´¥ {dir_path}: {e}")
    return removed_count

def remove_temp_files(directory):
    """åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ .tdel å’Œ .bak æ–‡ä»¶"""
    removed_count = 0
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(('.tdel', '.bak')):
                file_path = os.path.join(root, file)
                try:
                    os.remove(file_path)
                    removed_count += 1
                    logger.info(f"[#process_log]å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
                except Exception as e:
                    logger.info(f"[#process_log]åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    return removed_count

def count_files_in_zip(zip_path):
    """ç»Ÿè®¡zipæ–‡ä»¶ä¸­çš„æ–‡ä»¶æ•°é‡ï¼Œå¿½ç•¥ç‰¹å®šç±»å‹çš„æ–‡ä»¶"""
    ignore_extensions = ('.md', '.yaml', '.yml', '.txt', '.json', '.db', '.ini')
    try:
        with zipfile.ZipFile(zip_path) as zip_file:
            # è¿‡æ»¤æ‰è¦å¿½ç•¥çš„æ–‡ä»¶ç±»å‹å’Œç›®å½•æ¡ç›®
            valid_files = [name for name in zip_file.namelist() 
                         if not name.lower().endswith(ignore_extensions)
                         and not name.endswith('/')  # æ’é™¤ç›®å½•æ¡ç›®
                         and zip_file.getinfo(name).file_size > 0]  # æ’é™¤0å­—èŠ‚çš„ç›®å½•å ä½æ–‡ä»¶
            return len(valid_files)
    except Exception as e:
        logger.info(f"[#process_log]è¯»å–zipæ–‡ä»¶å¤±è´¥ {zip_path}: {str(e)}")
        return 0

@handle_file_operation(skip_errors=True)
def compare_and_copy_archives(source_dir, target_dir, is_move=False):
    # æ–°å¢ï¼šç»Ÿè®¡æ€»æ–‡ä»¶æ•°
    total_files = sum(
        len([f for f in files if f.endswith(('.cbz', '.zip'))])
        for root, _, files in os.walk(source_dir)
    )
    processed_files = 0
    
    # è®°å½•å¤„ç†ç»“æœ
    success_count = 0
    skip_count = 0
    error_files = []
    
    logger.info(f"[#current_stats]å¼€å§‹å¤„ç†ç›®å½•å¯¹ï¼š{source_dir} -> {target_dir}")
    
    # éå†æºç›®å½•
    for root, _, files in os.walk(source_dir):
        for file in files:
            if file.endswith(('.cbz', '.zip')):
                # æ„å»ºæºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶çš„è·¯å¾„
                source_path = os.path.join(root, file)
                rel_path = os.path.relpath(root, source_dir)
                
                # æ‰€æœ‰æ–‡ä»¶éƒ½ä½¿ç”¨zipæ‰©å±•å
                source_file = file.replace('.cbz', '.zip') if file.endswith('.cbz') else file
                target_file = source_file  # ç›®æ ‡æ–‡ä»¶ä¹Ÿç”¨zipæ‰©å±•å
                
                temp_source = os.path.join(root, source_file)
                target_path = os.path.join(target_dir, rel_path, target_file)
                
                # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(target_path), exist_ok=True)
                
                # å¦‚æœæ˜¯cbzï¼Œæ”¹åä¸ºzip
                if file.endswith('.cbz'):
                    try:
                        os.rename(source_path, temp_source)
                    except OSError as e:
                        logger.info(f"[#update_log]é‡å‘½åå¤±è´¥,è·³è¿‡å¤„ç†: {source_path} -> {temp_source}")
                        logger.info(f"[#process_log]é”™è¯¯ä¿¡æ¯: {str(e)}")
                        skip_count += 1
                        continue
                
                try:
                    # å¤„ç†å‰æ›´æ–°è¿›åº¦
                    processed_files += 1
                    progress = (processed_files / total_files) * 100 if total_files > 0 else 0
                    logger.info(f"[@current_progress]å¤„ç†ä¸­ ({processed_files}/{total_files}) {progress:.1f}%")
                    
                    # å¦‚æœç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥å¤åˆ¶æˆ–ç§»åŠ¨
                    if not os.path.exists(target_path):
                        try:
                            if is_move:
                                shutil.move(temp_source, target_path)
                                logger.info(f"[#process_log]ç§»åŠ¨æ–‡ä»¶: {file} -> {target_file}")
                            else:
                                shutil.copy2(temp_source, target_path)
                                logger.info(f"[#process_log]æ–°æ–‡ä»¶å¤åˆ¶: {file} -> {target_file}")
                            success_count += 1
                        except OSError as e:
                            logger.info(f"[#update_log]æ–‡ä»¶æ“ä½œå¤±è´¥,è·³è¿‡: {temp_source} -> {target_path}")
                            logger.info(f"[#process_log]é”™è¯¯ä¿¡æ¯: {str(e)}")
                            skip_count += 1
                            continue
                    else:
                        # æ¯”è¾ƒæ–‡ä»¶æ•°é‡ï¼ˆå¿½ç•¥ç‰¹å®šç±»å‹æ–‡ä»¶ï¼‰
                        source_count = count_files_in_zip(temp_source)
                        target_count = count_files_in_zip(target_path)
                            
                        if source_count == target_count:
                            if is_move:
                                try:
                                    send2trash.send2trash(target_path)  # å°†åŸæ–‡ä»¶ç§»åŠ¨åˆ°å›æ”¶ç«™
                                    logger.info(f"[#process_log]å·²å°†åŸæ–‡ä»¶ç§»åŠ¨åˆ°å›æ”¶ç«™: {target_path}")
                                    shutil.move(temp_source, target_path)
                                    logger.info(f"[#process_log]ç§»åŠ¨å¹¶è¦†ç›–: {file} -> {target_file}")
                                except Exception as e:
                                    logger.info(f"[#update_log]ç§»åŠ¨åˆ°å›æ”¶ç«™å¤±è´¥: {str(e)}")
                                    continue
                            else:
                                try:
                                    send2trash.send2trash(target_path)  # å°†åŸæ–‡ä»¶ç§»åŠ¨åˆ°å›æ”¶ç«™
                                    logger.info(f"[#process_log]å·²å°†åŸæ–‡ä»¶ç§»åŠ¨åˆ°å›æ”¶ç«™: {target_path}")
                                    shutil.copy2(temp_source, target_path)
                                    logger.info(f"[#process_log]è¦†ç›–æ–‡ä»¶: {file} -> {target_file}")
                                except Exception as e:
                                    logger.info(f"[#update_log]ç§»åŠ¨åˆ°å›æ”¶ç«™å¤±è´¥: {str(e)}")
                                    continue
                            success_count += 1
                            logger.info(f"[#process_log]æœ‰æ•ˆæ–‡ä»¶æ•°é‡: {source_count}")
                        else:
                            skip_count += 1
                            error_msg = f"[#process_log]è·³è¿‡: {file} - æ–‡ä»¶æ•°é‡ä¸ä¸€è‡´ (æº:{source_count}, ç›®æ ‡:{target_count})"
                            error_files.append(error_msg)
                            logger.info(error_msg)

                    # æˆåŠŸæ—¶æ›´æ–°ä¸ºç»¿è‰²å®ŒæˆçŠ¶æ€
                    if success_count % 10 == 0:  # æ¯10ä¸ªæ–‡ä»¶æ›´æ–°ä¸€æ¬¡è¿›åº¦
                        logger.info(f"[@current_progress]å·²å¤„ç† ({processed_files}/{total_files}) {progress:.1f}%")
                        
                except Exception as e:
                    # é”™è¯¯æ—¶æ›´æ–°ä¸ºçº¢è‰²è­¦ç¤º
                    logger.info(f"[@current_progress]âŒ é”™è¯¯ ({processed_files}/{total_files}) {progress:.1f}%")
                    skip_count += 1
                    error_msg = f"[#update_log]é”™è¯¯: {file} - {str(e)}"
                    error_files.append(error_msg)
                    logger.info(error_msg)
    
    # å¦‚æœæ˜¯ç§»åŠ¨æ¨¡å¼ï¼Œåˆ é™¤æºç›®å½•ä¸­çš„ç©ºæ–‡ä»¶å¤¹
    if is_move:
        removed_count = remove_empty_directories(source_dir)
        logger.info(f"[#process_log]\nå·²åˆ é™¤ {removed_count} ä¸ªç©ºæ–‡ä»¶å¤¹")
    
    # æœ€ç»ˆå®Œæˆè¿›åº¦
    logger.info(f"[@current_progress]âœ… å®Œæˆ ({processed_files}/{total_files}) 100%")
    
    # æ‰“å°æ€»ç»“
    logger.info("[#process_log]\nå¤„ç†å®Œæˆï¼")
    logger.info(f"[#current_stats]æˆåŠŸå¤„ç†: {success_count} ä¸ªæ–‡ä»¶")
    logger.info(f"[#current_stats]è·³è¿‡å¤„ç†: {skip_count} ä¸ªæ–‡ä»¶")
    if error_files:
        logger.info("[#process_log]\nè¯¦ç»†é”™è¯¯åˆ—è¡¨:")
        for error in error_files:
            logger.info(error)

def check_archive(file_path):
    """æ£€æµ‹å‹ç¼©åŒ…æ˜¯å¦æŸå"""
    try:
        result = subprocess.run(['7z', 't', file_path], 
                              capture_output=True, 
                              text=True)
        return result.returncode == 0
    except Exception as e:
        logger.info(f"[#process_log]æ£€æµ‹æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

def load_check_history(history_file):
    """åŠ è½½æ£€æµ‹å†å²è®°å½•ï¼Œåªè¯»å–æœ€åä¸€è¡Œæœ‰æ•ˆæ•°æ®"""
    history = {}
    if os.path.exists(history_file):
        with open(history_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # é€†åºè¯»å–ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆè¡Œ
            for line in reversed(lines):
                line = line.strip()
                if not line:
                    continue
                try:
                    entry = json.loads(line)
                    file_path = entry.get('path')
                    if file_path and file_path not in history:
                        history[file_path] = {
                            'time': entry.get('timestamp'),
                            'valid': entry.get('valid')
                        }
                except json.JSONDecodeError:
                    continue  # è·³è¿‡ä¸å®Œæ•´çš„è¡Œ
    return history

def save_check_history(history_file, new_entry):
    """è¿½åŠ æ–¹å¼ä¿å­˜æ£€æµ‹è®°å½•ï¼Œæ¯è¡Œä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡"""
    try:
        # åªä¿ç•™timestampå­—æ®µ
        new_entry['timestamp'] = datetime.now().isoformat()
        # åˆ é™¤æ—§çš„timeå­—æ®µ
        if 'time' in new_entry:
            del new_entry['time']
        with open(history_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(new_entry, ensure_ascii=False) + '\n')
    except Exception as e:
        logger.info(f"[#process_log]ä¿å­˜æ£€æŸ¥è®°å½•å¤±è´¥: {str(e)}")

def process_corrupted_archives(directory, skip_checked=True, max_workers=4):
    archive_extensions = ('.zip', '.rar', '.7z', '.cbz')
    history_file = os.path.join(directory, 'archive_check_history.json')
    
    # æ·»åŠ è¿™è¡Œåˆå§‹åŒ–check_history
    check_history = load_check_history(history_file)
    
    # åˆ é™¤temp_å¼€å¤´çš„æ–‡ä»¶å¤¹
    for root, dirs, _ in os.walk(directory, topdown=True):
        for dir_name in dirs[:]:
            if dir_name.startswith('temp_'):
                try:
                    dir_path = os.path.join(root, dir_name)
                    logger.info(f"[#process_log]æ­£åœ¨åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤¹: {dir_path}")
                    shutil.rmtree(dir_path)
                except Exception as e:
                    logger.info(f"[#process_log]åˆ é™¤æ–‡ä»¶å¤¹ {dir_path} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    # æ”¶é›†éœ€è¦å¤„ç†çš„æ–‡ä»¶
    files_to_process = []
    for root, _, files in os.walk(directory):
        for filename in files:
            if filename.lower().endswith(archive_extensions):
                file_path = os.path.join(root, filename)
                if file_path.endswith('.tdel'):
                    continue
                if skip_checked and file_path in check_history and check_history[file_path]['valid']:
                    logger.info(f"[#process_log]è·³è¿‡å·²æ£€æŸ¥ä¸”å®Œå¥½çš„æ–‡ä»¶: {file_path}")
                    continue
                files_to_process.append(file_path)

    # åˆå§‹åŒ–è¿›åº¦ç»Ÿè®¡ï¼ˆç§»åˆ°æ–‡ä»¶æ”¶é›†ä¹‹åï¼‰
    total = len(files_to_process)
    processed = 0
    
    if total == 0:
        logger.info("[#process_log]æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶")
        return

    @handle_file_operation(skip_errors=True)
    def process_single_file(file_path):
        nonlocal processed
        try:
            logger.info(f"[#current_progress]æ­£åœ¨æ£€æµ‹: {file_path}")
            is_valid = check_archive(file_path)
            return {
                'path': file_path,
                'valid': is_valid,
                'timestamp': datetime.now().isoformat(),
                'error': None
            }
        except Exception as e:
            logger.info(f"[#update_log]æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            return {
                'path': file_path,
                'valid': False,
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
        finally:
            processed += 1
            progress = (processed / total) * 100 if total > 0 else 0
            logger.info(f"[@current_progress]æ£€æµ‹ä¸­ ({processed}/{total}) {progress:.1f}%")

    # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†æ–‡ä»¶
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for file_path in files_to_process:
            future = executor.submit(process_single_file, file_path)
            futures.append(future)
        
        # å¤„ç†ç»“æœ
        for future in concurrent.futures.as_completed(futures):
            try:
                result = future.result()
            except Exception as e:
                logger.info(f"[#update_log]ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
                continue
            
            file_path = result['path']
            is_valid = result['valid']
            
            check_history[file_path] = {
                'timestamp': result['timestamp'],
                'valid': is_valid
            }
            
            # ä¿å­˜æ—¶æ”¹ä¸ºè°ƒç”¨æ–°çš„ä¿å­˜æ–¹æ³•
            save_check_history(history_file, {
                'path': file_path,
                'valid': is_valid,
                'timestamp': datetime.now().isoformat()
            })
            
            if not is_valid:
                new_path = file_path + '.tdel'
                if os.path.exists(new_path):
                    try:
                        os.remove(new_path)
                        logger.info(f"[#process_log]åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶: {new_path}")
                    except Exception as e:
                        logger.info(f"[#update_log]åˆ é™¤æ–‡ä»¶ {new_path} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                        continue
                
                try:
                    os.rename(file_path, new_path)
                    logger.info(f"[#process_log]æ–‡ä»¶æŸå,å·²é‡å‘½åä¸º: {new_path}")
                except Exception as e:
                    logger.info(f"[#update_log]é‡å‘½åæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            else:
                logger.info(f"[#process_log]æ–‡ä»¶å®Œå¥½")

    # æœ€ç»ˆå®Œæˆ
    logger.info(f"[@current_progress]âœ… å®Œæˆæ£€æµ‹ ({processed}/{total}) 100%")

if __name__ == "__main__":
    # å®šä¹‰ç›®å½•è·¯å¾„åˆ—è¡¨
    directory_pairs = [
        ("D:\\3EHV", "E:\\7EHV"),
        ("E:\\7EHV", "E:\\999EHV"),
    ]
    is_move = True  # è®¾ç½®ä¸ºTrueåˆ™ç§»åŠ¨æ–‡ä»¶ï¼ŒFalseåˆ™å¤åˆ¶æ–‡ä»¶
    
    # ä¾æ¬¡å¤„ç†æ¯å¯¹ç›®å½•
    for source_dir, target_dir in directory_pairs:
        logger.info(f"[#current_stats]\nå¼€å§‹å¤„ç†ç›®å½•å¯¹ï¼š")
        logger.info(f"[#process_log]æºç›®å½•: {source_dir}")
        logger.info(f"[#process_log]ç›®æ ‡ç›®å½•: {target_dir}")
        
        if not os.path.exists(source_dir):
            logger.info("[#process_log]æºç›®å½•ä¸å­˜åœ¨ï¼")
            continue
        elif not os.path.exists(target_dir):
            logger.info("[#process_log]ç›®æ ‡ç›®å½•ä¸å­˜åœ¨ï¼")
            continue
            
        # å…ˆæ£€æµ‹æŸåçš„å‹ç¼©åŒ…
        logger.info("[#process_log]\nå¼€å§‹æ£€æµ‹æŸåå‹ç¼©åŒ…...")
        process_corrupted_archives(source_dir)
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        temp_files_removed = remove_temp_files(source_dir)
        logger.info(f"[#process_log]\nå·²åˆ é™¤ {temp_files_removed} ä¸ªä¸´æ—¶æ–‡ä»¶")
        
        # æ‰§è¡Œæ–‡ä»¶ç§»åŠ¨/å¤åˆ¶æ“ä½œ
        compare_and_copy_archives(source_dir, target_dir, is_move)
