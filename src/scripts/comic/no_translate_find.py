import sys
import os
import logging
logging.basicConfig()  # åˆå§‹åŒ–æ ‡å‡†åº“logging

# æ ‡å‡†åº“å¯¼å…¥
import re
import shutil
from datetime import datetime
import argparse
import io
import functools
import subprocess
import threading
from functools import partial
import random
import zipfile
import win32com.client  # ç”¨äºåˆ›å»ºå¿«æ·æ–¹å¼

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import pyperclip
from PIL import Image
import pillow_avif
import pillow_jxl
from pathlib import Path
from colorama import init, Fore, Style
from typing import List, Dict, Set, Tuple, Optional
from opencc import OpenCC  # ç”¨äºç¹ç®€è½¬æ¢
from concurrent.futures import ThreadPoolExecutor, as_completed
from nodes.record.logger_config import setup_logger
from nodes.pics.calculate_hash_custom import ImageClarityEvaluator
from nodes.tui.textual_logger import TextualLoggerManager
from nodes.utils.number_shortener import shorten_number_cn

config = {
    'script_name': 'no_translate_find',
    'console_enabled': False
}
logger, config_info = setup_logger(config)
    # è®¾ç½®Textualæ—¥å¿—å¸ƒå±€
TEXTUAL_LAYOUT = {
    "stats": {
        "ratio": 2,
        "title": "ğŸ“Š å¤„ç†ç»Ÿè®¡",
        "style": "lightgreen"
    },
    "process": {
        "ratio": 2, 
        "title": "ğŸ”„ è¿›åº¦",
        "style": "lightcyan",
    },
    "file_ops": {
        "ratio": 3,
        "title": "ğŸ“‚ æ–‡ä»¶æ“ä½œ",
        "style": "lightblue",
    },
    "group_info": {
        "ratio": 3,
        "title": "ğŸ” ç»„å¤„ç†ä¿¡æ¯",
        "style": "lightpink",
    },
    "error_log": {
        "ratio": 2,
        "title": "âš ï¸ é”™è¯¯æ—¥å¿—",
        "style": "lightred",
    }
}

class ReportGenerator:
    """ç”Ÿæˆå¤„ç†æŠ¥å‘Šçš„ç±»"""
    def __init__(self):
        self.report_sections = []
        self.stats = {
            'total_files': 0,
            'total_groups': 0,
            'moved_to_trash': 0,
            'moved_to_multi': 0,
            'skipped_files': 0,
            'created_shortcuts': 0
        }
        self.group_details = []
        
    def add_group_detail(self, group_name: str, details: Dict):
        """æ·»åŠ ç»„å¤„ç†è¯¦æƒ…"""
        self.group_details.append({
            'name': group_name,
            'details': details
        })
        
    def update_stats(self, key: str, value: int = 1):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats[key] = self.stats.get(key, 0) + value
        
    def add_section(self, title: str, content: str):
        """æ·»åŠ æŠ¥å‘Šç« èŠ‚"""
        self.report_sections.append({
            'title': title,
            'content': content
        })
        
    def generate_report(self, base_dir: str) -> str:
        """ç”Ÿæˆæœ€ç»ˆçš„MDæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report = [
            f"# æ–‡ä»¶å¤„ç†æŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"å¤„ç†ç›®å½•: {base_dir}",
            "",
            "## å¤„ç†ç»Ÿè®¡",
            f"- æ€»æ–‡ä»¶æ•°: {shorten_number_cn(self.stats['total_files'])}",
            f"- æ€»åˆ†ç»„æ•°: {shorten_number_cn(self.stats['total_groups'])}",
            f"- ç§»åŠ¨åˆ°trashç›®å½•: {shorten_number_cn(self.stats['moved_to_trash'])}",
            f"- ç§»åŠ¨åˆ°multiç›®å½•: {shorten_number_cn(self.stats['moved_to_multi'])}",
            f"- è·³è¿‡çš„æ–‡ä»¶: {shorten_number_cn(self.stats['skipped_files'])}",
            f"- åˆ›å»ºçš„å¿«æ·æ–¹å¼: {shorten_number_cn(self.stats['created_shortcuts'])}",
            ""
        ]
        
        # æ·»åŠ ç»„è¯¦æƒ…ï¼ˆæ”¹ä¸ºåˆ—è¡¨å½¢å¼ï¼‰
        if self.group_details:
            report.append("## å¤„ç†è¯¦æƒ…åˆ—è¡¨")
            for group in self.group_details:
                report.append(f"- **{group['name']}**")
                details = group['details']
                if 'chinese_versions' in details:
                    report.append("  - æ±‰åŒ–ç‰ˆæœ¬:")
                    for file in details['chinese_versions']:
                        report.append(f"    - {file}")
                if 'other_versions' in details:
                    report.append("  - å…¶ä»–ç‰ˆæœ¬:")
                    for file in details['other_versions']:
                        report.append(f"    - {file}")
                if 'actions' in details:
                    report.append("  - æ‰§è¡Œæ“ä½œ:")
                    for action in details['actions']:
                        report.append(f"    - {action}")
                report.append("")  # ç»„é—´ç©ºè¡Œ
        
        # å…¶ä»–ç« èŠ‚ä¿æŒæ ‡é¢˜å½¢å¼
        for section in self.report_sections:
            report.append(f"## {section['title']}")
            report.append(section['content'])
            report.append("")
            
        return "\n".join(report)
        
    def save_report(self, base_dir: str, filename: Optional[str] = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if filename is None:
            filename = f"å¤„ç†æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        report_path = os.path.join(base_dir, filename)
        report_content = self.generate_report(base_dir)
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            return report_path
        except Exception as e:
            logger.error("[#error_log] âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: %s", str(e))
            logger.exception("[#error_log] å¼‚å¸¸å †æ ˆ:")  # è‡ªåŠ¨è®°å½•å †æ ˆä¿¡æ¯
            # åœ¨ç•Œé¢æ˜¾ç¤ºé”™è¯¯è¯¦æƒ…
            logger.info("[#process] ğŸ’¥ é‡åˆ°ä¸¥é‡é”™è¯¯ï¼Œè¯·æ£€æŸ¥error_logé¢æ¿")
            return None

# åˆå§‹åŒ–colorama
init()

# æ·»åŠ è‡ªå®šä¹‰æ¨¡å—è·¯å¾„å¹¶å¯¼å…¥
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åˆå§‹åŒ–OpenCC
cc_s2t = OpenCC('s2t')  # åˆ›å»ºç®€ä½“åˆ°ç¹ä½“è½¬æ¢å™¨
cc_t2s = OpenCC('t2s')  # åˆ›å»ºç¹ä½“åˆ°ç®€ä½“è½¬æ¢å™¨

# æ”¯æŒçš„å‹ç¼©åŒ…æ ¼å¼
ARCHIVE_EXTENSIONS = {
    '.zip', '.rar', '.7z', '.cbr', '.cbz', 
    '.cb7', '.cbt', '.tar', '.gz', '.bz2'
}

# æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
IMAGE_EXTENSIONS = {
    '.jpg', '.jpeg', '.png', '.webp', '.avif', '.jxl',
    '.gif', '.bmp', '.tiff', '.tif', '.heic', '.heif'
}

def preprocess_keywords(keywords: Set[str]) -> Set[str]:
    """é¢„å¤„ç†å…³é”®è¯é›†åˆï¼Œæ·»åŠ ç¹ç®€ä½“å˜ä½“"""
    processed = set()
    for keyword in keywords:
        # æ·»åŠ åŸå§‹å…³é”®è¯ï¼ˆå°å†™ï¼‰
        processed.add(keyword.lower())
        # æ·»åŠ ç¹ä½“ç‰ˆæœ¬
        traditional = cc_s2t.convert(keyword)
        processed.add(traditional.lower())
        # æ·»åŠ ç®€ä½“ç‰ˆæœ¬
        simplified = cc_t2s.convert(keyword)
        processed.add(simplified.lower())
    return processed

# é¢„å¤„ç†æ±‰åŒ–ç‰ˆæœ¬å…³é”®è¯é›†åˆ
CHINESE_VERSION_KEYWORDS = {
    'æ±‰åŒ–', 'æ¼¢åŒ–',  # æ±‰åŒ–/æ¼¢åŒ–
    'ç¿»è¯‘', 'ç¿»è¨³', 'ç¿»è­¯', # ç¿»è¯‘ç›¸å…³
    'ä¸­å›½ç¿»è¯‘', 'ä¸­å›½ç¿»è¨³', 'ä¸­å›½èª','chinese','ä¸­æ–‡','ä¸­å›½', # ä¸­æ–‡ç¿»è¯‘
    'åµŒå­—',  # åµŒå­—
    'æƒåœ–', 'æƒ', # æ‰«å›¾ç›¸å…³
    'åˆ¶ä½œ', 'è£½ä½œ', # åˆ¶ä½œç›¸å…³
    'é‡åµŒ',  # é‡æ–°åµŒå…¥
    'ä¸ªäºº', # ä¸ªäººç¿»è¯‘
    'ä¿®æ­£',  # ä¿®æ­£ç‰ˆæœ¬
    'å»ç ',
    'æ—¥è¯­ç¤¾',
    'åˆ¶ä½œ',
    'æœºç¿»',
    'èµåŠ©',
    'æ±‰', 'æ¼¢', # æ±‰å­—ç›¸å…³
    'æ•°ä½', 'æœªæ¥æ•°ä½', 'æ–°è§†ç•Œ', # æ±‰åŒ–ç›¸å…³
    'å‡ºç‰ˆ', 'é’æ–‡å‡ºç‰ˆ', # ç¿»è¯‘ç›¸å…³
    'è„¸è‚¿', 'æ— æ¯’', 'ç©ºæ°—ç³»', 'å¤¢ä¹‹è¡Œè¹¤', 'èŒå¹»é´¿é„‰', 'ç»…å£«ä»“åº“', 'Lolipoi', 'é´ä¸‹','CEå®¶æ—ç¤¾',
    'ä¸å¯è§†', 'ä¸€åŒ™å’–å•¡è±†', 'æ— é‚ªæ°”', 'æ´¨äº”', 'ç™½æ¨', 'ç‘æ ‘',  # å¸¸è§æ±‰åŒ–ç»„å
    'å†Šèªè‰å ‚','æ·«ä¹¦é¦†','æ˜¯å°ç‹ç‹¸å“¦','å·¥æˆ¿','å·¥åŠ','åŸºåœ°'
    'æ±‰åŒ–ç»„', 'æ¼¢åŒ–çµ„', 'æ±‰åŒ–ç¤¾', 'æ¼¢åŒ–ç¤¾', 'CE å®¶æ—ç¤¾', 'CE å®¶æ—ç¤¾',  # å¸¸è§åç¼€
    'ä¸ªäººæ±‰åŒ–', 'å€‹äººæ¼¢åŒ–'  # ä¸ªäººæ±‰åŒ–
}

# é¢„å¤„ç†åŸç‰ˆå…³é”®è¯é›†åˆ
ORIGINAL_VERSION_KEYWORDS = {
    'Digital', 'DLç‰ˆ', 'DL', 'ãƒ‡ã‚¸ã‚¿ãƒ«ç‰ˆ',  # æ•°å­—ç‰ˆæœ¬ç›¸å…³
    'å‡ºç‰ˆ', 'å‡ºç‰ˆç¤¾', 'æ›¸åº—ç‰ˆ', 'é€šå¸¸ç‰ˆ',  # å‡ºç‰ˆç›¸å…³
    'ç„¡ä¿®æ­£', 'æ— ä¿®æ­£', 'æ— ä¿®', 'ç„¡ä¿®',  # æ— ä¿®æ­£ç‰ˆæœ¬
    'å®Œå…¨ç‰ˆ', 'å®Œæ•´ç‰ˆ', # å®Œæ•´ç‰ˆæœ¬
}

# é¢„å¤„ç†é»‘åå•å…³é”®è¯é›†åˆ
BLACKLIST_KEYWORDS = {
    # ç”»é›†/å›¾é›†ç›¸å…³
    'trash', 'ç”»é›†', 'ç•«é›†', 'artbook', 'art book', 'art works', 'illustrations',
    'å›¾é›†', 'åœ–é›†', 'illust', 'collection',
    'æ‚å›¾', 'é›œåœ–', 'æ‚å›¾åˆé›†', 'é›œåœ–åˆé›†',
    # å…¶ä»–ä¸éœ€è¦å¤„ç†çš„ç±»å‹
    'pixiv', 'fanbox', 'gumroad', 'twitter',
    'å¾…åˆ†ç±»', 'å¾…è™•ç†', 'å¾…åˆ†é¡',
    'å›¾åŒ…', 'åœ–åŒ…',
    'å›¾ç‰‡', 'åœ–ç‰‡',
    'cg', 'CG',
}

# é¢„å¤„ç†æ‰€æœ‰å…³é”®è¯é›†åˆ
_CHINESE_VERSION_KEYWORDS_FULL = preprocess_keywords(CHINESE_VERSION_KEYWORDS)
_ORIGINAL_VERSION_KEYWORDS_FULL = preprocess_keywords(ORIGINAL_VERSION_KEYWORDS)
_BLACKLIST_KEYWORDS_FULL = preprocess_keywords(BLACKLIST_KEYWORDS)

# æ·»åŠ çº¿ç¨‹æœ¬åœ°å­˜å‚¨
thread_local = threading.local()

def clean_filename(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œåªä¿ç•™ä¸»æ–‡ä»¶åéƒ¨åˆ†è¿›è¡Œæ¯”è¾ƒ"""
    # ç§»é™¤æ‰©å±•å
    name = os.path.splitext(filename)[0]
    
    # æå–æ‹¬å·ä¸­çš„å†…å®¹
    pattern_brackets = re.compile(r'\[([^\[\]]+)\]')  # åŒ¹é…æ–¹æ‹¬å·
    pattern_parentheses = re.compile(r'\(([^\(\)]+)\)')  # åŒ¹é…åœ†æ‹¬å·
    pattern_curly_brackets = re.compile(r'\{(.*?)\}')  # åŒ¹é…èŠ±æ‹¬å·
    hanhua_match = re.search(r'\[(.*?æ±‰åŒ–.*?)\]', name)
    hanhua_info = hanhua_match.group(1) if hanhua_match else ''
    
    # ç§»é™¤æ‰€æœ‰æ‹¬å·å†…å®¹
    name = pattern_brackets.sub('', name)  # ç§»é™¤æ‰€æœ‰æ–¹æ‹¬å·å†…å®¹
    name = pattern_parentheses.sub('', name)  # ç§»é™¤æ‰€æœ‰åœ†æ‹¬å·å†…å®¹
    name = pattern_curly_brackets.sub('', name)  # ç§»é™¤æ‰€æœ‰èŠ±æ‹¬å·å†…å®¹
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    # name = re.sub(r'\s+', ' ', name)  # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ª
    name = re.sub(r'\s+', '', name)  # å®Œå…¨å»é™¤æ‰€æœ‰ç©ºæ ¼
    name = name.strip().lower()  # è½¬æ¢ä¸ºå°å†™å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    
    # è¿”å›æ¸…ç†åçš„åç§°å’Œæ±‰åŒ–ä¿¡æ¯
    return name, hanhua_info

@functools.lru_cache(maxsize=10000)
def is_chinese_version(filename: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ±‰åŒ–ç‰ˆæœ¬"""
    # è½¬æ¢æ–‡ä»¶åä¸ºå°å†™
    filename_lower = filename.lower()
    # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„å…³é”®è¯é›†åˆè¿›è¡Œæ£€æŸ¥
    return any(keyword in filename_lower for keyword in _CHINESE_VERSION_KEYWORDS_FULL)

@functools.lru_cache(maxsize=10000)
def has_original_keywords(filename: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦åŒ…å«åŸç‰ˆç‰¹æ®Šå…³é”®å­—"""
    # è½¬æ¢æ–‡ä»¶åä¸ºå°å†™
    filename_lower = filename.lower()
    # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„å…³é”®è¯é›†åˆè¿›è¡Œæ£€æŸ¥
    return any(keyword in filename_lower for keyword in _ORIGINAL_VERSION_KEYWORDS_FULL)

# ä½¿ç”¨ functools.lru_cache è£…é¥°å™¨ç¼“å­˜ç»“æœ
@functools.lru_cache(maxsize=10000)
def is_in_blacklist(filepath: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶åæˆ–è·¯å¾„æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯"""
    # è½¬æ¢è·¯å¾„ä¸ºå°å†™ï¼Œåªè½¬æ¢ä¸€æ¬¡
    filepath_lower = str(filepath).lower()
    # ç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„å…³é”®è¯é›†åˆè¿›è¡Œæ£€æŸ¥
    return any(keyword in filepath_lower for keyword in _BLACKLIST_KEYWORDS_FULL)

def is_besscan_version(filename: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºåˆ¥ã‚¹ã‚­ãƒ£ãƒ³ç‰ˆæœ¬"""
    return 'åˆ¥ã‚¹ã‚­ãƒ£ãƒ³' in filename

def group_similar_files(files: List[str]) -> Dict[str, List[str]]:
    """å°†ç›¸ä¼¼æ–‡ä»¶åˆ†ç»„"""
    groups: Dict[str, List[str]] = {}
    
    for file in files:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨é»‘åå•ä¸­
        if is_in_blacklist(file):
            logger.info("[#file_ops] â­ï¸ è·³è¿‡é»‘åå•æ–‡ä»¶: %s", file)
            continue
            
        clean_name, _ = clean_filename(file)
        if clean_name not in groups:
            groups[clean_name] = []
        groups[clean_name].append(file)
        
    return groups

def get_7zip_path() -> str:
    """è·å–7zipå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„"""
    # å¸¸è§çš„7zipå®‰è£…è·¯å¾„
    possible_paths = [
        r"C:\Program Files\7-Zip\7z.exe",
        r"C:\Program Files (x86)\7-Zip\7z.exe",
        os.path.join(os.getenv("ProgramFiles", ""), "7-Zip", "7z.exe"),
        os.path.join(os.getenv("ProgramFiles(x86)", ""), "7-Zip", "7z.exe"),
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
            
    # å¦‚æœæ‰¾ä¸åˆ°7zipï¼Œå°è¯•ä½¿ç”¨å‘½ä»¤è¡Œçš„7z
    try:
        subprocess.run(['7z'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return '7z'
    except:
        return None

def get_archive_info(archive_path: str) -> List[Tuple[str, int]]:
    """ä½¿ç”¨7zipè·å–å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶ä¿¡æ¯"""
    try:
        seven_zip = get_7zip_path()
        if not seven_zip:
            logger.info("[#error_log] âŒ æœªæ‰¾åˆ°7-Zip")
            return []
            
        # åˆ—å‡ºå‹ç¼©åŒ…å†…å®¹
        cmd = [seven_zip, 'l', archive_path]
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.info("[#error_log] âŒ 7-Zipå‘½ä»¤æ‰§è¡Œå¤±è´¥: %s", result.stderr)
            return []
            
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ä¿¡æ¯
        image_files = []
        for line in result.stdout.splitlines():
            for ext in IMAGE_EXTENSIONS:
                if line.lower().endswith(ext):
                    # è§£ææ–‡ä»¶å¤§å°ï¼ˆæ ¹æ®7zè¾“å‡ºæ ¼å¼è°ƒæ•´ï¼‰
                    parts = line.split()
                    if len(parts) >= 4:
                        try:
                            size = int(parts[3])
                            name = parts[-1]
                            image_files.append((name, size))
                        except:
                            continue
                    break
        return image_files
        
    except Exception as e:
        logger.info("[#error_log] âŒ è·å–å‹ç¼©åŒ…ä¿¡æ¯å¤±è´¥ %s: %s", archive_path, str(e))
        return []

def get_image_count(archive_path: str) -> int:
    """è®¡ç®—å‹ç¼©åŒ…ä¸­çš„å›¾ç‰‡æ€»æ•°"""
    image_files = get_archive_info(archive_path)
    return len(image_files)

def get_sample_images(archive_path: str, temp_dir: str, sample_count: int = 3) -> List[str]:
    """ä»å‹ç¼©åŒ…ä¸­æå–æ ·æœ¬å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•"""
    image_files = get_archive_info(archive_path)
    if not image_files:
        return []
        
    # æŒ‰æ–‡ä»¶å¤§å°æ’åº
    image_files.sort(key=lambda x: x[1], reverse=True)
    
    # é€‰æ‹©æ ·æœ¬
    samples = []
    if image_files:
        samples.append(image_files[0][0])  # æœ€å¤§çš„æ–‡ä»¶
        if len(image_files) > 2:
            samples.append(image_files[len(image_files)//2][0])  # ä¸­é—´çš„æ–‡ä»¶
        
        # ä»å‰30%é€‰æ‹©å‰©ä½™æ ·æœ¬
        top_30_percent = image_files[:max(3, len(image_files) // 3)]
        while len(samples) < sample_count and top_30_percent:
            sample = random.choice(top_30_percent)[0]
            if sample not in samples:
                samples.append(sample)
    
    # æå–é€‰ä¸­çš„æ ·æœ¬åˆ°ä¸´æ—¶ç›®å½•
    seven_zip = get_7zip_path()
    extracted_files = []
    for sample in samples:
        temp_file = os.path.join(temp_dir, os.path.basename(sample))
        cmd = [seven_zip, 'e', archive_path, sample, f'-o{temp_dir}', '-y']
        result = subprocess.run(cmd, capture_output=True)
        if result.returncode == 0 and os.path.exists(temp_file):
            extracted_files.append(temp_file)
            
    return extracted_files

def calculate_representative_width(archive_path: str, sample_count: int = 3) -> int:
    """è®¡ç®—å‹ç¼©åŒ…ä¸­å›¾ç‰‡çš„ä»£è¡¨å®½åº¦ï¼ˆä½¿ç”¨æŠ½æ ·å’Œä¸­ä½æ•°ï¼‰"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        ext = os.path.splitext(archive_path)[1].lower()
        if ext not in {'.zip', '.cbz'}:  # åªå¤„ç†zipæ ¼å¼
            return 0

        # è·å–å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶ä¿¡æ¯
        image_files = []
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for info in zf.infolist():
                    if os.path.splitext(info.filename.lower())[1] in IMAGE_EXTENSIONS:
                        image_files.append((info.filename, info.file_size))
        except zipfile.BadZipFile:
            logger.info("[#error_log] âš ï¸ æ— æ•ˆçš„ZIPæ–‡ä»¶: %s", archive_path)
            return 0

        if not image_files:
            return 0

        # æŒ‰æ–‡ä»¶å¤§å°æ’åº
        image_files.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©æ ·æœ¬
        samples = []
        if image_files:
            samples.append(image_files[0][0])  # æœ€å¤§çš„æ–‡ä»¶
            if len(image_files) > 2:
                samples.append(image_files[len(image_files)//2][0])  # ä¸­é—´çš„æ–‡ä»¶
            
            # ä»å‰30%é€‰æ‹©å‰©ä½™æ ·æœ¬
            top_30_percent = image_files[:max(3, len(image_files) // 3)]
            while len(samples) < sample_count and top_30_percent:
                sample = random.choice(top_30_percent)[0]
                if sample not in samples:
                    samples.append(sample)

        widths = []
        try:
            with zipfile.ZipFile(archive_path, 'r') as zf:
                for sample in samples:
                    try:
                        # ç›´æ¥ä»zipè¯»å–åˆ°å†…å­˜
                        with zf.open(sample) as file:
                            img_data = file.read()
                            with Image.open(io.BytesIO(img_data)) as img:
                                widths.append(img.width)
                    except Exception as e:
                        logger.info("[#error_log] âš ï¸ è¯»å–å›¾ç‰‡å®½åº¦å¤±è´¥ %s: %s", sample, str(e))
                        continue
        except Exception as e:
            logger.info("[#error_log] âš ï¸ æ‰“å¼€ZIPæ–‡ä»¶å¤±è´¥: %s", str(e))
            return 0

        if not widths:
            return 0

        # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºä»£è¡¨å®½åº¦
        return int(sorted(widths)[len(widths)//2])

    except Exception as e:
        logger.info("[#error_log] âŒ è®¡ç®—ä»£è¡¨å®½åº¦å¤±è´¥ %s: %s", archive_path, str(e))
        return 0

def extract_width_from_filename(filename: str) -> int:
    """ä»æ–‡ä»¶åä¸­æå–å®½åº¦ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›0"""
    # åŒ¹é…[æ•°å­—px]æ ¼å¼
    match = re.search(r'\[(\d+)px\]', filename)
    if match:
        try:
            return int(match.group(1))
        except ValueError:
            return 0
    return 0

def safe_move_file(src_path: str, dst_path: str, max_retries: int = 3, delay: float = 1.0) -> bool:
    """
    å®‰å…¨åœ°ç§»åŠ¨æ–‡ä»¶ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶å’Œå®Œæ•´æ€§æ£€æŸ¥
    
    Args:
        src_path: æºæ–‡ä»¶è·¯å¾„
        dst_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        handler: æ—¥å¿—å¤„ç†å™¨
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: é‡è¯•å»¶è¿Ÿæ—¶é—´(ç§’)
        
    Returns:
        bool: ç§»åŠ¨æ˜¯å¦æˆåŠŸ
    """
    import time
    import os
    
    # ç¡®ä¿æºæ–‡ä»¶å­˜åœ¨
    if not os.path.exists(src_path):
        logger.info("[#error_log] âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: %s", src_path)
        return False
        
    # ç¡®ä¿æºæ–‡ä»¶å¯è¯»
    if not os.access(src_path, os.R_OK):
        logger.info("[#error_log] âŒ æºæ–‡ä»¶æ— æ³•è¯»å–: %s", src_path)
        return False
        
    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    dst_dir = os.path.dirname(dst_path)
    try:
        os.makedirs(dst_dir, exist_ok=True)
    except Exception as e:
        logger.info("[#error_log] âŒ åˆ›å»ºç›®æ ‡ç›®å½•å¤±è´¥: %s, é”™è¯¯: %s", dst_dir, str(e))
        return False
        
    # æ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å¯å†™
    if not os.access(dst_dir, os.W_OK):
        logger.info("[#error_log] âŒ ç›®æ ‡ç›®å½•æ— å†™å…¥æƒé™: %s", dst_dir)
        return False
        
    # è·å–æºæ–‡ä»¶å¤§å°
    try:
        src_size = os.path.getsize(src_path)
    except Exception as e:
        logger.info("[#error_log] âŒ æ— æ³•è·å–æºæ–‡ä»¶å¤§å°: %s, é”™è¯¯: %s", src_path, str(e))
        return False
        
    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆå°è¯•åˆ é™¤
            if os.path.exists(dst_path):
                try:
                    os.remove(dst_path)
                except Exception as e:
                    # è®°å½•é”™è¯¯ä½†ç»§ç»­å°è¯•ç§»åŠ¨
                    logger.info("[#error_log] âš ï¸ æ— æ³•åˆ é™¤å·²å­˜åœ¨çš„ç›®æ ‡æ–‡ä»¶: %s, é”™è¯¯: %s", dst_path, str(e))
                    # å°è¯•ä½¿ç”¨å…¶ä»–æ–¹å¼å¤„ç†
                    try:
                        # 1. å°è¯•ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å
                        temp_dst_path = dst_path + f".temp_{attempt}"
                        shutil.move(src_path, temp_dst_path)
                        # å¦‚æœç§»åŠ¨åˆ°ä¸´æ—¶æ–‡ä»¶æˆåŠŸï¼Œå†å°è¯•é‡å‘½å
                        os.replace(temp_dst_path, dst_path)
                        return True
                    except Exception as move_error:
                        logger.info("[#error_log] âš ï¸ ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç§»åŠ¨å¤±è´¥: %s", str(move_error))
                        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                        if attempt < max_retries - 1:
                            time.sleep(delay)
                            continue
                        return False
            
            # æ‰§è¡Œç§»åŠ¨æ“ä½œ
            shutil.move(src_path, dst_path)
            
            # éªŒè¯ç§»åŠ¨åçš„æ–‡ä»¶
            if not os.path.exists(dst_path):
                raise Exception("ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨")
                
            # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦ä¸€è‡´
            dst_size = os.path.getsize(dst_path)
            if dst_size != src_size:
                raise Exception(f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æºæ–‡ä»¶ {src_size} å­—èŠ‚, ç›®æ ‡æ–‡ä»¶ {dst_size} å­—èŠ‚")
                
            return True
            
        except Exception as e:
            if attempt < max_retries - 1:
                logger.info("[#error_log] âš ï¸ ç§»åŠ¨æ–‡ä»¶å¤±è´¥ï¼Œå°è¯•é‡è¯• (%d/%d): %s", attempt + 1, max_retries, str(e))
                time.sleep(delay)
                continue
            else:
                logger.info("[#error_log] âŒ ç§»åŠ¨æ–‡ä»¶å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: %s", str(e))
                # å¦‚æœæœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œæ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦è¿˜å­˜åœ¨
                if not os.path.exists(src_path):
                    logger.info("[#error_log] âŒ æºæ–‡ä»¶å·²ä¸å­˜åœ¨: %s", src_path)
                return False
                
    return False

def process_file_with_count(file_path: str) -> Tuple[str, str, int, float]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŸå§‹è·¯å¾„ã€æ–°è·¯å¾„ã€å®½åº¦å’Œæ¸…æ™°åº¦"""
    full_path = file_path
    dir_name = os.path.dirname(file_path)
    file_name = os.path.basename(file_path)
    name, ext = os.path.splitext(file_name)
    
    # ç§»é™¤å·²æœ‰çš„æ ‡è®°
    name = re.sub(r'\{\d+p\}', '', name)
    name = re.sub(r'\{\d+w\}', '', name)
    name = re.sub(r'\{\d+de\}', '', name)
    
    # è®¡ç®—å…ƒæ•°æ®
    image_count = get_image_count(full_path)
    width = calculate_representative_width(full_path)
    
    # è®¡ç®—æ¸…æ™°åº¦è¯„åˆ†ï¼ˆæ–°å¢ï¼‰
    clarity_score = 0.0
    try:
        with zipfile.ZipFile(full_path, 'r') as zf:
            image_files = [f for f in zf.namelist() if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS]
            if image_files:
                sample_files = random.sample(image_files, min(5, len(image_files)))
                scores = []
                for sample in sample_files:
                    with zf.open(sample) as f:
                        img_data = f.read()
                        scores.append(ImageClarityEvaluator.calculate_definition(img_data))
                clarity_score = sum(scores) / len(scores) if scores else 0.0
    except Exception as e:
        logger.error("[#error_log] æ¸…æ™°åº¦è®¡ç®—å¤±è´¥ %s: %s", file_path, str(e))
    
    # ä¿®æ”¹åçš„æ ‡è®°ç”Ÿæˆéƒ¨åˆ†
    if image_count > 0:
        count_str = shorten_number_cn(image_count, use_w=False)  # ä½¿ç”¨kå•ä½
        name = f"{name}{{{count_str}@PX}}"
    if width > 0:
        width_str = shorten_number_cn(width, use_w=False)  # ä½¿ç”¨kå•ä½
        name = f"{name}{{{width_str}@WD}}"
    if clarity_score > 0:
        # æ¸…æ™°åº¦ä½¿ç”¨æ•´æ•°ç™¾åˆ†æ¯”æ ¼å¼
        name = f"{name}{{{clarity_score}@DE}}"
    
    new_name = f"{name}{ext}"
    new_path = os.path.join(dir_name, new_name) if dir_name else new_name
    return file_path, new_path, width, clarity_score

def process_file_group(group_files: List[str], base_dir: str, trash_dir: str, report_generator: ReportGenerator, dry_run: bool = False, create_shortcuts: bool = False, sample_count: int = 3) -> None:
    """å¤„ç†ä¸€ç»„ç›¸ä¼¼æ–‡ä»¶"""
    # è·å–ç»„çš„åŸºç¡€åç§°
    group_base_name, _ = clean_filename(group_files[0])
    logger.info("[#group_info] ğŸ” å¼€å§‹å¤„ç†ç»„: %s", group_base_name)
    
    # è¿‡æ»¤é»‘åå•æ–‡ä»¶
    filtered_files = []
    for file in group_files:
        if is_in_blacklist(file):
            logger.info("[#file_ops] â­ï¸ è·³è¿‡é»‘åå•æ–‡ä»¶: %s", file)
            report_generator.update_stats('skipped_files')
            continue
        filtered_files.append(file)
    
    if not filtered_files:
        logger.info("[#file_ops] ğŸš« æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨é»‘åå•ä¸­ï¼Œè·³è¿‡å¤„ç†")
        return
        
    # åˆ†ç±»æ–‡ä»¶
    chinese_versions = [f for f in filtered_files if is_chinese_version(f)]
    other_versions = [f for f in filtered_files if not is_chinese_version(f)]
    
    # æ£€æŸ¥æ±‰åŒ–ç‰ˆæœ¬ä¸­æ˜¯å¦æœ‰åŒ…å«åŸç‰ˆå…³é”®è¯çš„
    chinese_has_original = any(has_original_keywords(f) for f in chinese_versions)
    
    # å¦‚æœæ±‰åŒ–ç‰ˆæœ¬ä¸­æ²¡æœ‰åŸç‰ˆå…³é”®è¯ï¼Œåˆ™å°†å…¶ä»–ç‰ˆæœ¬ä¸­åŒ…å«åŸç‰ˆå…³é”®è¯çš„ä¹Ÿå½’ä¸ºéœ€è¦ä¿ç•™çš„ç‰ˆæœ¬
    original_keyword_versions = []
    if not chinese_has_original:
        original_keyword_versions = [f for f in other_versions if has_original_keywords(f)]
        if original_keyword_versions:
            chinese_versions.extend(original_keyword_versions)
            # ä»other_versionsä¸­ç§»é™¤è¿™äº›æ–‡ä»¶
            other_versions = [f for f in other_versions if not has_original_keywords(f)]
            logger.info("[#file_ops] ğŸ“ å°†%dä¸ªåŒ…å«åŸç‰ˆå…³é”®è¯çš„æ–‡ä»¶å½’å…¥ä¿ç•™åˆ—è¡¨", len(original_keyword_versions))
    
    # ä¸ºæ¯ä¸ªæ–‡ä»¶æ·»åŠ å›¾ç‰‡æ•°é‡æ ‡è®°å’Œè®¡ç®—å®½åº¦
    def process_file_with_count(file_path: str) -> Tuple[str, str, int, float]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŸå§‹è·¯å¾„ã€æ–°è·¯å¾„ã€å®½åº¦å’Œæ¸…æ™°åº¦"""
        full_path = os.path.join(base_dir, file_path)
        dir_name = os.path.dirname(file_path)
        file_name = os.path.basename(file_path)
        name, ext = os.path.splitext(file_name)
        
        # ç§»é™¤å·²æœ‰çš„æ ‡è®°
        name = re.sub(r'\{\d+p\}', '', name)
        name = re.sub(r'\{\d+w\}', '', name)
        name = re.sub(r'\{\d+de\}', '', name)
        
        # è®¡ç®—å…ƒæ•°æ®
        image_count = get_image_count(full_path)
        width = calculate_representative_width(full_path)
        
        # è®¡ç®—æ¸…æ™°åº¦è¯„åˆ†ï¼ˆæ–°å¢ï¼‰
        clarity_score = 0.0
        try:
            with zipfile.ZipFile(full_path, 'r') as zf:
                image_files = [f for f in zf.namelist() if os.path.splitext(f.lower())[1] in IMAGE_EXTENSIONS]
                if image_files:
                    sample_files = random.sample(image_files, min(5, len(image_files)))
                    scores = []
                    for sample in sample_files:
                        with zf.open(sample) as f:
                            img_data = f.read()
                            scores.append(ImageClarityEvaluator.calculate_definition(img_data))
                    clarity_score = sum(scores) / len(scores) if scores else 0.0
        except Exception as e:
            logger.error("[#error_log] æ¸…æ™°åº¦è®¡ç®—å¤±è´¥ %s: %s", file_path, str(e))
        
        # æ·»åŠ æ–°æ ‡è®°
        if image_count > 0:
            count_str = shorten_number_cn(image_count, use_w=False)  # ä½¿ç”¨kå•ä½
            name = f"{name}{{{count_str}@PX}}"
        if width > 0:
            width_str = shorten_number_cn(width, use_w=False)  # ä½¿ç”¨kå•ä½
            name = f"{name}{{{width_str}@WD}}"
        if clarity_score > 0:
            # æ¸…æ™°åº¦ä½¿ç”¨æ•´æ•°ç™¾åˆ†æ¯”æ ¼å¼
            name = f"{name}{{{clarity_score}@DE}}"
        
        new_name = f"{name}{ext}"
        new_path = os.path.join(dir_name, new_name) if dir_name else new_name
        return file_path, new_path, width, clarity_score
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    all_files = chinese_versions + other_versions
    processed_files = []
    
    # æ¸…ç©ºå®½åº¦ä¿¡æ¯é¢æ¿
    logger.info("[#file_ops]")
    
    # å‡†å¤‡ç»„è¯¦æƒ…æŠ¥å‘Š
    group_details = {
        'chinese_versions': chinese_versions,
        'other_versions': other_versions,
        'actions': []
    }
    
    for file in all_files:
        old_path, new_path, width, clarity = process_file_with_count(file)
        if old_path != new_path:
            old_full_path = os.path.join(base_dir, old_path)
            new_full_path = os.path.join(base_dir, new_path)
            if not dry_run:
                try:
                    os.rename(old_full_path, new_full_path)
                    processed_files.append((old_path, new_path))
                    logger.info("[#file_ops] âœ… å·²é‡å‘½å: %s -> %s", old_path, new_path)
                    group_details['actions'].append(f"é‡å‘½å: {old_path} -> {new_path}")
                except Exception as e:
                    logger.error("[#error_log] âŒ é‡å‘½åå¤±è´¥ %s: %s", old_path, str(e))
                    processed_files.append((old_path, old_path))
            else:
                processed_files.append((old_path, new_path))
                logger.info("[#file_ops] ğŸ”„ [DRY RUN] å°†é‡å‘½å: %s -> %s", old_path, new_path)
                group_details['actions'].append(f"[DRY RUN] å°†é‡å‘½å: {old_path} -> {new_path}")
        else:
            processed_files.append((old_path, old_path))
    
    # æ›´æ–°æ–‡ä»¶è·¯å¾„
    chinese_versions = [new_path for old_path, new_path in processed_files if old_path in chinese_versions]
    other_versions = [new_path for old_path, new_path in processed_files if old_path in other_versions]
    
    # å¤„ç†æ–‡ä»¶ç§»åŠ¨é€»è¾‘
    if chinese_versions:
        # æœ‰æ±‰åŒ–ç‰ˆæœ¬çš„æƒ…å†µ
        if len(chinese_versions) > 1:
            # å¤šä¸ªæ±‰åŒ–ç‰ˆæœ¬ï¼Œç§»åŠ¨åˆ°multi
            multi_dir = os.path.join(base_dir, 'multi')
            if not dry_run:
                os.makedirs(multi_dir, exist_ok=True)
            for file in chinese_versions:
                src_path = os.path.join(base_dir, file)
                rel_path = os.path.relpath(src_path, base_dir)
                dst_path = os.path.join(multi_dir, rel_path)
                if not dry_run:
                    logger.info("[#file_ops] ğŸ”„ æ­£åœ¨ç§»åŠ¨åˆ°multi: %s", file)
                    if safe_move_file(src_path, dst_path):
                        logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°multi: %s", file)
                        group_details['actions'].append(f"ç§»åŠ¨åˆ°multi: {file}")
                        report_generator.update_stats('moved_to_multi')
                else:
                    logger.info("[#file_ops] ğŸ”„ [DRY RUN] å°†ç§»åŠ¨åˆ°multi: %s", file)
                    group_details['actions'].append(f"[DRY RUN] å°†ç§»åŠ¨åˆ°multi: {file}")
            
            # ç§»åŠ¨å…¶ä»–éåŸç‰ˆåˆ°trash
            for other_file in other_versions:
                src_path = os.path.join(base_dir, other_file)
                rel_path = os.path.relpath(src_path, base_dir)
                dst_path = os.path.join(trash_dir, rel_path)
                if not dry_run:
                    logger.info("[#file_ops] ğŸ”„ æ­£åœ¨ç§»åŠ¨åˆ°trash: %s", other_file)
                    if create_shortcuts:
                        shortcut_path = os.path.splitext(dst_path)[0]
                        if create_shortcut(src_path, shortcut_path):
                            logger.info("[#file_ops] âœ… å·²åˆ›å»ºå¿«æ·æ–¹å¼: %s", other_file)
                            group_details['actions'].append(f"åˆ›å»ºå¿«æ·æ–¹å¼: {other_file}")
                            report_generator.update_stats('created_shortcuts')
                    else:
                        if safe_move_file(src_path, dst_path):
                            logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°trash: %s", other_file)
                            group_details['actions'].append(f"ç§»åŠ¨åˆ°trash: {other_file}")
                            report_generator.update_stats('moved_to_trash')
                else:
                    if create_shortcuts:
                        logger.info("[#file_ops] ğŸ”„ [DRY RUN] å°†åˆ›å»ºå¿«æ·æ–¹å¼: %s", other_file)
                        group_details['actions'].append(f"[DRY RUN] å°†åˆ›å»ºå¿«æ·æ–¹å¼: {other_file}")
                    else:
                        logger.info("[#file_ops] ğŸ”„ [DRY RUN] å°†ç§»åŠ¨åˆ°trash: %s", other_file)
                        group_details['actions'].append(f"[DRY RUN] å°†ç§»åŠ¨åˆ°trash: {other_file}")
        else:
            # åªæœ‰ä¸€ä¸ªéœ€è¦ä¿ç•™çš„ç‰ˆæœ¬
            logger.info("[#group_info] ğŸ” ç»„[%s]å¤„ç†: å‘ç°1ä¸ªéœ€è¦ä¿ç•™çš„ç‰ˆæœ¬ï¼Œä¿æŒåŸä½ç½®", group_base_name)
            group_details['actions'].append(f"ä¿ç•™å•ä¸ªæ±‰åŒ–ç‰ˆæœ¬: {chinese_versions[0]}")
            # ç§»åŠ¨å…¶ä»–ç‰ˆæœ¬åˆ°trash
            for other_file in other_versions:
                src_path = os.path.join(base_dir, other_file)
                rel_path = os.path.relpath(src_path, base_dir)
                dst_path = os.path.join(trash_dir, rel_path)
                if not dry_run:
                    logger.info("[#file_ops] ğŸ”„ æ­£åœ¨ç§»åŠ¨åˆ°trash: %s", other_file)
                    if create_shortcuts:
                        shortcut_path = os.path.splitext(dst_path)[0]
                        if create_shortcut(src_path, shortcut_path):
                            logger.info("[#file_ops] âœ… å·²åˆ›å»ºå¿«æ·æ–¹å¼: %s", other_file)
                            group_details['actions'].append(f"åˆ›å»ºå¿«æ·æ–¹å¼: {other_file}")
                            report_generator.update_stats('created_shortcuts')
                    else:
                        if safe_move_file(src_path, dst_path):
                            logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°trash: %s", other_file)
                            group_details['actions'].append(f"ç§»åŠ¨åˆ°trash: {other_file}")
                            report_generator.update_stats('moved_to_trash')
                else:
                    if create_shortcuts:
                        logger.info("[#file_ops] ğŸ”„ [DRY RUN] å°†åˆ›å»ºå¿«æ·æ–¹å¼: %s", other_file)
                        group_details['actions'].append(f"[DRY RUN] å°†åˆ›å»ºå¿«æ·æ–¹å¼: {other_file}")
                    else:
                        logger.info("[#file_ops] ğŸ”„ [DRY RUN] å°†ç§»åŠ¨åˆ°trash: %s", other_file)
                        group_details['actions'].append(f"[DRY RUN] å°†ç§»åŠ¨åˆ°trash: {other_file}")
    else:
        # æ²¡æœ‰æ±‰åŒ–ç‰ˆæœ¬çš„æƒ…å†µ
        if len(other_versions) > 1:
            # å¤šä¸ªåŸç‰ˆï¼Œç§»åŠ¨åˆ°multi
            multi_dir = os.path.join(base_dir, 'multi')
            if not dry_run:
                os.makedirs(multi_dir, exist_ok=True)
            for file in other_versions:
                src_path = os.path.join(base_dir, file)
                rel_path = os.path.relpath(src_path, base_dir)
                dst_path = os.path.join(multi_dir, rel_path)
                if not dry_run:
                    logger.info("[#file_ops] ğŸ”„ æ­£åœ¨ç§»åŠ¨åˆ°multi: %s", file)
                    if safe_move_file(src_path, dst_path):
                        logger.info("[#file_ops] âœ… å·²ç§»åŠ¨åˆ°multi: %s", file)
                        group_details['actions'].append(f"ç§»åŠ¨åˆ°multi: {file}")
                        report_generator.update_stats('moved_to_multi')
                else:
                    logger.info("[#file_ops] ğŸ”„ [DRY RUN] å°†ç§»åŠ¨åˆ°multi: %s", file)
                    group_details['actions'].append(f"[DRY RUN] å°†ç§»åŠ¨åˆ°multi: {file}")
            logger.info("[#group_info] ğŸ” ç»„[%s]å¤„ç†: æœªå‘ç°æ±‰åŒ–ç‰ˆæœ¬ï¼Œå‘ç°%dä¸ªåŸç‰ˆï¼Œå·²ç§»åŠ¨åˆ°multi", group_base_name, len(other_versions))
        else:
            # å•ä¸ªåŸç‰ˆï¼Œä¿æŒåŸä½ç½®
            logger.info("[#group_info] ğŸ” ç»„[%s]å¤„ç†: æœªå‘ç°æ±‰åŒ–ç‰ˆæœ¬ï¼Œä»…æœ‰1ä¸ªåŸç‰ˆï¼Œä¿æŒåŸä½ç½®", group_base_name)
            group_details['actions'].append(f"ä¿ç•™å•ä¸ªåŸç‰ˆ: {other_versions[0]}")
    
    # æ·»åŠ ç»„è¯¦æƒ…åˆ°æŠ¥å‘Š
    report_generator.add_group_detail(group_base_name, group_details)

def process_directory(directory: str, report_generator: ReportGenerator, dry_run: bool = False, create_shortcuts: bool = False) -> None:
    """å¤„ç†å•ä¸ªç›®å½•"""
    # åˆ›å»ºtrashç›®å½•
    trash_dir = os.path.join(directory, 'trash')
    if not dry_run:
        os.makedirs(trash_dir, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰å‹ç¼©æ–‡ä»¶
    all_files = []
    logger.info("[#process] ğŸ” æ­£åœ¨æ‰«ææ–‡ä»¶...")
    
    for root, _, files in os.walk(directory):
        # è·³è¿‡trashå’Œmultiç›®å½•
        if 'trash' in root or 'multi' in root:
            logger.info("[#file_ops] â­ï¸ è·³è¿‡ç›®å½•: %s", root)
            continue
            
        for file in files:
            # ä½¿ç”¨æ–°å®šä¹‰çš„ARCHIVE_EXTENSIONS
            if os.path.splitext(file.lower())[1] in ARCHIVE_EXTENSIONS:
                rel_path = os.path.relpath(os.path.join(root, file), directory)
                all_files.append(rel_path)
                # æ›´æ–°æ‰«æè¿›åº¦
                logger.info("[@process] æ‰«æè¿›åº¦: %d/%d", len(all_files), len(all_files))
    
    if not all_files:
        logger.info("[#error_log] âš ï¸ ç›®å½• %s ä¸­æœªæ‰¾åˆ°å‹ç¼©æ–‡ä»¶", directory)
        return
        
    # æ›´æ–°æŠ¥å‘Šç»Ÿè®¡
    report_generator.update_stats('total_files', len(all_files))
    
    # å¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç»„
    groups = group_similar_files(all_files)
    logger.info("[#stats] ğŸ“Š æ€»è®¡: %dä¸ªæ–‡ä»¶, %dä¸ªç»„", len(all_files), len(groups))
    
    # æ›´æ–°æŠ¥å‘Šç»Ÿè®¡
    report_generator.update_stats('total_groups', len(groups))
    
    # åˆ›å»ºè¿›ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
    logger.info("[#process] ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶ç»„...")
    
    with ThreadPoolExecutor(max_workers=min(os.cpu_count() * 2, 8)) as executor:
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        futures = []
        for _, group_files in groups.items():
            if len(group_files) > 1:  # åªå¤„ç†æœ‰å¤šä¸ªç‰ˆæœ¬çš„ç»„
                future = executor.submit(
                    process_file_group,
                    group_files,
                    directory,
                    trash_dir,
                    report_generator,
                    dry_run,
                    create_shortcuts
                )
                futures.append(future)
        
        # æ›´æ–°ç»„å¤„ç†è¿›åº¦
        completed = 0
        for _ in as_completed(futures):
            completed += 1
            future_count = len(futures)
            scan_percent = completed / future_count * 100
            
            logger.info("[@stats] ç»„è¿›åº¦: (%d/%d) %.2f%%", completed, future_count, scan_percent)

def get_paths_from_clipboard():
    """ä»å‰ªè´´æ¿è¯»å–å¤šè¡Œè·¯å¾„"""
    try:
        clipboard_content = pyperclip.paste()
        if not clipboard_content:
            return []
            
        # åˆ†å‰²å¤šè¡Œå†…å®¹å¹¶æ¸…ç†
        paths = [
            path.strip().strip('"').strip("'")
            for path in clipboard_content.splitlines() 
            if path.strip()
        ]
        
        # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
        valid_paths = [
            path for path in paths 
            if os.path.exists(path)
        ]
        
        if valid_paths:
            logger.info("[#file_ops] ğŸ“‹ ä»å‰ªè´´æ¿è¯»å–åˆ° %d ä¸ªæœ‰æ•ˆè·¯å¾„", len(valid_paths))
        else:
            logger.info("[#error_log] âš ï¸ å‰ªè´´æ¿ä¸­æ²¡æœ‰æœ‰æ•ˆè·¯å¾„")
            
        return valid_paths
        
    except Exception as e:
        logger.info("[#error_log] âŒ è¯»å–å‰ªè´´æ¿æ—¶å‡ºé”™: %s", e)
        return []

def get_long_path_name(path_str: str) -> str:
    """è½¬æ¢ä¸ºé•¿è·¯å¾„æ ¼å¼"""
    if not path_str.startswith("\\\\?\\"):
        if os.path.isabs(path_str):
            return "\\\\?\\" + path_str
    return path_str

def safe_path(path: str) -> str:
    """ç¡®ä¿è·¯å¾„æ”¯æŒé•¿æ–‡ä»¶å"""
    try:
        abs_path = os.path.abspath(path)
        return get_long_path_name(abs_path)
    except Exception as e:
        # add_error_log(f"âŒ å¤„ç†è·¯å¾„æ—¶å‡ºé”™ {path}: {e}")
        return path

def process_paths(paths: List[str]) -> List[str]:
    """å¤„ç†è¾“å…¥çš„è·¯å¾„åˆ—è¡¨"""
    TextualLoggerManager.set_layout(TEXTUAL_LAYOUT,config_info['log_file'])

    # è¿‡æ»¤æ‰ç©ºè·¯å¾„å’Œå¼•å·
    valid_paths = []
    for path in paths:
        # ç§»é™¤è·¯å¾„ä¸¤ç«¯çš„å¼•å·å’Œç©ºç™½å­—ç¬¦
        path = path.strip()
        if path.startswith('"') and path.endswith('"'):
            path = path[1:-1]
        elif path.startswith("'") and path.endswith("'"):
            path = path[1:-1]
        
        if path:
            # å°è¯•è½¬æ¢è·¯å¾„ç¼–ç 
            try:
                # ä½¿ç”¨å®‰å…¨çš„è·¯å¾„å¤„ç†
                safe_path_str = safe_path(path)
                if os.path.exists(safe_path_str):
                    valid_paths.append(safe_path_str)
                else:
                    logger.info("[#error_log] âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: %s", path)
            except Exception as e:
                logger.info("[#error_log] âŒ å¤„ç†è·¯å¾„æ—¶å‡ºé”™: %s, é”™è¯¯: %s", path, str(e))
    
    if not valid_paths:
        logger.info("[#error_log] âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„è·¯å¾„")
        
    return valid_paths

def create_shortcut(src_path: str, dst_path: str) -> bool:
    """åˆ›å»ºæŒ‡å‘æºæ–‡ä»¶çš„å¿«æ·æ–¹å¼"""
    try:
        shell = win32com.client.Dispatch("WScript.Shell")
        shortcut = shell.CreateShortCut(dst_path + ".lnk")
        shortcut.Targetpath = src_path
        shortcut.save()
        return True
    except Exception as e:
        logger.error("[#error_log] åˆ›å»ºå¿«æ·æ–¹å¼å¤±è´¥: %s", str(e))
        return False

def main():

    
    parser = argparse.ArgumentParser(description='å¤„ç†é‡å¤å‹ç¼©åŒ…æ–‡ä»¶')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('-c', '--clipboard', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
    group.add_argument('-p', '--paths', nargs='+', help='è¦å¤„ç†çš„ç›®å½•è·¯å¾„')
    parser.add_argument('-s', '--sample-count', type=int, default=3, help='æ¯ä¸ªå‹ç¼©åŒ…æŠ½å–çš„å›¾ç‰‡æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤3ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='é¢„æ¼”æ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶')
    parser.add_argument('--create-shortcuts', action='store_true', help='åœ¨dryrunæ¨¡å¼ä¸‹åˆ›å»ºå¿«æ·æ–¹å¼è€Œä¸æ˜¯ç§»åŠ¨æ–‡ä»¶')
    parser.add_argument('--report', type=str, help='æŒ‡å®šæŠ¥å‘Šæ–‡ä»¶åï¼ˆé»˜è®¤ä¸º"å¤„ç†æŠ¥å‘Š_æ—¶é—´æˆ³.md"ï¼‰')
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—

        # è·å–è¦å¤„ç†çš„è·¯å¾„
    paths = []
    
    # ä»å‰ªè´´æ¿è¯»å–
    if args.clipboard:
        paths.extend(get_paths_from_clipboard())
    # ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–
    elif args.paths:
        paths.extend(args.paths)
    # é»˜è®¤ä»ç»ˆç«¯è¾“å…¥
    else:
        # ä½¿ç”¨æ™®é€šinputæç¤ºï¼Œä¸ä½¿ç”¨æ—¥å¿—é¢æ¿
        print("è¯·è¾“å…¥è¦å¤„ç†çš„è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
        while True:
            try:
                line = input().strip()
                if not line:  # ç©ºè¡Œç»“æŸè¾“å…¥
                    break
                paths.append(line)
            except EOFError:
                break
            except KeyboardInterrupt:
                print("ç”¨æˆ·å–æ¶ˆè¾“å…¥")
                return
        
    if not paths:
        logger.info("[#error_log] âŒ æœªæä¾›ä»»ä½•è·¯å¾„")
        return
        
    # å¤„ç†å’ŒéªŒè¯æ‰€æœ‰è·¯å¾„
    valid_paths = process_paths(paths)
    
    if not valid_paths:
        logger.info("[#error_log] âŒ æ²¡æœ‰æœ‰æ•ˆçš„è·¯å¾„å¯å¤„ç†")
        return
    
    # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
    report_generator = ReportGenerator()
    
    # å¤„ç†æ¯ä¸ªè·¯å¾„
    for path in valid_paths:
        logger.info("[#process] ğŸš€ å¼€å§‹å¤„ç†ç›®å½•: %s", path)
        process_directory(path, report_generator, args.dry_run, args.create_shortcuts)
        logger.info("[#process] âœ¨ ç›®å½•å¤„ç†å®Œæˆ: %s", path)
        
        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        if args.report:
            report_path = report_generator.save_report(path, args.report)
        else:
            report_path = report_generator.save_report(path)
            
        if report_path:
            logger.info("[#process] ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: %s", report_path)
        else:
            logger.info("[#error_log] âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥")

if __name__ == "__main__":
    main() 