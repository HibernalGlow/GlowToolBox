import os
import uuid
import yaml
import time
import subprocess
import difflib
from pathlib import Path
from nanoid import generate
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse
import pyperclip
import sys
import threading
import logging
from datetime import datetime
from colorama import init, Fore, Style
from logging.handlers import RotatingFileHandler
import win32file
import win32con
import shutil
import logging
import numpy as np
import yaml as yaml_c
from rich.progress import Progress, BarColumn, TextColumn
# å¯¼å…¥è‡ªå®šä¹‰æ—¥å¿—æ¨¡å—
# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from tui.config import create_config_app
# ================= æ—¥å¿—é…ç½® =================
script_name = os.path.basename(__file__).replace('.py', '')
logspath=r"D:/1VSCODE/1ehv/logs"
LOG_BASE_DIR = Path(logspath + f"/{script_name}")
DATE_STR = datetime.now().strftime("%Y%m%d")
HOUR_STR = datetime.now().strftime("%H")  # æ–°å¢å°æ—¶ç›®å½•
LOG_DIR = LOG_BASE_DIR / DATE_STR / HOUR_STR  # ä¿®æ”¹ç›®å½•ç»“æ„
LOG_FILE = LOG_DIR / f"{datetime.now().strftime('%M%S')}.log"  # æ–‡ä»¶ååªä¿ç•™åˆ†ç§’

# åˆ›å»ºæ—¥å¿—ç›®å½•
LOG_DIR.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥å¿—æ ¼å¼
LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"
formatter = logging.Formatter(LOG_FORMAT)

# æ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setFormatter(formatter)
file_handler.setLevel(logging.DEBUG)

# æ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
console_handler.setLevel(logging.INFO)

# ä¸»æ—¥å¿—å™¨é…ç½®
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ç¦ç”¨ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—
logging.getLogger("PIL").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
# åˆå§‹åŒ– colorama
init()

# å®šä¹‰æ–‡ä»¶è·¯å¾„å’Œçº¿ç¨‹é”
# uuid_file_path = r'E:\1BACKUP\ehv\uuid.md'  # å­˜å‚¨å”¯ä¸€ UUID çš„ Markdown æ–‡ä»¶
uuid_lock = threading.Lock()  # ç”¨äºä¿æŠ¤UUIDæ–‡ä»¶æ“ä½œçš„çº¿ç¨‹é”

class FastUUIDLoader:
    def __init__(self, yaml_path):
        self.yaml_path = yaml_path
        self.cache_path = os.path.splitext(yaml_path)[0] + '.cache'
        self._data = None
        self._index = {}  # åˆå§‹åŒ–ç´¢å¼•å­—å…¸
        self._lock = threading.Lock()
        
        # åˆå§‹åŒ–è¿›åº¦å±æ€§
        self.progress = {
            'total_steps': 4,
            'current_step': 0,
            'message': 'åˆå§‹åŒ–ä¸­',
            'percentage': 0.0,
            'timestamp': time.time()
        }
        
        # è‡ªåŠ¨æ£€æµ‹å¹¶ç”Ÿæˆä¼˜åŒ–æ ¼å¼
        if not self._check_cache_valid():
            self._build_cache()
    
    def _check_cache_valid(self):
        """æ ¡éªŒç¼“å­˜æœ‰æ•ˆæ€§"""
        if not os.path.exists(self.cache_path):
            return False
        # æ ¡éªŒæ—¶é—´æˆ³å’Œå¤§å°
        yaml_mtime = os.path.getmtime(self.yaml_path)
        cache_mtime = os.path.getmtime(self.cache_path)
        return yaml_mtime <= cache_mtime
    
    def _build_cache(self):
        """ä¿®å¤ç´¢å¼•åˆå§‹åŒ–é—®é¢˜"""
        with self._lock:
            try:
                # åˆå§‹åŒ–ç´¢å¼•
                self._index = {}
                
                # å¢åŠ é˜¶æ®µæ ‡è¯†
                self._update_progress("å¼€å§‹è§£æYAMLæ–‡ä»¶", 5)
                
                # ä½¿ç”¨æ›´é«˜æ•ˆçš„è§£ææ–¹å¼
                with open(self.yaml_path, 'rb') as f:
                    data = yaml.load(f, Loader=yaml.CSafeLoader)
                
                # æ·»åŠ æ•°æ®æ ¡éªŒ
                if not data or not isinstance(data, list):
                    raise ValueError("æ— æ•ˆçš„YAMLæ•°æ®æ ¼å¼")
                
                self._update_progress("æ•°æ®æ ¡éªŒé€šè¿‡", 20)
                
                # åˆ†æ‰¹æ¬¡æ„å»ºç´¢å¼•
                batch_size = 5000
                total = len(data)
                self._update_progress("å¼€å§‹æ„å»ºç´¢å¼•", 30)
                
                for i in range(0, total, batch_size):
                    batch = data[i:i+batch_size]
                    for j, record in enumerate(batch):
                        uuid = record.get('UUID')
                        if uuid:
                            self._index[uuid] = i + j
                    # å®æ—¶æ›´æ–°è¿›åº¦
                    progress = 30 + 60 * (i + batch_size) / total
                    self._update_progress(f"ç´¢å¼•æ„å»ºä¸­ ({i+batch_size}/{total})", min(90, progress))
                
                self._update_progress("å†™å…¥ç¼“å­˜æ–‡ä»¶", 95)
                with open(self.cache_path, 'wb') as f:
                    np.savez_compressed(f, data=data, index=self._index)
                
                self._update_progress("å®Œæˆ", 100)
                
            except Exception as e:
                self._index = None  # æ„å»ºå¤±è´¥æ—¶é‡ç½®ç´¢å¼•
                raise

    def _update_progress(self, message, percentage):
        """æ›´æ–°è¿›åº¦ä¿¡æ¯"""
        self.progress.update({
            'message': message,
            'percentage': min(100, max(0, percentage)),
            'timestamp': time.time()
        })
        logger.info(f"[ç¼“å­˜æ„å»º] {message} - è¿›åº¦: {self.progress['percentage']:.1f}%")

    def get_loading_progress(self):
        """è·å–å½“å‰åŠ è½½è¿›åº¦"""
        return self.progress
    
    def _load_cache(self):
        """åŠ è½½ä¼˜åŒ–æ ¼å¼"""
        with open(self.cache_path, 'rb') as f:
            cache = np.load(f, allow_pickle=True)
            self._data = cache['data'].tolist()
            self._index = cache['index'].item() if 'index' in cache else {}
    
    def get_uuids(self):
        """è·å–UUIDé›†åˆ"""
        if self._index is None:
            self._load_cache()
        return set(self._index.keys())
    
    def get_record(self, uuid):
        """å¿«é€ŸæŸ¥è¯¢è®°å½•"""
        if self._index is None:
            self._load_cache()
        return self._data[self._index[uuid]]

def repair_uuid_records(uuid_record_path):
    """ä¿®å¤æŸåçš„UUIDè®°å½•æ–‡ä»¶ã€‚"""
    backup_path = f"{uuid_record_path}.bak"
    
    # å¦‚æœå­˜åœ¨å¤‡ä»½æ–‡ä»¶ï¼Œå°è¯•ä»å¤‡ä»½æ¢å¤
    if os.path.exists(backup_path):
        try:
            with open(backup_path, 'r', encoding='utf-8') as file:
                records = yaml.safe_load(file) or []
                if isinstance(records, list):
                    return records
        except Exception:
            pass
    
    # å°è¯•ä¿®å¤åŸæ–‡ä»¶
    try:
        with open(uuid_record_path, 'r', encoding='utf-8', errors='ignore') as file:
            content = file.read()
            # å°è¯•è§£ææ¯ä¸ªè®°å½•
            records = []
            current_record = {}
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    if current_record:
                        records.append(current_record)
                        current_record = {}
                    continue
                
                if line.startswith('- ') or line.startswith('UUID:'):
                    if current_record:
                        records.append(current_record)
                    current_record = {}
                
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip('- ').strip()
                    value = value.strip()
                    if key and value:
                        current_record[key] = value
            
            if current_record:
                records.append(current_record)
            
            # éªŒè¯è®°å½•
            valid_records = []
            for record in records:
                if 'UUID' in record:
                    valid_records.append(record)
            
            return valid_records
    except Exception as e:
        print(f"ä¿®å¤UUIDè®°å½•æ–‡ä»¶å¤±è´¥: {e}")
        return []

def load_existing_uuids():
    """æ·»åŠ è¶…æ—¶æœºåˆ¶çš„åŠ è½½å‡½æ•°"""
    logger.info("ğŸ” å¼€å§‹åŠ è½½ç°æœ‰UUID...")
    start_time = time.time()
    loader = FastUUIDLoader(r'E:\1BACKUP\ehv\uuid\uuid_records.yaml')
    
    # è¶…æ—¶è®¾ç½®ï¼ˆ5åˆ†é’Ÿï¼‰
    timeout = 300  
    last_percent = 0
    
    while True:
        if time.time() - start_time > timeout:
            raise TimeoutError(f"UUIDåŠ è½½è¶…æ—¶ï¼Œå·²ç­‰å¾…{timeout}ç§’")
            
        progress = loader.get_loading_progress()
        
        # è¿›åº¦ç›‘æ§
        if progress['percentage'] != last_percent:
            logger.info(f"â³ {progress['message']} [{progress['percentage']:.1f}%]")
            last_percent = progress['percentage']
            
        if progress['percentage'] >= 100:
            if progress['message'].startswith("æ„å»ºå¤±è´¥"):
                raise RuntimeError("ç¼“å­˜æ„å»ºå¤±è´¥")
            break
            
        # åŠ¨æ€è°ƒæ•´è½®è¯¢é—´éš”
        sleep_time = 0.5 if progress['percentage'] < 50 else 0.1
        time.sleep(sleep_time)
    
    uuids = loader.get_uuids()
    elapsed = time.time() - start_time
    logger.info(f"âœ… åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(uuids)} ä¸ªUUIDï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
    return uuids

def add_uuid_to_file(uuid, timestamp, archive_name, artist_name, relative_path=None):
    """å°†ç”Ÿæˆçš„ UUID æ·»åŠ åˆ°è®°å½•æ–‡ä»¶ä¸­ã€‚"""
    uuid_record_path = r'E:\1BACKUP\ehv\uuid\uuid_records.yaml'
    os.makedirs(os.path.dirname(uuid_record_path), exist_ok=True)
    
    # è¯»å–ç°æœ‰è®°å½•
    records = []
    if os.path.exists(uuid_record_path):
        try:
            with open(uuid_record_path, 'r', encoding='utf-8') as file:
                records = yaml.safe_load(file) or []
        except Exception as e:
            print(f"è¯»å–è®°å½•æ–‡ä»¶å¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
            records = repair_uuid_records(uuid_record_path) or []
    
    # æ·»åŠ æ–°è®°å½•
    record = {
        'UUID': uuid,
        'CreatedAt': timestamp,
        'ArchiveName': archive_name,
        'ArtistName': artist_name,
        'LastModified': timestamp,
        'LastPath': relative_path or os.path.join(artist_name, archive_name) if artist_name else archive_name
    }
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥UUIDçš„è®°å½•
    for existing_record in records:
        if existing_record.get('UUID') == uuid:  # ä½¿ç”¨get()é¿å…KeyError
            existing_record.update({
                'LastModified': timestamp,
                'ArchiveName': archive_name,
                'ArtistName': artist_name,
                'LastPath': relative_path or os.path.join(artist_name, archive_name) if artist_name else archive_name
            })
            break
    else:
        records.append(record)
    
    # å†™å…¥è®°å½•ï¼ˆä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿å¹¶å‘å®‰å…¨ï¼‰
    with uuid_lock:
        try:
            # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_path = f"{uuid_record_path}.tmp"
            with open(temp_path, 'w', encoding='utf-8') as file:
                yaml.dump(records, file, allow_unicode=True, sort_keys=False)
            
            # éªŒè¯ä¸´æ—¶æ–‡ä»¶
            with open(temp_path, 'r', encoding='utf-8') as file:
                yaml.safe_load(file)
            
            # åˆ›å»ºå¤‡ä»½
            if os.path.exists(uuid_record_path):
                backup_path = f"{uuid_record_path}.bak"
                shutil.copy2(uuid_record_path, backup_path)
            
            # æ›¿æ¢åŸæ–‡ä»¶
            os.replace(temp_path, uuid_record_path)
            
        except Exception as e:
            print(f"å†™å…¥UUIDè®°å½•æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            if os.path.exists(temp_path):
                os.remove(temp_path)

def generate_uuid(existing_uuids):
    """ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ 16 ä½ UUIDã€‚"""
    while True:
        new_uuid = generate(size=16)  # ç”Ÿæˆ 16 ä½çš„ UUID
        if new_uuid not in existing_uuids:
            return new_uuid

def get_artist_name(target_directory, archive_path):
    """ä»å‹ç¼©æ–‡ä»¶è·¯å¾„ä¸­æå–è‰ºæœ¯å®¶åç§°ã€‚"""
    archive_path = Path(archive_path)
    relative_path = archive_path.relative_to(target_directory).parts
    return relative_path[0] if len(relative_path) > 0 else ""

def get_relative_path(target_directory, archive_path):
    """è·å–ç›¸å¯¹è·¯å¾„ã€‚"""
    return Path(archive_path).relative_to(target_directory).parent.as_posix()

def repair_yaml_file(yaml_path):
    """ä¿®å¤æŸåçš„YAMLæ–‡ä»¶ã€‚"""
    try:
        with open(yaml_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()

        if not lines:
            return []

        valid_data = []
        current_record = []
        
        for line in lines:
            current_record.append(line)
            if line.strip() == '' or line == lines[-1]:
                try:
                    record_str = ''.join(current_record)
                    parsed_data = yaml.safe_load(record_str)
                    if isinstance(parsed_data, list):
                        valid_data.extend(parsed_data)
                    elif parsed_data is not None:
                        valid_data.append(parsed_data)
                except yaml.YAMLError:
                    pass
                current_record = []

        if not valid_data:
            return []

        write_yaml(yaml_path, valid_data)
        return valid_data

    except Exception as e:
        print(f"ä¿®å¤YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
        return []

def read_yaml(yaml_path):
    """è¯»å–YAMLæ–‡ä»¶å†…å®¹ï¼Œå¦‚æœæ–‡ä»¶æŸååˆ™å°è¯•ä¿®å¤ã€‚"""
    if not os.path.exists(yaml_path):
        return []
        
    try:
        with open(yaml_path, 'r', encoding='utf-8') as file:
            data = yaml.safe_load(file)
            if not isinstance(data, list):
                data = [data] if data is not None else []
            return data
    except yaml.YAMLError as e:
        print(f"YAMLæ–‡ä»¶ {yaml_path} å·²æŸåï¼Œå°è¯•ä¿®å¤...")
        return repair_yaml_file(yaml_path)
    except Exception as e:
        print(f"è¯»å–YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
        return []

def write_yaml(yaml_path, data):
    """å°†æ•°æ®å†™å…¥YAMLæ–‡ä»¶ï¼Œç¡®ä¿å†™å…¥å®Œæ•´æ€§ã€‚"""
    temp_path = yaml_path + '.tmp'
    try:
        with open(temp_path, 'w', encoding='utf-8') as file:
            yaml.dump(data, file, allow_unicode=True)
        
        try:
            with open(temp_path, 'r', encoding='utf-8') as file:
                yaml.safe_load(file)
        except yaml.YAMLError:
            print(f"å†™å…¥çš„YAMLæ–‡ä»¶éªŒè¯å¤±è´¥: {yaml_path}")
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False
            
        if os.path.exists(yaml_path):
            os.replace(temp_path, yaml_path)
        else:
            os.rename(temp_path, yaml_path)
        return True
            
    except Exception as e:
        print(f"å†™å…¥YAMLæ–‡ä»¶æ—¶å‡ºé”™ {yaml_path}: {e}")
        if os.path.exists(temp_path):
            os.remove(temp_path)
        return False

def get_uuid_path(uuid_directory, timestamp):
    """æ ¹æ®æ—¶é—´æˆ³ç”ŸæˆæŒ‰å¹´æœˆæ—¥åˆ†å±‚çš„UUIDæ–‡ä»¶è·¯å¾„ã€‚"""
    date = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
    year = str(date.year)
    month = f"{date.month:02d}"
    day = f"{date.day:02d}"
    
    # åˆ›å»ºå¹´æœˆæ—¥å±‚çº§ç›®å½•
    year_dir = os.path.join(uuid_directory, year)
    month_dir = os.path.join(year_dir, month)
    day_dir = os.path.join(month_dir, day)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(day_dir, exist_ok=True)
    
    return day_dir

def create_yaml(yaml_path, artist_name, archive_name, relative_path, timestamp, uuid):
    """åˆ›å»ºæ–°çš„YAMLç»“æ„ï¼Œå¹¶å†™å…¥æ–‡ä»¶ã€‚"""
    # yaml_pathå·²ç»æ˜¯å®Œæ•´è·¯å¾„ï¼Œä¸éœ€è¦å†æ¬¡è·å–æ—¥æœŸè·¯å¾„
    data = [{
        'UUID': uuid,
        'Timestamp': timestamp,
        'ArtistName': artist_name,
        'ArchiveName': archive_name,
        'RelativePath': relative_path
    }]
    write_yaml(yaml_path, data)

def normalize_filename(filename):
    """æ ‡å‡†åŒ–æ–‡ä»¶åï¼Œç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦çš„å½±å“"""
    normalized = ' '.join(filename.split())
    normalized = normalized.replace('_1', '').replace('_2', '').strip()
    return normalized

def update_yaml(yaml_path, artist_name, archive_name, relative_path, timestamp):
    """æ›´æ–°å·²æœ‰çš„YAMLæ–‡ä»¶ï¼Œè®°å½•æ—¶é—´æˆ³å’Œä¿¡æ¯çš„å˜åŒ–ã€‚"""
    data = read_yaml(yaml_path)

    if not data:
        new_uuid = generate_uuid(load_existing_uuids())
        create_yaml(yaml_path, artist_name, archive_name, relative_path, timestamp, new_uuid)
        logging.info(f"âœ¨ åˆ›å»ºæ–°çš„YAMLè®°å½• [UUID: {new_uuid}]")
        return False

    if not isinstance(data, list) or not all(isinstance(record, dict) for record in data):
        raise ValueError("Invalid YAML format. Expected a list of dictionaries.")

    current_artist = None
    current_archive = None
    current_path = None
    
    for record in reversed(data):
        if current_artist is None and 'ArtistName' in record:
            current_artist = record['ArtistName']
        if current_archive is None and 'ArchiveName' in record:
            current_archive = record['ArchiveName']
        if current_path is None and 'RelativePath' in record:
            current_path = record['RelativePath']
        if current_artist is not None and current_archive is not None and current_path is not None:
            break

    normalized_current = normalize_filename(current_archive) if current_archive else None
    normalized_new = normalize_filename(archive_name)
    
    changes = []
    changes_data = {}
    
    if artist_name != current_artist:
        curr = current_artist or 'æ— '
        changes.append(f"è‰ºæœ¯å®¶:{curr} -> {artist_name}")
        changes_data['ArtistName'] = artist_name

    if normalized_current != normalized_new:
        changes.append(f"æ–‡ä»¶å: {current_archive} -> {archive_name}")
        changes_data['ArchiveName'] = archive_name

    if relative_path != current_path:
        changes.append(f"è·¯å¾„: {current_path} -> {relative_path}")
        changes_data['RelativePath'] = relative_path
    
    if changes:
        logging.info(f"ğŸ“ {os.path.basename(archive_name)}\n    " + "\n    ".join(changes))

    if not changes_data:
        logging.info("âœ“ æœªæ£€æµ‹åˆ°å˜åŒ–")
        return False

    logging.info(f"ğŸ”„ æ£€æµ‹åˆ°å˜åŒ–ï¼Œæ·»åŠ æ–°è®°å½•...")
    new_record = {
        'Timestamp': timestamp,
        **changes_data
    }

    data.append(new_record)
    write_yaml(yaml_path, data)
    logging.info("âœ… æˆåŠŸæ›´æ–°YAMLæ–‡ä»¶")
    return True

def add_yaml_to_zip(yaml_path, archive_path):
    """å°†YAMLæ–‡ä»¶æ·»åŠ åˆ°å‹ç¼©åŒ…ä¸­ï¼Œå¹¶ä¿ç•™å‹ç¼©åŒ…å’Œæ–‡ä»¶å¤¹çš„æ—¶é—´æˆ³ã€‚"""
    original_stat_archive = os.stat(archive_path)
    archive_folder_path = os.path.dirname(archive_path)
    original_stat_folder = os.stat(archive_folder_path)

    subprocess.run(['7z.exe', 'u', archive_path, yaml_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)

    os.utime(archive_path, (original_stat_archive.st_atime, original_stat_archive.st_mtime))
    os.utime(archive_folder_path, (original_stat_folder.st_atime, original_stat_folder.st_mtime))

def process_single_archive(archive_path, target_directory, uuid_directory, timestamp):
    """å¤„ç†å•ä¸ªå‹ç¼©æ–‡ä»¶çš„é€»è¾‘ã€‚"""
    try:
        yaml_uuid = load_yaml_uuid_from_archive(archive_path)
        artist_name = get_artist_name(target_directory, archive_path)
        archive_name = os.path.basename(archive_path)
        relative_path = get_relative_path(target_directory, archive_path)
        
        if yaml_uuid:
            yaml_filename = f"{yaml_uuid}.yaml"
            # æ›´æ–°ç°æœ‰UUIDçš„è®°å½•
            add_uuid_to_file(yaml_uuid, timestamp, archive_name, artist_name, relative_path)
        else:
            new_uuid = generate_uuid(load_existing_uuids())
            yaml_filename = f"{new_uuid}.yaml"
            # æ·»åŠ æ–°UUIDçš„è®°å½•
            add_uuid_to_file(new_uuid, timestamp, archive_name, artist_name, relative_path)
            yaml_uuid = new_uuid

        # è·å–æŒ‰å¹´æœˆæ—¥åˆ†å±‚çš„ç›®å½•è·¯å¾„
        day_dir = get_uuid_path(uuid_directory, timestamp)
        yaml_path = os.path.join(day_dir, yaml_filename)
        
        if os.path.exists(yaml_path):
            updated = update_yaml(yaml_path, artist_name, archive_name, relative_path, timestamp)
            if not updated:
                logging.info(f"â­ï¸ è·³è¿‡æ›´æ–°: {archive_name}")
                return False
        else:
            create_yaml(yaml_path, artist_name, archive_name, relative_path, timestamp, yaml_uuid)

        # ç¡®ä¿yamlæ–‡ä»¶å­˜åœ¨åå†æ·»åŠ åˆ°å‹ç¼©åŒ…
        if os.path.exists(yaml_path):
            try:
                add_yaml_to_zip(yaml_path, archive_path)
                logging.info(f"âœ… å·²æ·»åŠ YAMLåˆ°å‹ç¼©åŒ…: {archive_name}")
            except Exception as e:
                logging.error(f"æ·»åŠ YAMLåˆ°å‹ç¼©åŒ…å¤±è´¥: {archive_name} - {str(e)}")
        else:
            logging.error(f"YAMLæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ·»åŠ åˆ°å‹ç¼©åŒ…: {archive_name}")
            
        return True

    except subprocess.CalledProcessError:
        logging.error(f"å‘ç°æŸåçš„å‹ç¼©åŒ…: {archive_path}")
        return True
    except Exception as e:
        logging.info(f"å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™ {archive_path}: {str(e)}")
        return True  # é”™è¯¯æƒ…å†µä¸è®¡å…¥è·³è¿‡æ¬¡æ•°

def warm_up_cache(target_directory, max_workers=32, handler=None):
    """å¹¶è¡Œé¢„çƒ­ç³»ç»Ÿç¼“å­˜"""
    return _warm_up_cache_internal(target_directory, max_workers)

def _warm_up_cache_internal(target_directory, max_workers):
    """é¢„çƒ­ç¼“å­˜çš„å†…éƒ¨å®ç°"""
    logging.info("ğŸ”„ å¼€å§‹é¢„çƒ­ç³»ç»Ÿç¼“å­˜")
    
    # é¦–å…ˆè®¡ç®—æ€»æ–‡ä»¶æ•°
    total_files = 0
    for root, _, files in os.walk(target_directory):
        total_files += sum(1 for file in files if file.endswith(('.zip', '.rar', '.7z')))
    
    scan_task = logging.info("æ‰«ææ–‡ä»¶")
    archive_files = []
    current_count = 0
    for root, _, files in os.walk(target_directory):
        for file in files:
            if file.endswith(('.zip', '.rar', '.7z')):
                archive_files.append(os.path.join(root, file))
                current_count += 1
                logging.info("å·²æ‰«æ %d ä¸ªæ–‡ä»¶", current_count)
    
    logging.info(f"ğŸ“Š æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶å¾…é¢„çƒ­")
    
    warm_task = logging.info("é¢„çƒ­ç¼“å­˜")
    
    def read_file_header_with_progress(file_path):
        try:
            # ä½¿ç”¨Windows APIç›´æ¥æ‰“å¼€æ–‡ä»¶
            handle = win32file.CreateFile(
                file_path,
                win32con.GENERIC_READ,
                win32con.FILE_SHARE_READ | win32con.FILE_SHARE_WRITE,
                None,
                win32con.OPEN_EXISTING,
                win32con.FILE_FLAG_SEQUENTIAL_SCAN,  # æç¤ºç³»ç»Ÿè¿™æ˜¯é¡ºåºè¯»å–
                None
            )
            try:
                # è¯»å–æ–‡ä»¶å¤´éƒ¨
                win32file.ReadFile(handle, 32)
            finally:
                handle.Close()
            logging.info(f"âœ… å·²é¢„çƒ­: {os.path.basename(file_path)}")
        except Exception as e:
            logging.info(f"é¢„çƒ­å¤±è´¥: {os.path.basename(file_path)} - {str(e)}")
        finally:
            logging.info( advance=1)

    # ä½¿ç”¨æ›´å¤šçº¿ç¨‹
    with ThreadPoolExecutor(max_workers=128) as executor:
        executor.map(read_file_header_with_progress, archive_files)
        
    logging.info("âœ¨ ç¼“å­˜é¢„çƒ­å®Œæˆ")

def process_archives(target_directory, max_workers=5, handler=None):
    """éå†ç›®å½•ä¸­çš„å‹ç¼©æ–‡ä»¶ï¼Œç”Ÿæˆæˆ–æ›´æ–°YAMLæ–‡ä»¶ã€‚"""
    if handler is None:
        return _process_archives_internal(target_directory, max_workers)
    else:
        return _process_archives_internal(target_directory, max_workers)

def _process_archives_internal(target_directory, max_workers):
    """å¤„ç†å‹ç¼©æ–‡ä»¶çš„å†…éƒ¨å®ç°"""
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    uuid_directory = r'E:\1BACKUP\ehv\uuid'
    os.makedirs(uuid_directory, exist_ok=True)

    logging.info("ğŸ” å¼€å§‹æ‰«æå‹ç¼©æ–‡ä»¶")
    
    scan_task = logging.info("æ‰«ææ–‡ä»¶")
    
    archive_files = []
    file_count = 0
    for root, _, files in os.walk(target_directory):
        for file in files:
            if file.endswith(('.zip', '.rar', '.7z')):
                full_path = os.path.join(root, file)
                archive_files.append((full_path, os.path.getmtime(full_path)))
                file_count += 1
                logging.info(f"å·²æ‰«æ {file_count} ä¸ªæ–‡ä»¶")
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    archive_files.sort(key=lambda x: x[1], reverse=True)
    archive_files = [file_path for file_path, _ in archive_files]
    
    logging.info(f"ğŸ“Š å…±å‘ç° {file_count} ä¸ªå‹ç¼©æ–‡ä»¶")
    
    # åŠ è½½ç°æœ‰UUID
    logging.info("ğŸ’¾ æ­£åœ¨åŠ è½½ç°æœ‰UUID...")
    existing_uuids = load_existing_uuids()
    logging.info(f"ğŸ“ å·²åŠ è½½ {len(existing_uuids)} ä¸ªç°æœ‰UUID")
    
    process_task = logging.info("å¤„ç†å‹ç¼©æ–‡ä»¶")
    
    # æ·»åŠ è·³è¿‡è®¡æ•°å™¨
    skip_count = 0
    
    def process_with_progress(archive_path):
        nonlocal skip_count
        try:
            start_time = time.time()
            result = process_single_archive(archive_path, target_directory, uuid_directory, timestamp)
            
            # è®°å½•å¤„ç†æ—¶é•¿
            duration = time.time() - start_time
            if duration > 30:
                logging.warning(f"â±ï¸ å¤„ç†æ—¶é—´è¿‡é•¿: {os.path.basename(archive_path)} è€—æ—¶{duration:.1f}ç§’")
            
            return result
        except Exception as e:
            logging.error(f"ğŸ”¥ ä¸¥é‡é”™è¯¯: {str(e)}")
            raise
    
    # ä¿®æ”¹ä»»åŠ¡åˆ†å‘æ–¹å¼
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ä½¿ç”¨æ‰¹é‡æäº¤ä»»åŠ¡
        batch_size = 100
        futures = []
        
        for i in range(0, len(archive_files), batch_size):
            batch = archive_files[i:i+batch_size]
            futures.extend(executor.submit(process_with_progress, path) for path in batch)
            
            # å®æ—¶æ˜¾ç¤ºæäº¤è¿›åº¦
            submitted = min(i + batch_size, len(archive_files))
            total_files = len(archive_files)
            logging.info(f"ğŸ—‚ï¸ å·²æäº¤ {submitted}/{total_files} ä¸ªæ–‡ä»¶åˆ°å¤„ç†é˜Ÿåˆ—")

        # æ·»åŠ è¶…æ—¶æœºåˆ¶
        for future in as_completed(futures, timeout=300):
            try:
                result = future.result(timeout=60)  # æ¯ä¸ªä»»åŠ¡æœ€å¤š60ç§’
                if result == "SKIP_LIMIT_REACHED":
                    logging.info("â© è¾¾åˆ°è·³è¿‡é™åˆ¶ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡...")
                    for f in futures:
                        f.cancel()
                    break
            except TimeoutError:
                logging.warning("âŒ› ä»»åŠ¡è¶…æ—¶ï¼Œå·²è·³è¿‡")
                skip_count += 1
            except Exception as e:
                logging.error(f"ä»»åŠ¡å¤±è´¥: {str(e)}")
                skip_count = 0

    if skip_count >= 100:
        logging.info("ğŸ”„ ç”±äºè¿ç»­è·³è¿‡æ¬¡æ•°è¾¾åˆ°100ï¼Œæå‰ç»“æŸå½“å‰é˜¶æ®µ")
    else:
        logging.info("âœ¨ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ")
    
    return skip_count >= 100  # è¿”å›æ˜¯å¦å› ä¸ºè·³è¿‡æ¬¡æ•°è¾¾åˆ°é™åˆ¶è€Œæå‰ç»“æŸ

def load_yaml_uuid_from_archive(archive_path):
    """å°è¯•ä»å‹ç¼©åŒ…å†…åŠ è½½ YAML æ–‡ä»¶ä»¥è·å– UUIDã€‚"""
    try:
        short_path = get_short_path(archive_path)
        
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            
        command = ['7z', 'l', short_path]
        result = subprocess.run(
            command, 
            capture_output=True, 
            text=True, 
            encoding='gbk',  # ä½¿ç”¨GBKç¼–ç 
            errors='ignore',  # å¿½ç•¥æ— æ³•è§£ç çš„å­—ç¬¦
            startupinfo=startupinfo,
            check=False
        )
        
        if result.returncode != 0:
            print(f"åˆ—å‡ºå‹ç¼©åŒ…å†…å®¹å¤±è´¥: {archive_path}")
            return None
            
        if result.stdout:
            for line in result.stdout.splitlines():
                if not line:
                    continue
                    
                line = line.strip()
                if line.endswith('.yaml'):
                    parts = line.split()
                    if parts:
                        yaml_filename = parts[-1]
                        yaml_uuid = os.path.splitext(yaml_filename)[0]
                        return yaml_uuid

    except Exception as e:
        print(f"æ— æ³•åŠ è½½å‹ç¼©åŒ…ä¸­çš„ YAML æ–‡ä»¶ ({archive_path}): {e}")
        
    return None

def get_short_path(long_path):
    """å°†é•¿è·¯å¾„è½¬æ¢ä¸ºçŸ­è·¯å¾„æ ¼å¼ã€‚"""
    try:
        import win32api
        return win32api.GetShortPathName(long_path)
    except ImportError:
        return long_path

def main():
    """ä¸»å‡½æ•°"""
    # å®šä¹‰å¤é€‰æ¡†é€‰é¡¹
    checkbox_options = [
        ("æ— ç”»å¸ˆæ¨¡å¼ - ä¸æ·»åŠ ç”»å¸ˆå", "no_artist", "--no-artist"),
        ("ä¿æŒæ—¶é—´æˆ³ - ä¿æŒæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´", "keep_timestamp", "--keep-timestamp", True),
        ("å¤šç”»å¸ˆæ¨¡å¼ - å¤„ç†æ•´ä¸ªç›®å½•", "multi_mode", "--mode multi"),
        ("å•ç”»å¸ˆæ¨¡å¼ - åªå¤„ç†å•ä¸ªç”»å¸ˆçš„æ–‡ä»¶å¤¹", "single_mode", "--mode single"),
        ("ä»å‰ªè´´æ¿è¯»å–è·¯å¾„", "clipboard", "-c", True),  # é»˜è®¤å¼€å¯
        ("è‡ªåŠ¨åºåˆ— - æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹", "auto_sequence", "-a"),  # æ·»åŠ åºåˆ—æ¨¡å¼é€‰é¡¹
        ("é‡ç»„UUID - æŒ‰æ—¶é—´é‡ç»„UUIDæ–‡ä»¶", "reorganize", "-r"),  # æ·»åŠ é‡ç»„é€‰é¡¹
        ("æ›´æ–°è®°å½• - æ›´æ–°UUIDè®°å½•æ–‡ä»¶", "update_records", "-u"),  # æ·»åŠ æ›´æ–°è®°å½•é€‰é¡¹
    ]

    # å®šä¹‰è¾“å…¥æ¡†é€‰é¡¹
    input_options = [
        ("è·¯å¾„", "path", "--path", "", "è¾“å…¥è¦å¤„ç†çš„è·¯å¾„ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤è·¯å¾„"),
    ]

    # é¢„è®¾é…ç½®
    preset_configs = {
        "æ ‡å‡†å¤šç”»å¸ˆ": {
            "description": "æ ‡å‡†å¤šç”»å¸ˆæ¨¡å¼ï¼Œä¼šæ·»åŠ ç”»å¸ˆå",
            "checkbox_options": ["keep_timestamp", "multi_mode", "clipboard"],
            "input_values": {"path": ""}
        },
        "æ ‡å‡†å•ç”»å¸ˆ": {
            "description": "æ ‡å‡†å•ç”»å¸ˆæ¨¡å¼ï¼Œä¼šæ·»åŠ ç”»å¸ˆå", 
            "checkbox_options": ["keep_timestamp", "single_mode", "clipboard"],
            "input_values": {"path": ""}
        },
        "æ— ç”»å¸ˆæ¨¡å¼": {
            "description": "ä¸æ·»åŠ ç”»å¸ˆåçš„é‡å‘½åæ¨¡å¼",
            "checkbox_options": ["no_artist", "keep_timestamp", "clipboard"],
            "input_values": {"path": ""}
        },
        "å®Œæ•´åºåˆ—": {
            "description": "æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹ï¼šUUID-YAML -> è‡ªåŠ¨æ–‡ä»¶å -> UUID-YAML",
            "checkbox_options": ["keep_timestamp", "clipboard", "auto_sequence"],
            "input_values": {"path": ""}
        },
        "UUIDæ›´æ–°": {
            "description": "é‡ç»„UUIDæ–‡ä»¶ç»“æ„å¹¶æ›´æ–°è®°å½•",
            "checkbox_options": ["reorganize", "update_records"],
            "input_values": {"path": ""}
        },
        "å®Œæ•´ç»´æŠ¤": {
            "description": "æ‰§è¡Œå®Œæ•´åºåˆ—å¹¶æ›´æ–°UUIDè®°å½•",
            "checkbox_options": ["keep_timestamp", "clipboard", "auto_sequence", "reorganize", "update_records"],
            "input_values": {"path": ""}
        }
    }

    # åˆ›å»ºå¹¶è¿è¡Œé…ç½®ç•Œé¢
    app = create_config_app(
        program=__file__,
        checkbox_options=checkbox_options,
        input_options=input_options,
        title="UUID-YAML å·¥å…·",
        preset_configs=preset_configs
    )
    app.run()

def reorganize_uuid_files(uuid_directory=r'E:\1BACKUP\ehv\uuid', handler=None):
    """æ ¹æ®æœ€åä¿®æ”¹æ—¶é—´é‡æ–°ç»„ç»‡ UUID æ–‡ä»¶çš„ç›®å½•ç»“æ„"""
    logging.info("ğŸ”„ å¼€å§‹é‡æ–°ç»„ç»‡ UUID æ–‡ä»¶...")
    
    # åŠ è½½è®°å½•æ–‡ä»¶
    uuid_record_path = os.path.join(uuid_directory, 'uuid_records.yaml')
    if not os.path.exists(uuid_record_path):
        logging.info("âŒ UUID è®°å½•æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        with open(uuid_record_path, 'r', encoding='utf-8') as file:
            records = yaml.safe_load(file) or []
    except Exception as e:
        logging.info(f"âŒ è¯»å–è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    
    # éå†æ‰€æœ‰è®°å½•
    for record in records:
        try:
            uuid = record.get('UUID')
            if not uuid:
                continue
                
            # è·å–æ—¶é—´æˆ³
            timestamp = record.get('LastModified') or record.get('CreatedAt')
            if not timestamp:
                continue
            
            # æŸ¥æ‰¾å½“å‰ UUID çš„ YAML æ–‡ä»¶
            yaml_found = False
            current_yaml_path = None
            
            # åœ¨ç›®å½•ç»“æ„ä¸­æŸ¥æ‰¾ç°æœ‰çš„ YAML æ–‡ä»¶
            for root, _, files in os.walk(uuid_directory):
                for file in files:
                    if file == f"{uuid}.yaml":
                        current_yaml_path = os.path.join(root, file)
                        yaml_found = True
                        break
                if yaml_found:
                    break
            
            if not yaml_found:
                logging.info(f"âš ï¸ æœªæ‰¾åˆ° UUID {uuid} çš„ YAML æ–‡ä»¶")
                continue
            
            # è·å–ç›®æ ‡è·¯å¾„
            try:
                date = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
                year = str(date.year)
                month = f"{date.month:02d}"
                day = f"{date.day:02d}"
                
                # åˆ›å»ºå¹´æœˆæ—¥å±‚çº§ç›®å½•
                year_dir = os.path.join(uuid_directory, year)
                month_dir = os.path.join(year_dir, month)
                day_dir = os.path.join(month_dir, day)
                target_path = os.path.join(day_dir, f"{uuid}.yaml")
                
                # å¦‚æœæ–‡ä»¶å·²ç»åœ¨æ­£ç¡®çš„ä½ç½®ï¼Œè·³è¿‡
                if current_yaml_path == target_path:
                    logging.info(f"âœ“ UUID {uuid} å·²åœ¨æ­£ç¡®ä½ç½®")
                    continue
                
                # å¦‚æœæ–‡ä»¶åœ¨å¹´/æœˆç›®å½•ä¸‹ä½†æ²¡æœ‰æ—¥æœŸç›®å½•
                current_parts = current_yaml_path.split(os.sep)
                target_parts = target_path.split(os.sep)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»åŠ¨
                need_move = True
                if len(current_parts) >= 2:
                    current_year = current_parts[-3] if len(current_parts) >= 3 else None
                    current_month = current_parts[-2] if len(current_parts) >= 2 else None
                    
                    if current_year == year and current_month == month:
                        # å¦‚æœå¹´æœˆæ­£ç¡®ï¼Œåªéœ€è¦ç§»åŠ¨åˆ°æ—¥æœŸç›®å½•
                        logging.info(f"ğŸ“ UUID {uuid} å·²åœ¨æ­£ç¡®çš„å¹´æœˆç›®å½•ï¼Œç§»åŠ¨åˆ°æ—¥æœŸç›®å½•")
                    
                if need_move:
                    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                    os.makedirs(day_dir, exist_ok=True)
                    # ç§»åŠ¨æ–‡ä»¶
                    shutil.move(current_yaml_path, target_path)
                
            except ValueError as e:
                logging.info(f"âŒ UUID {uuid} çš„æ—¶é—´æˆ³æ ¼å¼æ— æ•ˆ: {timestamp}")
            
        except Exception as e:
            logging.info(f"âŒ å¤„ç† UUID {uuid} æ—¶å‡ºé”™: {e}")

    
    logging.info("âœ¨ UUID æ–‡ä»¶é‡ç»„å®Œæˆ")

def update_uuid_records(uuid_directory=r'E:\1BACKUP\ehv\uuid', handler=None):
    """æ›´æ–° UUID è®°å½•æ–‡ä»¶ï¼Œç¡®ä¿æ‰€æœ‰ UUID éƒ½è¢«è®°å½•"""
    logging.info("ğŸ”„ å¼€å§‹æ›´æ–° UUID è®°å½•...")
    
    uuid_record_path = os.path.join(uuid_directory, 'uuid_records.yaml')
    
    # åŠ è½½ç°æœ‰è®°å½•
    existing_records = {}
    if os.path.exists(uuid_record_path):
        try:
            with open(uuid_record_path, 'r', encoding='utf-8') as file:
                records = yaml.safe_load(file) or []
                existing_records = {record['UUID']: record for record in records if 'UUID' in record}
        except Exception as e:
            logging.info(f"âŒ è¯»å–è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
            return
    
    # éå†ç›®å½•ç»“æ„æŸ¥æ‰¾æ‰€æœ‰ YAML æ–‡ä»¶
    new_uuids = []
    for root, _, files in os.walk(uuid_directory):
        for file in files:
            if file.endswith('.yaml') and file != 'uuid_records.yaml':
                uuid = os.path.splitext(file)[0]
                if uuid not in existing_records:
                    yaml_path = os.path.join(root, file)
                    try:
                        with open(yaml_path, 'r', encoding='utf-8') as f:
                            yaml_data = yaml.safe_load(f)
                            if yaml_data and isinstance(yaml_data, list):
                                latest_record = yaml_data[-1]
                                new_record = {
                                    'UUID': uuid,
                                    'CreatedAt': latest_record.get('Timestamp', ''),
                                    'ArchiveName': latest_record.get('ArchiveName', ''),
                                    'ArtistName': latest_record.get('ArtistName', ''),
                                    'LastModified': latest_record.get('Timestamp', ''),
                                    'LastPath': latest_record.get('RelativePath', '')
                                }
                                new_uuids.append(new_record)
                                logging.info(f"âœ¨ å‘ç°æ–° UUID: {uuid}")
                    except Exception as e:
                        logging.info(f"âŒ å¤„ç† YAML æ–‡ä»¶å¤±è´¥ {yaml_path}: {e}")
    
    if new_uuids:
        # æ›´æ–°è®°å½•æ–‡ä»¶
        all_records = list(existing_records.values()) + new_uuids
        try:
            # åˆ›å»ºå¤‡ä»½
            if os.path.exists(uuid_record_path):
                backup_path = f"{uuid_record_path}.bak"
                shutil.copy2(uuid_record_path, backup_path)
            
            # å†™å…¥æ›´æ–°åçš„è®°å½•
            with open(uuid_record_path, 'w', encoding='utf-8') as file:
                yaml.dump(all_records, file, allow_unicode=True, sort_keys=False)
            
            logging.info(f"âœ… å·²æ·»åŠ  {len(new_uuids)} ä¸ªæ–° UUID åˆ°è®°å½•")
        except Exception as e:
            logging.info(f"âŒ æ›´æ–°è®°å½•æ–‡ä»¶å¤±è´¥: {e}")
    else:
        logging.info("âœ“ æ‰€æœ‰ UUID éƒ½å·²åœ¨è®°å½•ä¸­")

def validate_yaml_file(file_path):
    """äº¤äº’å¼YAMLæ–‡ä»¶éªŒè¯å·¥å…·"""
    from yaml import scanner
    try:
        with open(file_path, 'r') as f:
            data = yaml.safe_load(f)
            print(f"âœ… æ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œå…±åŒ…å«{len(data)}æ¡è®°å½•")
            return True
    except scanner.ScannerError as e:
        print(f"âŒ æ‰«æé”™è¯¯ï¼š{e}")
        print(f"å»ºè®®ï¼šæ£€æŸ¥ç¬¬{e.problem_mark.line+1}è¡Œé™„è¿‘çš„ç¼©è¿›å’Œç¬¦å·")
    except yaml.parser.ParserError as e:
        print(f"âŒ è§£æé”™è¯¯ï¼š{e}")
        print(f"å»ºè®®ï¼šæ£€æŸ¥ç¬¬{e.problem_mark.line+1}è¡Œçš„è¯­æ³•ç»“æ„")
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯ï¼š{e}")
    return False

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å¤„ç†æ–‡ä»¶UUIDå’ŒYAMLç”Ÿæˆ')
    parser.add_argument('-c', '--clipboard', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
    parser.add_argument('-m', '--mode', choices=['multi', 'single'], help='å¤„ç†æ¨¡å¼ï¼šmulti(å¤šäººæ¨¡å¼)æˆ–single(å•äººæ¨¡å¼)')
    parser.add_argument('--no-artist', action='store_true', help='æ— ç”»å¸ˆæ¨¡å¼ - ä¸æ·»åŠ ç”»å¸ˆå')
    parser.add_argument('--keep-timestamp', action='store_true', help='ä¿æŒæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´')
    parser.add_argument('--path', help='è¦å¤„ç†çš„è·¯å¾„')
    parser.add_argument('-a', '--auto-sequence', action='store_true', help='è‡ªåŠ¨æ‰§è¡Œå®Œæ•´åºåˆ—ï¼šUUID-YAML -> è‡ªåŠ¨æ–‡ä»¶å -> UUID-YAML')
    parser.add_argument('-r', '--reorganize', action='store_true', help='é‡æ–°ç»„ç»‡ UUID æ–‡ä»¶ç»“æ„')
    parser.add_argument('-u', '--update-records', action='store_true', help='æ›´æ–° UUID è®°å½•æ–‡ä»¶')
    args = parser.parse_args()

    if len(sys.argv) == 1:  # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œå¯åŠ¨TUIç•Œé¢
        main()
        sys.exit(0)

    # å¤„ç†è·¯å¾„å‚æ•°
    if args.clipboard:
        try:
            target_directory = pyperclip.paste().strip().strip('"')
            if not os.path.exists(target_directory):
                print(f"{Fore.RED}å‰ªè´´æ¿ä¸­çš„è·¯å¾„æ— æ•ˆ: {target_directory}{Style.RESET_ALL}")
                exit(1)
            print(f"{Fore.GREEN}å·²ä»å‰ªè´´æ¿è¯»å–è·¯å¾„: {target_directory}{Style.RESET_ALL}")
        except Exception as e:
            print(f"{Fore.RED}ä»å‰ªè´´æ¿è¯»å–è·¯å¾„å¤±è´¥: {e}{Style.RESET_ALL}")
            exit(1)
    else:
        target_directory = args.path or r"E:\1EHV"
        print(f"{Fore.GREEN}ä½¿ç”¨è·¯å¾„: {target_directory}{Style.RESET_ALL}")

    print(f"\n{Fore.CYAN}å½“å‰æ¨¡å¼: {'å¤šäººæ¨¡å¼' if args.mode == 'multi' else 'å•äººæ¨¡å¼'}{Style.RESET_ALL}")

    # æ ¹æ®ç³»ç»Ÿèµ„æºè‡ªåŠ¨è®¾ç½®çº¿ç¨‹æ•°
    import multiprocessing
    max_workers = min(32, (multiprocessing.cpu_count() * 4) + 1)
    
    
    if args.reorganize:
        logging.info("\nğŸ“ å¼€å§‹é‡æ–°ç»„ç»‡ UUID æ–‡ä»¶...")
        reorganize_uuid_files(r'E:\1BACKUP\ehv\uuid')
        
    if args.update_records:
        logging.info("\nğŸ“ å¼€å§‹æ›´æ–° UUID è®°å½•...")
        update_uuid_records(r'E:\1BACKUP\ehv\uuid')
    
    if args.auto_sequence:
        logging.info("ğŸ”„ å¼€å§‹æ‰§è¡Œå®Œæ•´åºåˆ—...")
        
        logging.info("\nğŸ“ ç¬¬1æ­¥ï¼šæ‰§è¡ŒUUID-YAMLå¤„ç†...")
        if args.mode == 'multi':
            warm_up_cache(target_directory, max_workers)
        elif args.mode == 'single':
            logging.info("ğŸ”„ å¼€å§‹æ‰§è¡Œå•äººæ¨¡å¼...")
            skip_limit_reached = process_archives(target_directory, max_workers)
        else:
            logging.info("ğŸ”„ å¼€å§‹æ‰§è¡Œæ— äººæ¨¡å¼...")
            skip_limit_reached = process_archives(target_directory, max_workers)
        
        if skip_limit_reached:
            logging.info("\nâ© ç”±äºè¿ç»­è·³è¿‡æ¬¡æ•°è¾¾åˆ°é™åˆ¶ï¼Œæå‰è¿›å…¥ä¸‹ä¸€é˜¶æ®µ")
        
        logging.info("\nğŸ“ ç¬¬2æ­¥ï¼šæ‰§è¡Œè‡ªåŠ¨æ–‡ä»¶åå¤„ç†...")
        auto_filename_script = os.path.join(os.path.dirname(__file__), '011-è‡ªåŠ¨å”¯ä¸€æ–‡ä»¶å.py')
        if os.path.exists(auto_filename_script):
            try:
                cmd = [sys.executable, auto_filename_script]
                if args.clipboard:
                    cmd.extend(['-c'])
                if args.mode:
                    cmd.extend(['-m', args.mode])
                
                startupinfo = None
                if os.name == 'nt':
                    startupinfo = subprocess.STARTUPINFO()
                    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

                result = subprocess.run(
                    cmd, 
                    check=True,
                    capture_output=True,
                    text=True,
                    encoding='gbk',
                    errors='ignore',
                    startupinfo=startupinfo
                )
                
                for line in result.stdout.splitlines():
                    if line.strip():
                        logging.info(line)
                
                logging.info("âœ… è‡ªåŠ¨æ–‡ä»¶åå¤„ç†å®Œæˆ")
            except subprocess.CalledProcessError as e:
                logging.info(f"è‡ªåŠ¨æ–‡ä»¶åå¤„ç†å¤±è´¥: {str(e)}")
                if e.output:
                    logging.info(f"é”™è¯¯è¾“å‡º: {e.output}")
        else:
            logging.info(f"æ‰¾ä¸åˆ°è‡ªåŠ¨æ–‡ä»¶åè„šæœ¬: {auto_filename_script}")
            
        logging.info("\nğŸ“ ç¬¬3æ­¥ï¼šå†æ¬¡æ‰§è¡ŒUUID-YAMLå¤„ç†...")
        if args.mode == 'multi':
            warm_up_cache(target_directory, max_workers)
        process_archives(target_directory, max_workers)
        
        logging.info("\nâœ¨ å®Œæ•´åºåˆ—æ‰§è¡Œå®Œæˆï¼")
    
    elif not args.reorganize and not args.update_records:
        if args.mode == 'multi':
            warm_up_cache(target_directory, max_workers)
        process_archives(target_directory, max_workers)
    
    if not validate_yaml_file(r'E:\1BACKUP\ehv\uuid\uuid_records.yaml'):
        print("è¯·å…ˆä¿®å¤YAMLæ–‡ä»¶åå†ç»§ç»­æ“ä½œ")
        sys.exit(1)
    