from src.core.archive_processor import ArchiveProcessor
from src.services.logging_service import LoggingService
from src.services.stats_service import StatsService
import logging
import os
import shutil
from src.utils.hash_utils import HashFileHandler
from src.utils.path_utils import PathManager
from src.services.backup_service import BackupHandler
import subprocess
from src.utils.content_filter import ContentFilter
from src.utils.directory_handler import DirectoryHandler
from src.services.logging_service import LoggingService

class ProcessManager:
    """
    ç±»æè¿°
    """
    @staticmethod
    def generate_summary_report(processed_archives):
        """ç”Ÿæˆå¤„ç†æ‘˜è¦å¹¶æ˜¾ç¤ºåˆ°é¢æ¿"""
        if not processed_archives:
            logging.info( 'æ²¡æœ‰å¤„ç†ä»»ä½•å‹ç¼©åŒ…ã€‚')
            return
            
        # ä½¿ç”¨StatsServiceä¸­çš„ç»Ÿè®¡æ•°æ®
        summary = [
            "ğŸ“Š å¤„ç†å®Œæˆæ‘˜è¦",
            f"æ€»å…±å¤„ç†: {len(processed_archives)} ä¸ªå‹ç¼©åŒ…",
            f"åˆ é™¤å“ˆå¸Œé‡å¤å›¾ç‰‡: {StatsService.hash_duplicates_count} å¼ ",
            f"åˆ é™¤æ™®é€šé‡å¤å›¾ç‰‡: {StatsService.normal_duplicates_count} å¼ ",
            f"åˆ é™¤å°å›¾: {StatsService.small_images_count} å¼ ",
            f"åˆ é™¤ç™½å›¾: {StatsService.white_images_count} å¼ ",
            f"æ€»å…±å‡å°‘: {sum(archive['size_reduction_mb'] for archive in processed_archives):.2f} MB",
            "\nè¯¦ç»†ä¿¡æ¯:"
        ]
        
        # æŒ‰ç›®å½•ç»„ç»‡å¤„ç†ç»“æœ
        common_path_prefix = os.path.commonpath([archive['file_path'] for archive in processed_archives])
        tree_structure = {}
        for archive in processed_archives:
            relative_path = os.path.relpath(archive['file_path'], common_path_prefix)
            path_parts = relative_path.split(os.sep)
            current_level = tree_structure
            for part in path_parts:
                current_level = current_level.setdefault(part, {})
            current_level['_summary'] = (
                f"å“ˆå¸Œé‡å¤: {archive.get('hash_duplicates_removed', 0)} å¼ , "
                f"æ™®é€šé‡å¤: {archive.get('normal_duplicates_removed', 0)} å¼ , "
                f"å°å›¾: {archive.get('small_images_removed', 0)} å¼ , "
                f"ç™½å›¾: {archive.get('white_images_removed', 0)} å¼ , "
                f"å‡å°‘: {archive['size_reduction_mb']:.2f} MB"
            )
        
        # ç”Ÿæˆæ ‘å½¢ç»“æ„çš„è¯¦ç»†ä¿¡æ¯
        def build_tree_text(level, indent=''):
            tree_text = []
            for name, content in level.items():
                if name == '_summary':
                    tree_text.append(f'{indent}{content}')
                else:
                    tree_text.append(f'{indent}â”œâ”€ {name}')
                    tree_text.extend(build_tree_text(content, indent + 'â”‚   '))
            return tree_text
        
        # æ·»åŠ æ ‘å½¢ç»“æ„åˆ°æ‘˜è¦
        summary.extend(build_tree_text(tree_structure))
        
        # æ›´æ–°åˆ°é¢æ¿
        logging.info( '\n'.join(summary))
        logging.info( "âœ… å¤„ç†å®Œæˆï¼Œå·²ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š")


    @staticmethod
    def process_normal_archives(directories, args):
        """å¤„ç†æ™®é€šæ¨¡å¼çš„å‹ç¼©åŒ…æˆ–ç›®å½•"""
        for directory in directories:
            if os.path.exists(directory):
                params = InputHandler.prepare_params(args)
                ProcessManager.process_all_archives([directory], params)
                if args.bak_mode != 'keep':
                    for root, _, files in os.walk(directory):
                        for file in files:
                            if file.endswith('.bak'):
                                bak_path = os.path.join(root, file)
                                BackupHandler.handle_bak_file(bak_path, args)
                DirectoryHandler.remove_empty_directories(directory)
            else:
                logging.info( f"è¾“å…¥çš„è·¯å¾„ä¸å­˜åœ¨: {directory}")

    @staticmethod
    def process_merged_archives(directories, args):
        """å¤„ç†åˆå¹¶æ¨¡å¼çš„å‹ç¼©åŒ…"""
        temp_dir, merged_zip, archive_paths = ArchiveProcessor.merge_archives(directories, args)
        if temp_dir and merged_zip:
            try:
                params = InputHandler.prepare_params(args)
                ProcessManager.process_all_archives([merged_zip], params)
                if ArchiveProcessor.split_merged_archive(merged_zip, archive_paths, temp_dir, params):
                    logging.info( 'æˆåŠŸå®Œæˆå‹ç¼©åŒ…çš„åˆå¹¶å¤„ç†å’Œæ‹†åˆ†')
                else:
                    logging.info( 'æ‹†åˆ†å‹ç¼©åŒ…å¤±è´¥')
            finally:
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
                if os.path.exists(merged_zip):
                    os.remove(merged_zip)

    @staticmethod
    def print_config(args, max_workers):
        """æ‰“å°å½“å‰é…ç½®ä¿¡æ¯"""
        # æ¸…å±
        os.system('cls' if os.name == 'nt' else 'clear')
        
        # ä½¿ç”¨log_panelè¾“å‡ºé…ç½®ä¿¡æ¯
        config_info = [
            '\n=== å½“å‰é…ç½®ä¿¡æ¯ ===',
            'å¯ç”¨çš„åŠŸèƒ½:',
            f"  - å°å›¾è¿‡æ»¤: {('æ˜¯' if args.remove_small else 'å¦')}"
        ]
        
        if args.remove_small:
            config_info.append(f'    æœ€å°å°ºå¯¸: {args.min_size}x{args.min_size} åƒç´ ')
            
        config_info.extend([
            f"  - é»‘ç™½å›¾è¿‡æ»¤: {('æ˜¯' if args.remove_grayscale else 'å¦')}"
        ])
        

            
        config_info.extend([
            f"  - é‡å¤å›¾ç‰‡è¿‡æ»¤: {('æ˜¯' if args.remove_duplicates else 'å¦')}"
        ])
        
        if args.remove_duplicates:
            config_info.extend([
                f'    å†…éƒ¨å»é‡æ±‰æ˜è·ç¦»é˜ˆå€¼: {args.hamming_distance}',
                f'    å¤–éƒ¨å‚è€ƒæ±‰æ˜è·ç¦»é˜ˆå€¼: {args.ref_hamming_distance}'
            ])
            
        config_info.extend([
            f"  - åˆå¹¶å‹ç¼©åŒ…å¤„ç†: {('æ˜¯' if args.merge_archives else 'å¦')}",
            f"ä»å‰ªè´´æ¿è¯»å–: {('æ˜¯' if args.clipboard else 'å¦')}",
            f'å¤‡ä»½æ–‡ä»¶å¤„ç†æ¨¡å¼: {args.bak_mode}',
            f'çº¿ç¨‹æ•°: {max_workers}',
            '==================\n'
        ])
        
        initialize_logger()
        logging.info( '\n'.join(config_info))

    @staticmethod
    def process_all_archives(directories, params):
        """
        ä¸»å¤„ç†å‡½æ•°
        
        Args:
            directories: è¦å¤„ç†çš„ç›®å½•åˆ—è¡¨
            params: å‚æ•°å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„å¤„ç†å‚æ•°
        """

            
        processed_archives = []
        logging.info( "å¼€å§‹å¤„ç†æ‹–å…¥çš„ç›®å½•æˆ–æ–‡ä»¶")
        
        # è®¡ç®—æ€»æ–‡ä»¶æ•°
        total_zip_files = sum((1 for directory in directories 
                             for root, _, files in os.walk(directory) 
                             for file in files if file.lower().endswith('zip')))
        
        # æ›´æ–°æ€»ä½“è¿›åº¦é¢æ¿
        logging.info( 
            f"æ€»æ–‡ä»¶æ•°: {total_zip_files}\n"
            f"å·²å¤„ç†: 0\n"
            f"æˆåŠŸ: 0\n"
            f"è­¦å‘Š: 0\n"
            f"é”™è¯¯: 0"
        )

        # è®¾ç½®æ€»æ•°
        StatsService.set_total(total_zip_files)
            
        for directory in directories:
            archives = ProcessManager.process_directory(directory, params)
            processed_archives.extend(archives)
                
            # æ›´æ–°æ€»ä½“è¿›åº¦
            success_count = sum(1 for a in archives if (a.get('duplicates_removed', 0) > 0 or 
                                                      a.get('small_images_removed', 0) > 0 or 
                                                      a.get('white_images_removed', 0) > 0))
            warning_count = sum(1 for a in archives if (a.get('duplicates_removed', 0) == 0 and 
                                                      a.get('small_images_removed', 0) == 0 and 
                                                      a.get('white_images_removed', 0) == 0))
            error_count = StatsService.processed_count - len(archives)
                
            logging.info( 
                f"æ€»æ–‡ä»¶æ•°: {total_zip_files}\n"
                f"å·²å¤„ç†: {StatsService.processed_count}\n"
                f"æˆåŠŸ: {success_count}\n"
                f"è­¦å‘Š: {warning_count}\n"
                f"é”™è¯¯: {error_count}"
            )
        
        ProcessManager.generate_summary_report(processed_archives)
        logging.info( "æ‰€æœ‰ç›®å½•å¤„ç†å®Œæˆ")
        return processed_archives

    @staticmethod
    def process_directory(directory, params):
        """å¤„ç†å•ä¸ªç›®å½•"""
        try:
            logging.info( f"\nå¼€å§‹å¤„ç†ç›®å½•: {directory}")
            processed_archives = []
            if os.path.isfile(directory):
                logging.info( f"å¤„ç†å•ä¸ªæ–‡ä»¶: {directory}")
                if directory.lower().endswith('zip'):
                    if ContentFilter.should_process_file(directory, params):
                        logging.info( f"å¼€å§‹å¤„ç†å‹ç¼©åŒ…: {directory}")
                        archives = ProcessManager.process_single_archive(directory, params)
                        processed_archives.extend(archives)
                    else:
                        logging.info( f"è·³è¿‡æ–‡ä»¶ï¼ˆæ ¹æ®è¿‡æ»¤è§„åˆ™ï¼‰: {directory}")
                    StatsService.increment()
                else:
                    logging.info( f"è·³è¿‡ézipæ–‡ä»¶: {directory}")
            elif os.path.isdir(directory):
                logging.info( f"æ‰«æç›®å½•ä¸­çš„æ–‡ä»¶: {directory}")
                files_to_process = []
                for root, _, files in os.walk(directory):
                    logging.debug( f"æ‰«æå­ç›®å½•: {root}")
                    for file in files:
                        if file.lower().endswith('zip'):
                            file_path = os.path.join(root, file)
                            logging.info( f"å‘ç°zipæ–‡ä»¶: {file_path}")
                            if ContentFilter.should_process_file(file_path, params):
                                logging.info( f"æ·»åŠ åˆ°å¤„ç†åˆ—è¡¨: {file_path}")
                                files_to_process.append(file_path)
                            else:
                                logging.info( f"è·³è¿‡æ–‡ä»¶ï¼ˆæ ¹æ®è¿‡æ»¤è§„åˆ™ï¼‰: {file_path}")
                                StatsService.increment()
                logging.info( f"æ‰«æå®Œæˆ: æ‰¾åˆ° {len(files_to_process)} ä¸ªè¦å¤„ç†çš„æ–‡ä»¶")
                for file_path in files_to_process:
                    try:
                        logging.info( f"\næ­£åœ¨å¤„ç†å‹ç¼©åŒ…: {file_path}")
                        archives = ProcessManager.process_single_archive(file_path, params)
                        if archives:
                            logging.info( f"æˆåŠŸå¤„ç†å‹ç¼©åŒ…: {file_path}")
                        else:
                            logging.info( f"å‹ç¼©åŒ…å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰å˜åŒ–: {file_path}")
                        processed_archives.extend(archives)
                    except Exception as e:
                        logging.info( f"å¤„ç†å‹ç¼©åŒ…å‡ºé”™: {file_path}\né”™è¯¯: {e}")
                    finally:
                        StatsService.increment()
            if os.path.isdir(directory):
                exclude_keywords = params.get('exclude_paths', [])
            return processed_archives
        except Exception as e:
            logging.info( f"å¤„ç†ç›®å½•æ—¶å‘ç”Ÿå¼‚å¸¸: {directory}\n{str(e)}")
            return []

    @staticmethod
    def process_single_archive(file_path, params):
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…æ–‡ä»¶"""
        try:
            logging.info( f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            
            if not os.path.exists(file_path):
                logging.info( f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return []
                
            cmd = ['7z', 'l', file_path]
            result = subprocess.run(cmd, capture_output=True, encoding='utf-8', errors='ignore')
            if result.returncode != 0:
                logging.info( f"âŒ å‹ç¼©åŒ…å¯èƒ½æŸå: {file_path}")
                return []
                
            if result.stdout is None:
                logging.info( f"âŒ æ— æ³•è¯»å–å‹ç¼©åŒ…å†…å®¹: {file_path}")
                return []
                
            has_images = any((line.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.jxl', '.avif', '.jxl')) 
                            for line in result.stdout.splitlines()))
            if not has_images:
                logging.info( f"âš ï¸ è·³è¿‡æ— å›¾ç‰‡çš„å‹ç¼©åŒ…: {file_path}")
                return []
            processed_archives = []
            if not params['ignore_processed_log']:
                if ProcessedLogHandler.has_processed_log(file_path):
                    logging.info( f"âš ï¸ æ–‡ä»¶å·²æœ‰å¤„ç†è®°å½•: {file_path}")
                    return processed_archives
                    
            logging.info( "å¼€å§‹å¤„ç†å‹ç¼©åŒ…å†…å®¹...")
            processed_archives.extend(ArchiveProcessor.process_archive_in_memory(file_path, params))
            
            if processed_archives:
                # æ›´æ–°é‡å¤ä¿¡æ¯é¢æ¿
                info = processed_archives[-1]
                logging.info( 
                    f"å¤„ç†ç»“æœ:\n"
                    f"- å“ˆå¸Œé‡å¤: {info.get('hash_duplicates_removed', 0)} å¼ \n"
                    f"- æ™®é€šé‡å¤: {info.get('normal_duplicates_removed', 0)} å¼ \n"
                    f"- å°å›¾: {info.get('small_images_removed', 0)} å¼ \n"
                    f"- ç™½å›¾: {info.get('white_images_removed', 0)} å¼ \n"
                    f"- å‡å°‘å¤§å°: {info['size_reduction_mb']:.2f} MB"
                )
                
                # æ›´æ–°è¿›åº¦é¢æ¿
                logging.info( f"âœ… æˆåŠŸå¤„ç†: {os.path.basename(file_path)}")
                
                    
                if params['add_processed_log_enabled']:
                    processed_info = {
                        'hash_duplicates_removed': info.get('hash_duplicates_removed', 0),
                        'normal_duplicates_removed': info.get('normal_duplicates_removed', 0),
                        'small_images_removed': info.get('small_images_removed', 0),
                        'white_images_removed': info.get('white_images_removed', 0)
                    }
                    ProcessedLogHandler.add_processed_log(file_path, processed_info)
                    logging.info( "å·²æ·»åŠ å¤„ç†æ—¥å¿—")
            else:
                logging.info( f"âš ï¸ å‹ç¼©åŒ…å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰éœ€è¦å¤„ç†çš„å†…å®¹: {file_path}")
                
            backup_file_path = file_path + '.bak'
            if os.path.exists(backup_file_path):
                BackupHandler.handle_bak_file(backup_file_path, params)
                logging.info( "å·²å¤„ç†å¤‡ä»½æ–‡ä»¶")
                
            return processed_archives
            
        except UnicodeDecodeError as e:
            logging.info( f"âŒ å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºç°ç¼–ç é”™è¯¯ {file_path}: {e}")
            return []
        except Exception as e:
            logging.info( f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {file_path}\n{str(e)}")
            return []

    @staticmethod
    def get_max_workers():
        """è·å–æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°"""
        return max_workers  # è¿”å›å…¨å±€é…ç½®çš„max_workerså€¼

    """å¤„ç†ç®¡ç†ç±»"""
    @staticmethod
    def process_normal_archives(directories, args):
        """å¤„ç†æ™®é€šæ¨¡å¼çš„å‹ç¼©åŒ…æˆ–ç›®å½•"""
        pass

    @staticmethod
    def process_merged_archives(directories, args):
        """å¤„ç†åˆå¹¶æ¨¡å¼çš„å‹ç¼©åŒ…"""
        pass

    @staticmethod
    def process_all_archives(directories, params):
        """ä¸»å¤„ç†å‡½æ•°"""
        pass

    @staticmethod
    def process_directory(directory, params):
        """å¤„ç†å•ä¸ªç›®å½•"""
        pass

    @staticmethod
    def process_single_archive(file_path, params):
        """å¤„ç†å•ä¸ªå‹ç¼©åŒ…æ–‡ä»¶"""
        pass 