from PIL import Image
from src.utils.hash_utils import HashUtils
from src.services.logging_service import LoggingService
import logging
from src.utils.hash_utils import HashFileHandler
import os
from src.core.duplicate_detector import DuplicateDetector
from src.utils.hash_utils import ImageHashCalculator
from src.utils.path_utils import PathURIGenerator
from src.utils.archive_utils import ArchiveExtractor
from src.utils.hash_utils import ImageHashCalculator
from src.utils.path_utils import PathURIGenerator
from src.utils.hash_utils import ImageHashCalculator
from src.utils.path_utils import PathURIGenerator
from src.utils.hash_utils import ImageHashCalculator
from src.utils.path_utils import PathURIGenerator
from io import BytesIO
from pics.grayscale_detector import GrayscaleDetector
from concurrent.futures import ThreadPoolExecutor
import threading
from pathlib import Path




class ImageProcessor:
    """
    ç±»æè¿°
    """
    def __init__(self):
        """åˆå§‹åŒ–å›¾ç‰‡å¤„ç†å™¨"""
        self.grayscale_detector = GrayscaleDetector()
        self.global_hashes = {}  # åˆå§‹åŒ–ä¸ºç©ºå­—å…¸
        self.temp_hashes = {}  # ä¸´æ—¶å­˜å‚¨å½“å‰å‹ç¼©åŒ…çš„å“ˆå¸Œå€¼

    def set_global_hashes(self, hashes):
        """è®¾ç½®å…¨å±€å“ˆå¸Œç¼“å­˜"""
        self.global_hashes = hashes

    @staticmethod
    def calculate_phash(image_path_or_data):
        """ä½¿ç”¨æ„ŸçŸ¥å“ˆå¸Œç®—æ³•è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼
        
        Args:
            image_path_or_data: å¯ä»¥æ˜¯å›¾ç‰‡è·¯å¾„(str/Path)æˆ–BytesIOå¯¹è±¡
            
        Returns:
            str: 16è¿›åˆ¶æ ¼å¼çš„æ„ŸçŸ¥å“ˆå¸Œå€¼ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            hash_value = ImageHashCalculator.calculate_phash(image_path_or_data)
            if isinstance(image_path_or_data, (str, Path)):
                logging.info( f"[#hash_calc]è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼: {os.path.basename(str(image_path_or_data))} -> {hash_value}")
            return hash_value
        except Exception as e:
            logging.info(f"[#hash_calc]è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼å¤±è´¥: {e}")
            return None

    def process_images_in_directory(self, temp_dir, params):
        """å¤„ç†ç›®å½•ä¸­çš„å›¾ç‰‡"""
        try:
            # æ¸…ç©ºä¸´æ—¶å“ˆå¸Œå­˜å‚¨
            self.temp_hashes.clear()
            
            image_files = ArchiveExtractor.get_image_files(temp_dir)
            if not image_files:
                logging.info( f"âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
                return (set(), set())
            removed_files = set()
            duplicate_files = set()
            lock = threading.Lock()
            existing_file_names = set()
            with ThreadPoolExecutor(max_workers=params['max_workers']) as executor:
                futures = []
                for file_path in image_files:
                    rel_path = os.path.relpath(file_path, os.path.dirname(file_path))
                    future = executor.submit(self.process_single_image, file_path, rel_path, existing_file_names, params, lock)
                    futures.append((future, file_path))
                image_hashes = []
                for future, file_path in futures:
                    try:
                        img_hash, img_data, rel_path, reason = future.result()
                        if reason in ['small_image', 'white_image']:
                            removed_files.add(file_path)
                        elif img_hash is not None:
                            image_hashes.append((img_hash, img_data, file_path, reason))
                    except Exception as e:
                        logging.info( f"âŒ å¤„ç†å›¾ç‰‡å¤±è´¥ {file_path}: {e}")
            if params['remove_duplicates'] and image_hashes:
                unique_images, _, removal_reasons = DuplicateDetector.remove_duplicates_in_memory(image_hashes, params)
                processed_files = {img[2] for img in unique_images}
                for img_hash, _, file_path, _ in image_hashes:
                    if file_path not in processed_files:
                        duplicate_files.add(file_path)
                        
                # # å¤„ç†å®Œæˆåï¼Œå°†ä¸´æ—¶å“ˆå¸Œæ›´æ–°åˆ°å…¨å±€å“ˆå¸Œ
                # if self.temp_hashes:
                #     with lock:
                #         self.global_hashes.update(self.temp_hashes)
                #         logging.info(f"[#hash_calc]å·²æ‰¹é‡æ·»åŠ  {len(self.temp_hashes)} ä¸ªå“ˆå¸Œåˆ°å…¨å±€ç¼“å­˜")
                #         # æ¸…ç©ºä¸´æ—¶å­˜å‚¨
                #         self.temp_hashes.clear()
                        
            return (removed_files, duplicate_files)
        except Exception as e:
            logging.info( f"âŒ å¤„ç†ç›®å½•ä¸­çš„å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return (set(), set())

    def process_single_image(self, file_path, rel_path, existing_file_names, params, lock): 
        """å¤„ç†å•ä¸ªå›¾ç‰‡æ–‡ä»¶"""
        try:
            if not file_path or not os.path.exists(file_path):
                logging.info(f"[#file_ops]âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return (None, None, None, 'file_not_found')
                            
            try:
                with open(file_path, 'rb') as f:
                    file_data = f.read()
                    logging.info(f"[#file_ops]ğŸ“·è¯»å›¾: {file_path}")  # æ·»åŠ é¢æ¿æ ‡è¯†
            except (IOError, OSError) as e:
                logging.info(f"[#file_ops]âŒ å›¾ç‰‡æ–‡ä»¶æŸåæˆ–æ— æ³•è¯»å– {rel_path}: {e}")
                return (None, None, None, 'corrupted_image')
            except Exception as e:
                logging.info(f"[#file_ops]âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {rel_path}: {e}")
                return (None, None, None, 'read_error')
                
            if file_path.lower().endswith(('png', 'webp', 'jxl', 'avif', 'jpg', 'jpeg', 'bmp', 'tiff', 'tif', 'heic', 'heif', 'bmp')):
                # === ç‹¬ç«‹å¤„ç†æ­¥éª¤ ===
                processed_data = file_data
                removal_reason = None

                # æ­¥éª¤1: å°å›¾æ£€æµ‹ (ç‹¬ç«‹åˆ¤æ–­)
                if params.get('filter_height_enabled', False):
                    logging.info(f"[#image_processing]ğŸ–¼ï¸ æ­£åœ¨æ£€æµ‹å°å›¾: {os.path.basename(file_path)}")
                    processed_data, removal_reason = self.detect_small_image(processed_data, params)
                    if removal_reason:
                        return (None, file_data, file_path, removal_reason)

                # æ­¥éª¤2: ç™½å›¾æ£€æµ‹ (ç‹¬ç«‹åˆ¤æ–­)
                if params.get('remove_grayscale', False):
                    logging.info(f"[#image_processing]ğŸ¨ æ­£åœ¨æ£€æµ‹ç™½å›¾: {os.path.basename(file_path)}")
                    processed_data, removal_reason = self.detect_grayscale_image(processed_data)
                    if removal_reason:
                        return (None, file_data, file_path, removal_reason)

                # æ­¥éª¤3: é‡å¤æ£€æµ‹ - ç›´æ¥è®¡ç®—å“ˆå¸Œ,ä¸æ£€æŸ¥å…¨å±€åŒ¹é…
                if params.get('remove_duplicates', False):
                    img_hash = self.handle_duplicate_detection(file_path, rel_path, params, lock, processed_data)
                    if not img_hash:
                        return (None, file_data, file_path, 'hash_error')
                    return (img_hash, file_data, file_path, None)

                return (None, file_data, file_path, None)
            else:
                return (None, file_data, file_path, 'non_image_file')
        except Exception as e:
            logging.info(f"[#file_ops]âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {rel_path}: {e}")
            return (None, None, None, 'processing_error')

       # ä¿®æ”¹åçš„ç‹¬ç«‹å°å›¾æ£€æµ‹æ–¹æ³•
    def detect_small_image(self, image_data, params):
        """ç‹¬ç«‹çš„å°å›¾æ£€æµ‹"""
        try:
            # å¦‚æœæ˜¯PILå›¾åƒå¯¹è±¡ï¼Œå…ˆè½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
            if isinstance(image_data, Image.Image):
                img_byte_arr = BytesIO()
                image_data.save(img_byte_arr, format=image_data.format)
                img_data = img_byte_arr.getvalue()
            else:
                img_data = image_data
                
            img = Image.open(BytesIO(img_data))
            width, height = img.size
            if height < params.get('min_size', 631):
                return None, 'small_image'
            return img_data, None
        except Exception as e:
            return None, 'size_detection_error'

    def detect_grayscale_image(self, image_data):
        """ç‹¬ç«‹çš„ç™½å›¾æ£€æµ‹"""
        white_keywords = ['pure_white', 'white', 'pure_black', 'grayscale']
        try:
            result = self.grayscale_detector.analyze_image(image_data)
            if result is None:
                logging.info( f"ç°åº¦åˆ†æè¿”å›None")
                return (None, 'grayscale_detection_error')
                
            # è¯¦ç»†è®°å½•åˆ†æç»“æœ
            # logging.info( f"ç°åº¦åˆ†æç»“æœ: {result}")
            
            if hasattr(result, 'removal_reason') and result.removal_reason:
                logging.info( f"æ£€æµ‹åˆ°ç§»é™¤åŸå› : {result.removal_reason}")
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å…³é”®å­—
                matched_keywords = [keyword for keyword in white_keywords 
                                  if keyword in result.removal_reason]
                if matched_keywords:
                    logging.info( f"åŒ¹é…åˆ°ç™½å›¾å…³é”®å­—: {matched_keywords}")
                    return (None, 'white_image')
                    
                # å¦‚æœæœ‰removal_reasonä½†ä¸åŒ¹é…å…³é”®å­—ï¼Œè®°å½•è¿™ç§æƒ…å†µ
                logging.info( f"æœªåŒ¹é…å…³é”®å­—çš„ç§»é™¤åŸå› : {result.removal_reason}")
                return (None, result.removal_reason)
                
            return (image_data, None)
            
        except ValueError as ve:
            logging.info( f"ç°åº¦æ£€æµ‹å‘ç”ŸValueError: {str(ve)}")
            return (None, 'grayscale_detection_error')
        except Exception as e:
            logging.info( f"ç°åº¦æ£€æµ‹å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None, 'grayscale_detection_error'

    def handle_duplicate_detection(self, file_path, rel_path, params, lock, image_data):
        """å¤„ç†é‡å¤æ£€æµ‹ - åªè®¡ç®—å“ˆå¸Œ"""
        try:
            # è®¡ç®—æ–°çš„å“ˆå¸Œå€¼
            img_hash = ImageHashCalculator.calculate_phash(image_data)
            if img_hash:
                # è·å–å‹ç¼©åŒ…è·¯å¾„å¹¶æ„å»ºURI
                zip_path = params.get('zip_path')
                if zip_path:
                    img_uri = PathURIGenerator.generate(f"{zip_path}!{rel_path}")
                    # æ·»åŠ å“ˆå¸Œæ“ä½œé¢æ¿æ ‡è¯†
                    logging.info(f"[#hash_calc]è®¡ç®—å“ˆå¸Œå€¼: {img_uri} -> {img_hash['hash']}")  
            return img_hash
            
        except Exception as e:
            # é”™è¯¯æ—¥å¿—ä¹ŸæŒ‡å‘å“ˆå¸Œæ“ä½œé¢æ¿
            logging.info(f"[#hash_calc]âŒ è®¡ç®—å“ˆå¸Œå€¼å¤±è´¥: {str(e)}")  
            return None
