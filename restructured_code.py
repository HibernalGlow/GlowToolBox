"""
é‡ç»„åçš„ä»£ç æ–‡ä»¶
æ ¹æ®ç›®æ ‡ç»“æ„è‡ªåŠ¨ç”Ÿæˆ
"""

from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from diff_match_patch import diff_match_patch
from nodes.record.logger_config import setup_logger
from nodes.tui.textual_logger import TextualLoggerManager
from nodes.tui.textual_preset import create_config_app
from opencc import OpenCC
from pathlib import Path
from queue import Queue
from rapidfuzz import fuzz, process
import argparse
import ctypes
import difflib
import functools
import logging
import os
import os
import pyperclip
import re
import shutil
import signal
import subprocess
import sys
import sys
import tempfile
import threading
import time
import win32api

sys.path.append(os.path.dirname(os.path.dirname(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
TEXTUAL_LAYOUT = {'current_stats': {'ratio': 2, 'title': 'ğŸ“Š æ€»ä½“è¿›åº¦', 'style': 'lightyellow'}, 'current_progress': {'ratio': 2, 'title': 'ğŸ”„ å½“å‰è¿›åº¦', 'style': 'lightcyan'}, 'process': {'ratio': 3, 'title': 'ğŸ“ å¤„ç†æ—¥å¿—', 'style': 'lightpink'}, 'update': {'ratio': 2, 'title': 'â„¹ï¸ æ›´æ–°æ—¥å¿—', 'style': 'lightblue'}}
config = {'script_name': 'comic_auto_uuid', 'console_enabled': False}
cc_t2s = OpenCC('t2s')
cc_s2t = OpenCC('s2t')
CATEGORY_RULES = {'1. åŒäººå¿—': {'patterns': ['\\[C\\d+\\]', '\\(C\\d+\\)', 'ã‚³ãƒŸã‚±\\d+', 'COMIC\\s*MARKET', 'COMIC1', 'åŒäººèªŒ', 'åŒäººå¿—', 'ã‚³ãƒŸã‚±', 'ã‚³ãƒŸãƒƒã‚¯ãƒãƒ¼ã‚±ãƒƒãƒˆ', 'ä¾‹å¤§ç¥­', 'ã‚µãƒ³ã‚¯ãƒª', '(?i)doujin', 'COMIC1â˜†\\d+'], 'exclude_patterns': ['ç”»é›†', 'artbook', 'art\\s*works', '01è§†é¢‘', '02åŠ¨å›¾', 'art\\s*works']}, '2. å•†ä¸šå¿—': {'patterns': ['(?i)magazine', '(?i)COMIC', 'é›‘èªŒ', 'æ‚å¿—', 'å•†ä¸š', 'é€±åˆŠ', 'æœˆåˆŠ', 'æœˆå·', 'COMIC\\s*REX', 'ã‚³ãƒŸãƒƒã‚¯', 'ãƒ¤ãƒ³ã‚°ãƒã‚¬ã‚¸ãƒ³', '\\d{4}å¹´\\d{1,2}æœˆå·'], 'exclude_patterns': ['åŒäºº', '(?i)doujin', 'å˜è¡Œæœ¬', 'ç”»é›†']}, '3. å•è¡Œæœ¬': {'patterns': ['å˜è¡Œæœ¬', 'å•è¡Œæœ¬', '(?i)tankoubon', 'ç¬¬\\d+å·»', 'vol\\.?\\d+', 'volume\\s*\\d+'], 'exclude_patterns': ['ç”»é›†', 'artbook', 'art\\s*works']}, '4. ç”»é›†': {'patterns': ['ç”»é›†', '(?i)art\\s*book', '(?i)art\\s*works', 'ã‚¤ãƒ©ã‚¹ãƒˆé›†', 'æ‚å›¾åˆé›†', 'ä½œå“é›†', 'illustrations?', '(?i)illust\\s*collection'], 'exclude_patterns': []}, '5. åŒäººCG': {'patterns': ['åŒäººCG'], 'exclude_patterns': []}}
if sys.platform == 'win32':
    try:
        import win32api

        def win32_path_exists(path):
            try:
                win32api.GetFileAttributes(path)
                return True
            except:
                print('æœªå®‰è£…win32apiæ¨¡å—ï¼ŒæŸäº›è·¯å¾„å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†')
                win32_path_exists = os.path.exists
    except ImportError:
        print('æœªå®‰è£…win32apiæ¨¡å—ï¼ŒæŸäº›è·¯å¾„å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†')
        win32_path_exists = os.path.exists
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.jxl', '.avif', '.heic', '.heif', '.jfif', '.tiff', '.tif', '.psd', '.xcf'}
SERIES_PREFIXES = {'[#s]', '#'}
PATH_BLACKLIST = {'ç”»é›†', '01è§†é¢‘', '02åŠ¨å›¾', 'æŸåå‹ç¼©åŒ…'}
SERIES_BLACKLIST_PATTERNS = ['ç”»é›†', 'fanbox', 'pixiv', 'ãƒ»', 'æ‚å›¾åˆé›†', '01è§†é¢‘', '02åŠ¨å›¾', 'ä½œå“é›†', '01è§†é¢‘', '02åŠ¨å›¾', 'æŸåå‹ç¼©åŒ…']
SIMILARITY_CONFIG = {'THRESHOLD': 75, 'LENGTH_DIFF_MAX': 0.3, 'RATIO_THRESHOLD': 75, 'PARTIAL_THRESHOLD': 85, 'TOKEN_THRESHOLD': 80}
if __name__ == '__main__':
    main()

class Config:
    """
    ç±»æè¿°
    """

class Logger:
    """
    ç±»æè¿°
    """

class PathManager:
    """
    ç±»æè¿°
    """

class ImageAnalyzer:
    """
    ç±»æè¿°
    """

class ImageProcessor:
    """
    ç±»æè¿°
    """

class DuplicateDetector:
    """
    ç±»æè¿°
    """

class FileNameHandler:
    """
    ç±»æè¿°
    """

class DirectoryHandler:
    """
    ç±»æè¿°
    """

class ArchiveExtractor:
    """
    ç±»æè¿°
    """

class ArchiveCompressor:
    """
    ç±»æè¿°
    """

class ArchiveProcessor:
    """
    ç±»æè¿°
    """

class ProcessedLogHandler:
    """
    ç±»æè¿°
    """

class BackupHandler:
    """
    ç±»æè¿°
    """

class ContentFilter:
    """
    ç±»æè¿°
    """

class ProgressTracker:
    """
    ç±»æè¿°
    """

class InputHandler:
    """
    ç±»æè¿°
    """

class ProcessManager:
    """
    ç±»æè¿°
    """

class MangaClassifier:
    """
    ç±»æè¿°
    """

class Utils:
    """
    ç±»æè¿°
    """

class UnclassifiedFunctions:
    """
    æœªåˆ†ç±»çš„å‡½æ•°é›†åˆ
    """

    def normalize_filename(self, filename):
        """å»é™¤æ–‡ä»¶åä¸­çš„åœ†æ‹¬å·ã€æ–¹æ‹¬å·åŠå…¶å†…å®¹ï¼Œè¿”å›è§„èŒƒåŒ–çš„æ–‡ä»¶å"""
        name = os.path.splitext(filename)[0]
        name = re.sub('\\[[^\\]]*\\]', '', name)
        name = re.sub('\\([^)]*\\)', '', name)
        name = re.sub('vol\\.?|ç¬¬|å·»|å·', '', name, flags=re.IGNORECASE)
        name = re.sub('[\\s!ï¼?ï¼Ÿ_~ï½]+', ' ', name)
        name = name.strip()
        return name

    def is_similar_to_existing_folder(self, dir_path, series_name, handler=None):
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸ä¼¼çš„æ–‡ä»¶å¤¹åç§°"""
        try:
            existing_folders = [d for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]
        except Exception as e:
            logger.error(f'[#update] âŒ è¯»å–ç›®å½•å¤±è´¥: {dir_path}')
            return False
        series_key = UnclassifiedFunctions.get_series_key(series_name)
        for folder in existing_folders:
            is_series_folder = False
            folder_name = folder
            for prefix in SERIES_PREFIXES:
                if folder.startswith(prefix):
                    folder_name = folder[len(prefix):]
                    is_series_folder = True
                    break
            if is_series_folder:
                folder_key = UnclassifiedFunctions.get_series_key(folder_name, handler)
                if series_key == folder_key:
                    if handler:
                        handler.update_panel('update_log', f"ğŸ“ æ‰¾åˆ°ç›¸åŒç³»åˆ—æ–‡ä»¶å¤¹: '{folder}'")
                    return True
                similarity = UnclassifiedFunctions.calculate_similarity(series_key, folder_key, handler)
                if similarity >= SIMILARITY_CONFIG['THRESHOLD']:
                    if handler:
                        handler.update_panel('update_log', f"ğŸ“ æ‰¾åˆ°ç›¸ä¼¼æ–‡ä»¶å¤¹: '{folder}'")
                    return True
            else:
                similarity = UnclassifiedFunctions.calculate_similarity(series_name, folder, handler)
                if similarity >= SIMILARITY_CONFIG['THRESHOLD']:
                    if handler:
                        handler.update_panel('update_log', f"ğŸ“ æ‰¾åˆ°ç›¸ä¼¼æ–‡ä»¶å¤¹: '{folder}'")
                    return True
        return False

    def preprocess_filenames(self, files, handler=None):
        """é¢„å¤„ç†æ‰€æœ‰æ–‡ä»¶å"""
        file_keys = {}
        for file_path in files:
            key = UnclassifiedFunctions.get_series_key(os.path.basename(file_path))
            file_keys[file_path] = key
            logger.info(f'[#update] ğŸ”„ é¢„å¤„ç†: {os.path.basename(file_path)} -> {key}')
        return file_keys

    def find_similar_files(self, current_file, files, file_keys, processed_files, handler=None):
        """æŸ¥æ‰¾ä¸å½“å‰æ–‡ä»¶ç›¸ä¼¼çš„æ–‡ä»¶"""
        current_key = file_keys[current_file]
        similar_files = [current_file]
        to_process = set()
        if not current_key.strip():
            return (similar_files, to_process)
        for other_file in files:
            if other_file in processed_files or other_file == current_file:
                continue
            if UnclassifiedFunctions.is_in_series_folder(other_file):
                continue
            if UnclassifiedFunctions.is_essentially_same_file(current_file, other_file):
                to_process.add(other_file)
                continue
            other_key = file_keys[other_file]
            if not other_key.strip():
                continue
            ratio = fuzz.ratio(current_key.lower(), other_key.lower())
            partial = fuzz.partial_ratio(current_key.lower(), other_key.lower())
            token = fuzz.token_sort_ratio(current_key.lower(), other_key.lower())
            len_diff = abs(len(current_key) - len(other_key)) / max(len(current_key), len(other_key))
            is_similar = ratio >= SIMILARITY_CONFIG['RATIO_THRESHOLD'] and partial >= SIMILARITY_CONFIG['PARTIAL_THRESHOLD'] and (token >= SIMILARITY_CONFIG['TOKEN_THRESHOLD']) and (len_diff <= SIMILARITY_CONFIG['LENGTH_DIFF_MAX'])
            if is_similar:
                logger.info(f'[#update] âœ¨ å‘ç°ç›¸ä¼¼æ–‡ä»¶: {os.path.basename(other_file)} (ç›¸ä¼¼åº¦: {max(ratio, partial, token)}%)')
                similar_files.append(other_file)
                to_process.add(other_file)
        return (similar_files, to_process)

    def find_keyword_based_groups(self, remaining_files, file_keys, processed_files, handler=None):
        """åŸºäºå…³é”®è¯æŸ¥æ‰¾ç³»åˆ—ç»„"""
        keyword_groups = defaultdict(list)
        file_keywords = {}
        to_process = set()
        for file_path in remaining_files:
            if file_path in processed_files:
                continue
            keywords = UnclassifiedFunctions.extract_keywords(os.path.basename(file_path))
            if len(keywords) >= 1:
                file_keywords[file_path] = keywords
    
        def process_file_keywords(self, args):
            file_path, keywords = args
            if file_path in processed_files:
                return None
            current_group = [file_path]
            group_keywords = set(keywords)
            current_to_process = set()
            for other_path, other_keywords in file_keywords.items():
                if other_path == file_path or other_path in processed_files:
                    continue
                common_keywords = set(keywords) & set(other_keywords)
                if common_keywords and any((len(k) > 1 for k in common_keywords)):
                    current_group.append(other_path)
                    current_to_process.add(other_path)
                    group_keywords &= set(other_keywords)
            if len(current_group) > 1:
                series_name = max(group_keywords, key=len) if group_keywords else max(keywords, key=len)
                return (series_name, current_group, current_to_process)
            return None
        with ThreadPoolExecutor(max_workers=16) as executor:
            results = list(executor.map(process_file_keywords, file_keywords.items()))
        for result in results:
            if result:
                series_name, group, current_to_process = result
                logger.info(f'[#update] ğŸ“š å‘ç°ç³»åˆ—: {series_name} ({len(group)}ä¸ªæ–‡ä»¶)')
                for file_path in group:
                    logger.info(f'[#update]   â””â”€ {os.path.basename(file_path)}')
                keyword_groups[series_name] = group
                to_process.update(current_to_process)
                to_process.add(group[0])
        return (keyword_groups, to_process)

    def main(self):
        if sys.platform == 'win32':
            try:
                import ctypes
                kernel32 = ctypes.windll.kernel32
                kernel32.SetConsoleCP(65001)
                kernel32.SetConsoleOutputCP(65001)
            except:
                print('æ— æ³•è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8')
        paths, args = UnclassifiedFunctions.process_args()
        UnclassifiedFunctions.run_classifier(paths, args)

    def decorator(self, func):
    
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
    
            def handler(self, signum, frame):
                raise TimeoutError(f'å‡½æ•°æ‰§è¡Œè¶…æ—¶ ({seconds}ç§’)')
            if sys.platform != 'win32':
                original_handler = signal.signal(signal.SIGALRM, handler)
                signal.alarm(seconds)
            else:
                timer = threading.Timer(seconds, lambda: threading._shutdown())
                timer.start()
            try:
                result = func(*args, **kwargs)
            finally:
                if sys.platform != 'win32':
                    signal.alarm(0)
                    signal.signal(signal.SIGALRM, original_handler)
                else:
                    timer.cancel()
            return result
        return wrapper

    def process_file_keywords(self, args):
        file_path, keywords = args
        if file_path in processed_files:
            return None
        current_group = [file_path]
        group_keywords = set(keywords)
        current_to_process = set()
        for other_path, other_keywords in file_keywords.items():
            if other_path == file_path or other_path in processed_files:
                continue
            common_keywords = set(keywords) & set(other_keywords)
            if common_keywords and any((len(k) > 1 for k in common_keywords)):
                current_group.append(other_path)
                current_to_process.add(other_path)
                group_keywords &= set(other_keywords)
        if len(current_group) > 1:
            series_name = max(group_keywords, key=len) if group_keywords else max(keywords, key=len)
            return (series_name, current_group, current_to_process)
        return None

    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
    
        def handler(self, signum, frame):
            raise TimeoutError(f'å‡½æ•°æ‰§è¡Œè¶…æ—¶ ({seconds}ç§’)')
        if sys.platform != 'win32':
            original_handler = signal.signal(signal.SIGALRM, handler)
            signal.alarm(seconds)
        else:
            timer = threading.Timer(seconds, lambda: threading._shutdown())
            timer.start()
        try:
            result = func(*args, **kwargs)
        finally:
            if sys.platform != 'win32':
                signal.alarm(0)
                signal.signal(signal.SIGALRM, original_handler)
            else:
                timer.cancel()
        return result

    def handler(self, signum, frame):
        raise TimeoutError(f'å‡½æ•°æ‰§è¡Œè¶…æ—¶ ({seconds}ç§’)')

    def calculate_similarity(self, str1, str2, handler=None):
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦"""
        str1 = UnclassifiedFunctions.normalize_chinese(str1)
        str2 = UnclassifiedFunctions.normalize_chinese(str2)
        ratio = fuzz.ratio(str1.lower(), str2.lower())
        partial = fuzz.partial_ratio(str1.lower(), str2.lower())
        token = fuzz.token_sort_ratio(str1.lower(), str2.lower())
        max_similarity = max(ratio, partial, token)
        if max_similarity >= SIMILARITY_CONFIG['THRESHOLD']:
            logger.info(f'[#update] ğŸ” ç›¸ä¼¼åº¦: {max_similarity}%')
        return max_similarity

    def is_in_series_folder(self, file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»åœ¨ç³»åˆ—æ–‡ä»¶å¤¹å†…"""
        parent_dir = os.path.dirname(file_path)
        parent_name = os.path.basename(parent_dir)
        for prefix in SERIES_PREFIXES:
            if parent_name.startswith(prefix):
                series_name = parent_name[len(prefix):]
                parent_key = UnclassifiedFunctions.get_series_key(series_name)
                file_key = UnclassifiedFunctions.get_series_key(os.path.basename(file_path))
                return parent_key == file_key
        parent_key = UnclassifiedFunctions.get_series_key(parent_name)
        file_key = UnclassifiedFunctions.get_series_key(os.path.basename(file_path))
        if parent_key and parent_key in file_key:
            return True
        return False

    def is_essentially_same_file(self, file1, file2):
        """æ£€æŸ¥ä¸¤ä¸ªæ–‡ä»¶æ˜¯å¦æœ¬è´¨ä¸Šæ˜¯åŒä¸€ä¸ªæ–‡ä»¶ï¼ˆåªæ˜¯æ ‡ç­¾ä¸åŒï¼‰"""
        name1 = os.path.splitext(os.path.basename(file1))[0]
        name2 = os.path.splitext(os.path.basename(file2))[0]
        if name1 == name2:
            return True
        base1 = re.sub('\\[[^\\]]*\\]|\\([^)]*\\)', '', name1)
        base2 = re.sub('\\[[^\\]]*\\]|\\([^)]*\\)', '', name2)
        base1 = re.sub('[\\s]+', '', base1).lower()
        base2 = re.sub('[\\s]+', '', base2).lower()
        base1 = UnclassifiedFunctions.normalize_chinese(base1)
        base2 = UnclassifiedFunctions.normalize_chinese(base2)
        return base1 == base2

    def extract_keywords(self, filename):
        """ä»æ–‡ä»¶åä¸­æå–å…³é”®è¯"""
        name = UnclassifiedFunctions.get_base_filename(filename)
        separators = '[\\s]+'
        keywords = []
        name = re.sub('\\[[^\\]]*\\]|\\([^)]*\\)', ' ', name)
        parts = [p.strip() for p in re.split(separators, name) if p.strip()]
        for part in parts:
            if len(part) > 1:
                keywords.append(part)
        return keywords

    def process_args(self):
        """å¤„ç†å‘½ä»¤è¡Œå‚æ•°"""
        parser = argparse.ArgumentParser(description='æ¼«ç”»å‹ç¼©åŒ…åˆ†ç±»å·¥å…·')
        parser.add_argument('paths', nargs='*', help='è¦å¤„ç†çš„è·¯å¾„åˆ—è¡¨')
        parser.add_argument('-c', '--clipboard', action='store_true', help='ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
        parser.add_argument('-f', '--features', type=str, default='', help='å¯ç”¨çš„åŠŸèƒ½ï¼ˆ1-4ï¼Œç”¨é€—å·åˆ†éš”ï¼‰ï¼š1=åˆ†ç±»ï¼Œ2=ç³»åˆ—æå–ï¼Œ3=åˆ é™¤ç©ºæ–‡ä»¶å¤¹ï¼Œ4=åºå·ä¿®å¤ã€‚é»˜è®¤å…¨éƒ¨å¯ç”¨')
        parser.add_argument('--similarity', type=float, default=80, help='è®¾ç½®åŸºæœ¬ç›¸ä¼¼åº¦é˜ˆå€¼(0-100)ï¼Œé»˜è®¤80')
        parser.add_argument('--ratio', type=float, default=75, help='è®¾ç½®å®Œå…¨åŒ¹é…é˜ˆå€¼(0-100)ï¼Œé»˜è®¤75')
        parser.add_argument('--partial', type=float, default=85, help='è®¾ç½®éƒ¨åˆ†åŒ¹é…é˜ˆå€¼(0-100)ï¼Œé»˜è®¤85')
        parser.add_argument('--token', type=float, default=80, help='è®¾ç½®æ ‡è®°åŒ¹é…é˜ˆå€¼(0-100)ï¼Œé»˜è®¤80')
        parser.add_argument('--length-diff', type=float, default=0.3, help='è®¾ç½®é•¿åº¦å·®å¼‚æœ€å¤§å€¼(0-1)ï¼Œé»˜è®¤0.3')
        parser.add_argument('--wait', action='store_true', help='å¤„ç†å®Œæ¯ä¸ªè·¯å¾„åç­‰å¾…ç”¨æˆ·ç¡®è®¤')
        if len(sys.argv) == 1 or (len(sys.argv) == 2 and sys.argv[1] in ['-c', '--clipboard']):
            presets = {'é»˜è®¤é…ç½®': {'features': '1,2,3,4', 'similarity': '80', 'ratio': '75', 'partial': '85', 'token': '80', 'length_diff': '0.3', 'clipboard': True, 'wait': False}, 'ä»…åˆ†ç±»': {'features': '1', 'similarity': '80', 'ratio': '75', 'partial': '85', 'token': '80', 'length_diff': '0.3', 'clipboard': True, 'wait': False}, 'ä»…ç³»åˆ—æå–': {'features': '2', 'similarity': '80', 'ratio': '75', 'partial': '85', 'token': '80', 'length_diff': '0.3', 'clipboard': True, 'wait': False}, 'åˆ†ç±»+ç³»åˆ—': {'features': '1,2', 'similarity': '80', 'ratio': '75', 'partial': '85', 'token': '80', 'length_diff': '0.3', 'clipboard': True, 'wait': False}}
            checkbox_options = [('ä»å‰ªè´´æ¿è¯»å–', 'clipboard', '-c', True), ('åˆ†ç±»åŠŸèƒ½', 'feature1', '-f 1'), ('ç³»åˆ—æå–', 'feature2', '-f 2'), ('åˆ é™¤ç©ºæ–‡ä»¶å¤¹', 'feature3', '-f 3'), ('åºå·ä¿®å¤', 'feature4', '-f 4'), ('ç­‰å¾…ç¡®è®¤', 'wait', '--wait', False)]
            input_options = [('åŸºæœ¬ç›¸ä¼¼åº¦é˜ˆå€¼', 'similarity', '--similarity', '80', '0-100'), ('å®Œå…¨åŒ¹é…é˜ˆå€¼', 'ratio', '--ratio', '75', '0-100'), ('éƒ¨åˆ†åŒ¹é…é˜ˆå€¼', 'partial', '--partial', '85', '0-100'), ('æ ‡è®°åŒ¹é…é˜ˆå€¼', 'token', '--token', '80', '0-100'), ('é•¿åº¦å·®å¼‚æœ€å¤§å€¼', 'length_diff', '--length-diff', '0.3', '0-1')]
            app = create_config_app(program=__file__, checkbox_options=checkbox_options, input_options=input_options, title='æ¼«ç”»å‹ç¼©åŒ…åˆ†ç±»å·¥å…·é…ç½®', preset_configs=presets)
            app.run()
            return (None, None)
        args = parser.parse_args()
        if args.clipboard:
            try:
                import pyperclip
                clipboard_content = pyperclip.paste().strip()
                if clipboard_content:
                    args.paths.extend([p.strip() for p in clipboard_content.replace('\r\n', '\n').split('\n') if p.strip()])
                    print('ä»å‰ªè´´æ¿è¯»å–åˆ°ä»¥ä¸‹è·¯å¾„ï¼š')
                    for path in args.paths:
                        print(path)
            except ImportError:
                print('æœªå®‰è£… pyperclip æ¨¡å—ï¼Œæ— æ³•ä»å‰ªè´´æ¿è¯»å–è·¯å¾„')
        return (args.paths, args)

    def run_classifier(self, paths, args):
        """è¿è¡Œåˆ†ç±»å™¨ä¸»é€»è¾‘"""
        if not paths or not args:
            return
        similarity_config = {'THRESHOLD': args.similarity, 'RATIO_THRESHOLD': args.ratio, 'PARTIAL_THRESHOLD': args.partial, 'TOKEN_THRESHOLD': args.token, 'LENGTH_DIFF_MAX': args.length_diff}
        SIMILARITY_CONFIG.update(similarity_config)
        enabled_features = set()
        if args.features:
            try:
                enabled_features = {int(f.strip()) for f in args.features.split(',') if f.strip()}
                for f in enabled_features.copy():
                    if f not in {1, 2, 3, 4}:
                        print(f'æ— æ•ˆçš„åŠŸèƒ½ç¼–å·: {f}')
                        enabled_features.remove(f)
            except ValueError:
                print('æ— æ•ˆçš„åŠŸèƒ½ç¼–å·æ ¼å¼ï¼Œå°†å¯ç”¨æ‰€æœ‰åŠŸèƒ½')
                enabled_features = {1, 2, 3, 4}
        else:
            enabled_features = {1, 2, 3, 4}
        UnclassifiedFunctions.process_paths(paths, enabled_features=enabled_features, wait_for_confirm=args.wait)

    def process_paths(self, paths, enabled_features=None, similarity_config=None, wait_for_confirm=False):
        """å¤„ç†è¾“å…¥çš„è·¯å¾„åˆ—è¡¨"""
        UnclassifiedFunctions.init_TextualLogger()
        if similarity_config:
            SIMILARITY_CONFIG.update(similarity_config)
        valid_paths = []
        for path in paths:
            path = path.strip().strip('"').strip("'")
            if path:
                try:
                    if sys.platform == 'win32':
                        if UnclassifiedFunctions.win32_path_exists(path):
                            valid_paths.append(path)
                        else:
                            print(f'âŒ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: {path}')
                    elif os.path.exists(path):
                        valid_paths.append(path)
                    else:
                        print(f'âŒ è·¯å¾„ä¸å­˜åœ¨: {path}')
                except Exception as e:
                    print(f'âŒ å¤„ç†è·¯å¾„æ—¶å‡ºé”™: {path}, é”™è¯¯: {str(e)}')
        if not valid_paths:
            print('âŒ æ²¡æœ‰æœ‰æ•ˆçš„è·¯å¾„')
            return
        total_paths = len(valid_paths)
        print(f"\nğŸš€ å¼€å§‹{('å¤„ç†' if wait_for_confirm else 'æ‰¹é‡å¤„ç†')} {total_paths} ä¸ªè·¯å¾„...")
        if not wait_for_confirm:
            print('è·¯å¾„åˆ—è¡¨:')
            for path in valid_paths:
                print(f'  - {path}')
            print()
        UnclassifiedFunctions.init_TextualLogger()
        for i, path in enumerate(valid_paths, 1):
            try:
                if wait_for_confirm:
                    logger.info(f'[#current_progress] ğŸ“ å¤„ç†ç¬¬ {i}/{total_paths} ä¸ªè·¯å¾„: {path}')
                else:
                    logger.info(f'[#current_progress] å¤„ç†: {os.path.basename(path)}')
                if sys.platform == 'win32':
                    if UnclassifiedFunctions.win32_path_exists(path):
                        if os.path.isdir(path):
                            UnclassifiedFunctions.process_directory(path, enabled_features=enabled_features)
                        elif os.path.isfile(path) and UnclassifiedFunctions.is_archive(path):
                            if 1 in enabled_features:
                                if wait_for_confirm:
                                    logger.info(f'[#current_progress] ğŸ“¦ å¤„ç†å•ä¸ªæ–‡ä»¶: {path}')
                                UnclassifiedFunctions.process_single_file(path)
                                if wait_for_confirm:
                                    logger.info('[#update] âœ¨ æ–‡ä»¶å¤„ç†å®Œæˆ')
                elif os.path.isdir(path):
                    UnclassifiedFunctions.process_directory(path, enabled_features=enabled_features)
                elif os.path.isfile(path) and UnclassifiedFunctions.is_archive(path):
                    if 1 in enabled_features:
                        if wait_for_confirm:
                            logger.info(f'[#current_progress] ğŸ“¦ å¤„ç†å•ä¸ªæ–‡ä»¶: {path}')
                        UnclassifiedFunctions.process_single_file(path)
                        if wait_for_confirm:
                            logger.info('[#update] âœ¨ æ–‡ä»¶å¤„ç†å®Œæˆ')
                if wait_for_confirm and i < total_paths:
                    logger.info(f'[#current_progress] â¸ï¸ å·²å¤„ç†å®Œç¬¬ {i}/{total_paths} ä¸ªè·¯å¾„')
                    input('æŒ‰å›è½¦é”®ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè·¯å¾„...')
            except Exception as e:
                logger.error(f'[#update] âŒ å¤„ç†è·¯å¾„æ—¶å‡ºé”™: {path}, é”™è¯¯: {str(e)}')
                if wait_for_confirm and i < total_paths:
                    logger.warning('[#update] âš ï¸ å¤„ç†å‡ºé”™ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ')
                    input('æŒ‰å›è½¦é”®ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè·¯å¾„ï¼ŒæŒ‰ Ctrl+C ç»ˆæ­¢ç¨‹åº...')
        if wait_for_confirm:
            logger.info('[#update] âœ… æ‰€æœ‰è·¯å¾„å¤„ç†å®Œæˆï¼')
        else:
            logger.info(f'[#update] âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼å…±å¤„ç† {total_paths} ä¸ªè·¯å¾„')

    def win32_path_exists(self, path):
        try:
            win32api.GetFileAttributes(path)
            return True
        except:
            print('æœªå®‰è£…win32apiæ¨¡å—ï¼ŒæŸäº›è·¯å¾„å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†')
            win32_path_exists = os.path.exists

    def process_directory(self, directory_path, progress_task=None, enabled_features=None, handler=None):
        """å¤„ç†ç›®å½•ä¸‹çš„å‹ç¼©åŒ…"""
        try:
            if enabled_features is None:
                enabled_features = {1, 2, 3, 4}
            abs_dir_path = UnclassifiedFunctions.validate_directory(directory_path)
            if not abs_dir_path:
                return []
            UnclassifiedFunctions.init_TextualLogger()
            try:
                logger.info(f'[#process] ğŸ“‚ å¼€å§‹å¤„ç†ç›®å½•: {abs_dir_path}')
                if 2 in enabled_features:
                    logger.info('[#process] ğŸ”„ æ£€æŸ¥å¹¶æ›´æ–°æ—§çš„ç³»åˆ—æ–‡ä»¶å¤¹åç§°...')
                    UnclassifiedFunctions.update_all_series_folders(abs_dir_path)
                if 1 in enabled_features:
                    UnclassifiedFunctions.create_category_folders(abs_dir_path)
                category_folders = set(CATEGORY_RULES.keys())
                found_archives = False
                if 2 in enabled_features:
                    logger.info('[#process] ğŸ” å¼€å§‹æŸ¥æ‰¾å¯æå–ç³»åˆ—çš„å‹ç¼©åŒ…...')
                    archives = UnclassifiedFunctions.collect_archives_for_series(abs_dir_path, category_folders)
                    if archives:
                        found_archives = True
                        total_archives = len(archives)
                        logger.info(f"[#update] âœ¨ åœ¨ç›®å½• '{abs_dir_path}' åŠå…¶å­æ–‡ä»¶å¤¹ä¸‹æ‰¾åˆ° {total_archives} ä¸ªæœ‰æ•ˆå‹ç¼©åŒ…")
                        UnclassifiedFunctions.create_series_folders(abs_dir_path, archives)
                        logger.info('[#current_progress] ç³»åˆ—æå–å®Œæˆ')
                    else:
                        logger.info('[#process] æ²¡æœ‰æ‰¾åˆ°å¯æå–ç³»åˆ—çš„å‹ç¼©åŒ…')
                if 1 in enabled_features:
                    logger.info('[#process] ğŸ” å¼€å§‹æŸ¥æ‰¾éœ€è¦åˆ†ç±»çš„å‹ç¼©åŒ…...')
                    archives = UnclassifiedFunctions.collect_archives_for_category(abs_dir_path, category_folders)
                    if archives:
                        found_archives = True
                        total_archives = len(archives)
                        logger.info(f"[#update] âœ¨ åœ¨ç›®å½• '{abs_dir_path}' ä¸‹æ‰¾åˆ° {total_archives} ä¸ªæœ‰æ•ˆå‹ç¼©åŒ…")
                        for i, archive in enumerate(archives, 1):
                            percentage = i / total_archives * 100
                            progress_text = f'æ­£åœ¨åˆ†ç±»å‹ç¼©åŒ…... {percentage:.1f}% ({i}/{total_archives})'
                            logger.info(f'[#current_progress] {progress_text}')
                            logger.info(f'[#process] å¤„ç†: {os.path.basename(archive)}')
                            UnclassifiedFunctions.process_single_file(archive)
                    else:
                        logger.info('[#process] æ²¡æœ‰æ‰¾åˆ°éœ€è¦åˆ†ç±»çš„å‹ç¼©åŒ…')
                if 3 in enabled_features or 4 in enabled_features:
                    logger.info('[#post_process] ğŸ”§ å¼€å§‹è¿è¡Œåç»­å¤„ç†...')
                    UnclassifiedFunctions.run_post_processing(abs_dir_path, enabled_features)
                if not found_archives:
                    logger.info(f"[#process] åœ¨ç›®å½• '{abs_dir_path}' ä¸‹æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„å‹ç¼©åŒ…")
                logger.info(f'[#process] âœ¨ ç›®å½•å¤„ç†å®Œæˆ: {abs_dir_path}')
            except Exception as e:
                logger.error(f'[#update] âŒ å¤„ç†ç›®å½•æ—¶å‡ºé”™ {directory_path}: {str(e)}')
                logger.error(f'[#process] âŒ å¤„ç†å‡ºé”™: {os.path.basename(directory_path)}')
            return []
        except Exception as e:
            logger.error(f'[#update] âŒ å¤„ç†ç›®å½•æ—¶å‡ºé”™ {directory_path}: {str(e)}')
            logger.error(f'[#process] âŒ å¤„ç†å‡ºé”™: {os.path.basename(directory_path)}')
            return []

    def validate_directory(self, directory_path, handler=None):
        """éªŒè¯ç›®å½•æ˜¯å¦æœ‰æ•ˆä¸”ä¸åœ¨é»‘åå•ä¸­"""
        abs_dir_path = os.path.abspath(directory_path)
        if not os.path.isdir(abs_dir_path):
            logger.error(f'[#update] âŒ ä¸æ˜¯æœ‰æ•ˆçš„ç›®å½•è·¯å¾„: {abs_dir_path}')
            return None
        if UnclassifiedFunctions.is_path_blacklisted(abs_dir_path):
            logger.warning(f'[#update] âš ï¸ ç›®å½•åœ¨é»‘åå•ä¸­ï¼Œè·³è¿‡å¤„ç†: {abs_dir_path}')
            return None
        return abs_dir_path

    def init_TextualLogger(self):
        """åˆå§‹åŒ–TextualLogger"""
        TextualLoggerManager.set_layout(TEXTUAL_LAYOUT, config_info['log_file'])

    def update_all_series_folders(self, directory_path, handler=None):
        """æ›´æ–°ç›®å½•ä¸‹æ‰€æœ‰çš„ç³»åˆ—æ–‡ä»¶å¤¹åç§°"""
        try:
            updated_count = 0
            for root, dirs, _ in os.walk(directory_path):
                for dir_name in dirs:
                    if dir_name.startswith('[#s]'):
                        full_path = os.path.join(root, dir_name)
                        if UnclassifiedFunctions.update_series_folder_name(full_path):
                            updated_count += 1
            if updated_count > 0:
                logger.info(f'[#update] âœ¨ æ›´æ–°äº† {updated_count} ä¸ªç³»åˆ—æ–‡ä»¶å¤¹åç§°')
            return updated_count
        except Exception as e:
            logger.error(f'[#update] âŒ æ›´æ–°ç³»åˆ—æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}')
            return 0

    def collect_archives_for_series(self, directory_path, category_folders, handler=None):
        """æ”¶é›†ç”¨äºç³»åˆ—æå–çš„å‹ç¼©åŒ…"""
        base_level = len(Path(directory_path).parts)
        archives = []
        archives_to_check = []
        for root, _, files in os.walk(directory_path):
            current_level = len(Path(root).parts)
            if current_level - base_level > 1:
                continue
            if UnclassifiedFunctions.is_path_blacklisted(root):
                logger.warning(f'[#update] âš ï¸ ç›®å½•åœ¨é»‘åå•ä¸­ï¼Œè·³è¿‡: {root}')
                continue
            current_dir = os.path.basename(root)
            if current_dir.startswith('[#s]') or current_dir == 'æŸåå‹ç¼©åŒ…':
                continue
            for file in files:
                if UnclassifiedFunctions.is_archive(file):
                    file_path = os.path.join(root, file)
                    if UnclassifiedFunctions.is_series_blacklisted(file):
                        logger.warning(f'[#update] âš ï¸ æ–‡ä»¶åœ¨ç³»åˆ—æå–é»‘åå•ä¸­ï¼Œè·³è¿‡: {file}')
                        continue
                    if UnclassifiedFunctions.is_path_blacklisted(file):
                        logger.warning(f'[#update] âš ï¸ æ–‡ä»¶åœ¨é»‘åå•ä¸­ï¼Œè·³è¿‡: {file}')
                        continue
                    archives_to_check.append(file_path)
        if archives_to_check:
            logger.info(f'[#update] ğŸ” æ­£åœ¨æ£€æŸ¥ {len(archives_to_check)} ä¸ªå‹ç¼©åŒ…çš„å®Œæ•´æ€§...')
            with ThreadPoolExecutor(max_workers=16) as executor:
                futures = {executor.submit(is_archive_corrupted, path): path for path in archives_to_check}
                for i, future in enumerate(futures, 1):
                    path = futures[future]
                    percentage = i / len(archives_to_check) * 100
                    if handler:
                        percentage = i / len(archives_to_check) * 100
                        handler.update_panel('current_task', f'æ£€æµ‹å‹ç¼©åŒ…å®Œæ•´æ€§... ({i}/{len(archives_to_check)}) {percentage:.1f}%')
                    try:
                        is_corrupted = future.result()
                        if is_corrupted:
                            if handler:
                                handler.update_panel('update_log', f'âš ï¸ å‹ç¼©åŒ…å·²æŸå: {os.path.basename(path)}')
                            UnclassifiedFunctions.move_corrupted_archive(path, directory_path, handler)
                        else:
                            archives.append(path)
                    except TimeoutError:
                        if handler:
                            handler.update_panel('update_log', f'âš ï¸ å‹ç¼©åŒ…å¤„ç†è¶…æ—¶: {os.path.basename(path)}')
                        UnclassifiedFunctions.move_corrupted_archive(path, directory_path, handler)
                    except Exception as e:
                        if handler:
                            handler.update_panel('update_log', f'âŒ æ£€æŸ¥å‹ç¼©åŒ…æ—¶å‡ºé”™: {os.path.basename(path)}')
                        UnclassifiedFunctions.move_corrupted_archive(path, directory_path, handler)
        return archives

    def collect_archives_for_category(self, directory_path, category_folders, handler=None):
        """æ”¶é›†ç”¨äºåˆ†ç±»çš„å‹ç¼©åŒ…"""
        archives = []
        archives_to_check = []
        with os.scandir(directory_path) as entries:
            for entry in entries:
                if entry.is_file() and UnclassifiedFunctions.is_archive(entry.name):
                    parent_dir = os.path.basename(os.path.dirname(entry.path))
                    if parent_dir == 'æŸåå‹ç¼©åŒ…' or parent_dir in category_folders:
                        continue
                    archives_to_check.append(entry.path)
        if archives_to_check:
            logger.info(f'[#update] ğŸ” æ­£åœ¨æ£€æŸ¥ {len(archives_to_check)} ä¸ªå‹ç¼©åŒ…çš„å®Œæ•´æ€§...')
            with ThreadPoolExecutor(max_workers=16) as executor:
                futures = {executor.submit(is_archive_corrupted, path): path for path in archives_to_check}
                for i, future in enumerate(futures, 1):
                    path = futures[future]
                    percentage = i / len(archives_to_check) * 100
                    logger.info(f'[#current_progress] æ£€æµ‹å‹ç¼©åŒ…å®Œæ•´æ€§... ({i}/{len(archives_to_check)}) {percentage:.1f}%')
                    try:
                        is_corrupted = future.result()
                        if not is_corrupted:
                            archives.append(path)
                        else:
                            logger.warning(f'[#update] âš ï¸ å‹ç¼©åŒ…å·²æŸåï¼Œè·³è¿‡: {os.path.basename(path)}')
                    except TimeoutError:
                        logger.warning(f'[#update] âš ï¸ å‹ç¼©åŒ…å¤„ç†è¶…æ—¶ï¼Œè·³è¿‡: {os.path.basename(path)}')
                    except Exception as e:
                        logger.error(f'[#update] âŒ æ£€æŸ¥å‹ç¼©åŒ…æ—¶å‡ºé”™: {os.path.basename(path)}')
        return archives

    def run_post_processing(self, directory_path, enabled_features, handler=None):
        """è¿è¡Œåç»­å¤„ç†è„šæœ¬ï¼ˆåˆ é™¤ç©ºæ–‡ä»¶å¤¹å’Œåºå·ä¿®å¤ï¼‰"""
        if 3 in enabled_features:
            try:
                handler.update_panel('post_process', 'ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤ç©ºæ–‡ä»¶å¤¹...')
                result = subprocess.run(f'python "D:\\1VSCODE\\1ehv\\archive\\013-åˆ é™¤ç©ºæ–‡ä»¶å¤¹é‡Šæ”¾å•ç‹¬æ–‡ä»¶å¤¹.py" "{directory_path}" -r', shell=True, capture_output=True, text=True, encoding='utf-8')
                if result.returncode != 0:
                    raise subprocess.CalledProcessError(result.returncode, result.args)
                handler.update_panel('post_process', 'âœ… ç©ºæ–‡ä»¶å¤¹å¤„ç†å®Œæˆ')
            except subprocess.CalledProcessError as e:
                if handler:
                    handler.update_panel('update_log', f'âŒ è¿è¡Œåˆ é™¤ç©ºæ–‡ä»¶å¤¹è„šæœ¬å¤±è´¥: {str(e)}')
                    handler.update_panel('post_process', 'âŒ ç©ºæ–‡ä»¶å¤¹å¤„ç†å¤±è´¥')
        if 4 in enabled_features:
            try:
                handler.update_panel('post_process', 'ğŸ”§ æ­£åœ¨ä¿®å¤åºå·...')
                result = subprocess.run(f'python "D:\\1VSCODE\\1ehv\\other\\012-æ–‡ä»¶å¤¹åºå·ä¿®å¤å·¥å…·.py" "{directory_path}"', shell=True, capture_output=True, text=True, encoding='utf-8')
                if result.returncode != 0:
                    raise subprocess.CalledProcessError(result.returncode, result.args)
                handler.update_panel('post_process', 'âœ… åºå·ä¿®å¤å®Œæˆ')
            except subprocess.CalledProcessError as e:
                if handler:
                    handler.update_panel('update_log', f'âŒ è¿è¡Œåºå·ä¿®å¤è„šæœ¬å¤±è´¥: {str(e)}')
                    handler.update_panel('post_process', 'âŒ åºå·ä¿®å¤å¤±è´¥')

    def create_series_folders(self, directory_path, archives, handler=None):
        """ä¸ºåŒä¸€ç³»åˆ—çš„æ–‡ä»¶åˆ›å»ºæ–‡ä»¶å¤¹"""
        dir_groups = defaultdict(list)
        archives = [f for f in archives if os.path.isfile(f) and UnclassifiedFunctions.is_archive(f)]
        for archive in archives:
            dir_path = os.path.dirname(archive)
            parent_name = os.path.basename(dir_path)
            is_series_dir = any((parent_name.startswith(prefix) for prefix in SERIES_PREFIXES))
            if is_series_dir:
                continue
            dir_groups[dir_path].append(archive)
        for dir_path, dir_archives in dir_groups.items():
            if len(dir_archives) <= 1:
                continue
            logger.info(f'[#update] æ‰¾åˆ° {len(dir_archives)} ä¸ªå‹ç¼©åŒ…')
            series_groups = UnclassifiedFunctions.find_series_groups(dir_archives)
            if series_groups:
                logger.info(f'[#update] ğŸ“š æ‰¾åˆ° {len(series_groups)} ä¸ªç³»åˆ—')
                total_files = len(dir_archives)
                for series_name, files in series_groups.items():
                    if series_name == 'å…¶ä»–':
                        continue
                    if len(files) == total_files:
                        logger.warning(f'[#update] âš ï¸ æ‰€æœ‰æ–‡ä»¶éƒ½å±äºåŒä¸€ä¸ªç³»åˆ—ï¼Œè·³è¿‡åˆ›å»ºå­æ–‡ä»¶å¤¹')
                        return
                series_folders = {}
                for series_name, files in series_groups.items():
                    if series_name == 'å…¶ä»–' or len(files) <= 1:
                        if series_name == 'å…¶ä»–':
                            logger.warning(f'[#update] âš ï¸ {len(files)} ä¸ªæ–‡ä»¶æœªèƒ½åŒ¹é…åˆ°ç³»åˆ—')
                        else:
                            logger.warning(f"[#update] âš ï¸ ç³»åˆ— '{series_name}' åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡åˆ›å»ºæ–‡ä»¶å¤¹")
                        continue
                    series_folder = os.path.join(dir_path, f'[#s]{series_name.strip()}')
                    if not os.path.exists(series_folder):
                        os.makedirs(series_folder)
                        logger.info(f'[#update] ğŸ“ åˆ›å»ºç³»åˆ—æ–‡ä»¶å¤¹: [#s]{series_name}')
                    series_folders[series_name] = series_folder
                for series_name, folder_path in series_folders.items():
                    files = series_groups[series_name]
                    logger.info(f"[#update] ğŸ“¦ å¼€å§‹ç§»åŠ¨ç³»åˆ— '{series_name}' çš„æ–‡ä»¶...")
                    for file_path in files:
                        target_path = os.path.join(folder_path, os.path.basename(file_path))
                        if not os.path.exists(target_path):
                            shutil.move(file_path, target_path)
                            logger.info(f'[#update]   â””â”€ ç§»åŠ¨: {os.path.basename(file_path)}')
                        else:
                            logger.warning(f"[#update] âš ï¸ æ–‡ä»¶å·²å­˜åœ¨äºç³»åˆ— '{series_name}': {os.path.basename(file_path)}")
                logger.info('[#current_progress] ç³»åˆ—æå–å®Œæˆ')
            logger.info(f'[#process] âœ¨ ç›®å½•å¤„ç†å®Œæˆ: {dir_path}')

    def process_single_file(self, abs_path, handler=None):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            if not os.path.exists(abs_path):
                logger.error(f'[#update] âŒ è·¯å¾„ä¸å­˜åœ¨: {abs_path}')
                return
            logger.info(f'[#current_progress] å¤„ç†æ–‡ä»¶: {os.path.basename(abs_path)}')
            logger.info(f'[#process] åˆ†æ: {os.path.basename(abs_path)}')
            UnclassifiedFunctions.create_category_folders(os.path.dirname(abs_path))
            category = UnclassifiedFunctions.get_category(abs_path)
            if category == 'æŸå':
                logger.warning(f'[#update] âš ï¸ å‹ç¼©åŒ…å·²æŸå: {os.path.basename(abs_path)}')
                logger.warning(f'[#process] âŒ æŸå: {os.path.basename(abs_path)}')
                UnclassifiedFunctions.move_corrupted_archive(abs_path, os.path.dirname(abs_path))
                return
            UnclassifiedFunctions.move_file_to_category(abs_path, category)
            logger.info(f'[#process] âœ… å®Œæˆ: {os.path.basename(abs_path)} -> {category}')
        except Exception as e:
            logger.error(f'[#update] âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {abs_path}: {str(e)}')
            logger.error(f'[#process] âŒ é”™è¯¯: {os.path.basename(abs_path)}')

    def update_series_folder_name(self, old_path, handler=None):
        """æ›´æ–°ç³»åˆ—æ–‡ä»¶å¤¹åç§°ä¸ºæœ€æ–°æ ‡å‡†"""
        try:
            dir_name = os.path.basename(old_path)
            is_series = False
            prefix_used = None
            for prefix in SERIES_PREFIXES:
                if dir_name.startswith(prefix):
                    is_series = True
                    prefix_used = prefix
                    break
            if not is_series:
                return False
            old_series_name = dir_name[len(prefix_used):]
            new_series_name = UnclassifiedFunctions.get_series_key(old_series_name)
            if not new_series_name or new_series_name == old_series_name:
                return False
            new_path = os.path.join(os.path.dirname(old_path), f'[#s]{new_series_name}')
            if os.path.exists(new_path) and os.path.abspath(new_path) != os.path.abspath(old_path):
                if handler:
                    handler.update_panel('update_log', f'âš ï¸ ç›®æ ‡è·¯å¾„å·²å­˜åœ¨: {new_path}')
                return False
            os.rename(old_path, new_path)
            if handler:
                handler.update_panel('update_log', f'ğŸ“ æ›´æ–°ç³»åˆ—æ–‡ä»¶å¤¹åç§°: {dir_name} -> [#s]{new_series_name}')
            return True
        except Exception as e:
            if handler:
                handler.update_panel('update_log', f'âŒ æ›´æ–°ç³»åˆ—æ–‡ä»¶å¤¹åç§°å¤±è´¥ {old_path}: {str(e)}')
            return False

    def is_path_blacklisted(self, path):
        """æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨é»‘åå•ä¸­"""
        path_lower = path.lower()
        return any((keyword.lower() in path_lower for keyword in PATH_BLACKLIST))

    def is_series_blacklisted(self, filename):
        """æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åœ¨ç³»åˆ—æå–é»‘åå•ä¸­"""
        for pattern in SERIES_BLACKLIST_PATTERNS:
            if re.search(pattern, filename, re.IGNORECASE):
                return True
        return False

    def create_category_folders(self, base_path, handler=None):
        """åœ¨æŒ‡å®šè·¯å¾„åˆ›å»ºåˆ†ç±»æ–‡ä»¶å¤¹"""
        for category in CATEGORY_RULES.keys():
            category_path = os.path.join(base_path, category)
            if not os.path.exists(category_path):
                os.makedirs(category_path)
                logger.info(f'[#update] ğŸ“ åˆ›å»ºåˆ†ç±»æ–‡ä»¶å¤¹: {category}')
        corrupted_path = os.path.join(base_path, 'æŸåå‹ç¼©åŒ…')
        if not os.path.exists(corrupted_path):
            os.makedirs(corrupted_path)
            logger.info(f'[#update] ğŸ“ åˆ›å»ºæŸåå‹ç¼©åŒ…æ–‡ä»¶å¤¹')

    def get_category(self, path, handler=None):
        """æ ¹æ®è·¯å¾„ååˆ¤æ–­ç±»åˆ«ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡ŒåŒ¹é…"""
        filename = os.path.basename(path)
        if not UnclassifiedFunctions.is_archive(path):
            return 'æœªåˆ†ç±»'
        if UnclassifiedFunctions.is_archive_corrupted(path):
            return 'æŸå'
        for pattern in CATEGORY_RULES['4. ç”»é›†']['patterns']:
            if re.search(pattern, filename, re.IGNORECASE):
                return '4. ç”»é›†'
        image_count = UnclassifiedFunctions.count_images_in_archive(path)
        if image_count == -1:
            return 'æŸå'
        logger.info(f"[#update] å‹ç¼©åŒ… '{filename}' ä¸­åŒ…å« {image_count} å¼ å›¾ç‰‡")
        if image_count >= 100:
            for category, rules in CATEGORY_RULES.items():
                if category == '4. ç”»é›†':
                    continue
                excluded = False
                for exclude_pattern in rules['exclude_patterns']:
                    if re.search(exclude_pattern, filename, re.IGNORECASE):
                        excluded = True
                        break
                if excluded:
                    continue
                for pattern in rules['patterns']:
                    if re.search(pattern, filename, re.IGNORECASE):
                        return category
            return '3. å•è¡Œæœ¬'
        for category, rules in CATEGORY_RULES.items():
            if category == '4. ç”»é›†':
                continue
            excluded = False
            for exclude_pattern in rules['exclude_patterns']:
                if re.search(exclude_pattern, filename, re.IGNORECASE):
                    excluded = True
                    break
            if excluded:
                continue
            for pattern in rules['patterns']:
                if re.search(pattern, filename, re.IGNORECASE):
                    return category
        return 'æœªåˆ†ç±»'

    def move_file_to_category(self, file_path, category, handler=None):
        """å°†æ–‡ä»¶ç§»åŠ¨åˆ°å¯¹åº”çš„åˆ†ç±»æ–‡ä»¶å¤¹"""
        if category == 'æœªåˆ†ç±»':
            logger.info(f"[#update] æ–‡ä»¶ '{file_path}' æœªèƒ½åŒ¹é…ä»»ä½•åˆ†ç±»è§„åˆ™ï¼Œä¿æŒåŸä½ç½®")
            return
        target_dir = os.path.join(os.path.dirname(file_path), category)
        target_path = os.path.join(target_dir, os.path.basename(file_path))
        if not os.path.exists(target_path):
            shutil.move(file_path, target_path)
            logger.info(f'[#update] å·²ç§»åŠ¨åˆ°: {target_path}')
        else:
            logger.info(f'[#update] ç›®æ ‡è·¯å¾„å·²å­˜åœ¨æ–‡ä»¶: {target_path}')

    def move_corrupted_archive(self, file_path, base_path, handler=None):
        """ç§»åŠ¨æŸåçš„å‹ç¼©åŒ…åˆ°æŸåå‹ç¼©åŒ…æ–‡ä»¶å¤¹ï¼Œä¿æŒåŸæœ‰ç›®å½•ç»“æ„"""
        try:
            rel_path = os.path.relpath(os.path.dirname(file_path), base_path)
            corrupted_base = os.path.join(base_path, 'æŸåå‹ç¼©åŒ…')
            target_dir = os.path.join(corrupted_base, rel_path)
            os.makedirs(target_dir, exist_ok=True)
            target_path = os.path.join(target_dir, os.path.basename(file_path))
            if os.path.exists(target_path):
                base, ext = os.path.splitext(target_path)
                counter = 1
                while os.path.exists(f'{base}_{counter}{ext}'):
                    counter += 1
                target_path = f'{base}_{counter}{ext}'
            shutil.move(file_path, target_path)
            logger.info(f'[#update] ğŸ“¦ å·²ç§»åŠ¨æŸåå‹ç¼©åŒ…: {os.path.basename(file_path)} -> æŸåå‹ç¼©åŒ…/{rel_path}')
        except Exception as e:
            logger.error(f'[#update] âŒ ç§»åŠ¨æŸåå‹ç¼©åŒ…å¤±è´¥ {file_path}: {str(e)}')

    def get_series_key(self, filename, handler=None):
        """è·å–ç”¨äºç³»åˆ—æ¯”è¾ƒçš„é”®å€¼"""
        logger.info(f'[#process] å¤„ç†æ–‡ä»¶: {filename}')
        test_group = [filename, filename]
        series_groups = UnclassifiedFunctions.find_series_groups(test_group)
        if series_groups:
            series_name = next(iter(series_groups.keys()))
            logger.info(f'[#process] æ‰¾åˆ°ç³»åˆ—åç§°: {series_name}')
            return series_name
        name = UnclassifiedFunctions.preprocess_filename(filename)
        name = UnclassifiedFunctions.normalize_chinese(name)
        logger.info(f'[#process] ä½¿ç”¨é¢„å¤„ç†ç»“æœ: {name}')
        if handler:
            handler.update_panel('series_extract', f'ä½¿ç”¨é¢„å¤„ç†ç»“æœ: {name}')
        return name.strip()

    @UnclassifiedFunctions.timeout(60)
    def count_images_in_archive(self, archive_path, handler=None):
        """ä½¿ç”¨7zçš„åˆ—è¡¨æ¨¡å¼ç»Ÿè®¡å‹ç¼©åŒ…ä¸­çš„å›¾ç‰‡æ•°é‡"""
        try:
            if UnclassifiedFunctions.is_archive_corrupted(archive_path):
                logger.warning(f'[#update] âš ï¸ å‹ç¼©åŒ…å·²æŸåï¼Œè·³è¿‡å¤„ç†: {archive_path}')
                return -1
            output = UnclassifiedFunctions.run_7z_command('l', archive_path, additional_args=['-slt'])
            if not output:
                logger.error(f'[#update] âŒ æ— æ³•è·å–å‹ç¼©åŒ…å†…å®¹åˆ—è¡¨: {archive_path}')
                return 0
            image_count = sum((1 for ext in IMAGE_EXTENSIONS if ext in output.lower()))
            logger.info(f"[#update] ğŸ“¦ å‹ç¼©åŒ… '{os.path.basename(archive_path)}' ä¸­åŒ…å« {image_count} å¼ å›¾ç‰‡")
            return image_count
        except TimeoutError as e:
            logger.error(f'[#update] âŒ å¤„ç†å‹ç¼©åŒ…è¶…æ—¶ {archive_path}: {str(e)}')
            return -1
        except Exception as e:
            logger.error(f'[#update] âŒ å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™ {archive_path}: {str(e)}')
            return -1

    def is_archive(self, path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæ”¯æŒçš„å‹ç¼©åŒ…æ ¼å¼"""
        return Path(path).suffix.lower() in {'.zip', '.rar', '.7z', '.cbz', '.cbr'}

    def find_series_groups(self, filenames, handler=None):
        """æŸ¥æ‰¾å±äºåŒä¸€ç³»åˆ—çš„æ–‡ä»¶ç»„ï¼Œä½¿ç”¨ä¸‰é˜¶æ®µåŒ¹é…ç­–ç•¥"""
        processed_names = {f: UnclassifiedFunctions.preprocess_filename(f) for f in filenames}
        processed_keywords = {f: UnclassifiedFunctions.get_keywords(processed_names[f]) for f in filenames}
        simplified_names = {f: UnclassifiedFunctions.normalize_chinese(n) for f, n in processed_names.items()}
        simplified_keywords = {f: [UnclassifiedFunctions.normalize_chinese(k) for k in kws] for f, kws in processed_keywords.items()}
        series_groups = defaultdict(list)
        remaining_files = set(filenames)
        matched_files = set()
        logger.info('[#process] ğŸ” é¢„å¤„ç†é˜¶æ®µï¼šæ£€æŸ¥å·²æ ‡è®°çš„ç³»åˆ—')
        for file_path in list(remaining_files):
            if file_path in matched_files:
                continue
            file_name = os.path.basename(file_path)
            for prefix in SERIES_PREFIXES:
                if file_name.startswith(prefix):
                    series_name = file_name[len(prefix):]
                    series_name = re.sub('\\[.*?\\]|\\(.*?\\)', '', series_name)
                    series_name = series_name.strip()
                    if series_name:
                        series_groups[series_name].append(file_path)
                        matched_files.add(file_path)
                        remaining_files.remove(file_path)
                        logger.info(f"[#process] âœ¨ é¢„å¤„ç†é˜¶æ®µï¼šæ–‡ä»¶ '{os.path.basename(file_path)}' å·²æ ‡è®°ä¸ºç³»åˆ— '{series_name}'")
                    break
        logger.info('[#process] ğŸ” ç¬¬ä¸€é˜¶æ®µï¼šé£æ ¼åŒ¹é…ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰')
        while remaining_files:
            best_length = 0
            best_common = None
            best_pair = None
            best_series_name = None
            for file1 in remaining_files:
                if file1 in matched_files:
                    continue
                keywords1 = simplified_keywords[file1]
                base_name1 = UnclassifiedFunctions.get_base_filename(os.path.basename(file1))
                for file2 in remaining_files - {file1}:
                    if file2 in matched_files:
                        continue
                    base_name2 = UnclassifiedFunctions.get_base_filename(os.path.basename(file2))
                    if base_name1 == base_name2:
                        logger.info(f"[#process] âœ¨ ç¬¬ä¸€é˜¶æ®µï¼šæ–‡ä»¶ '{os.path.basename(file1)}' å’Œ '{os.path.basename(file2)}' åŸºç¡€åå®Œå…¨ç›¸åŒï¼Œè·³è¿‡")
                        continue
                    keywords2 = simplified_keywords[file2]
                    common = UnclassifiedFunctions.find_longest_common_keywords(keywords1, keywords2)
                    if common:
                        original_kw1 = processed_keywords[file1]
                        original_common = original_kw1[keywords1.index(common[0]):keywords1.index(common[-1]) + 1]
                        series_name = UnclassifiedFunctions.validate_series_name(' '.join(original_common))
                        if series_name and len(common) > best_length:
                            best_length = len(common)
                            best_common = common
                            best_pair = (file1, file2)
                            best_series_name = series_name
            if best_pair and best_series_name:
                matched_files_this_round = set(best_pair)
                base_name1 = UnclassifiedFunctions.get_base_filename(os.path.basename(best_pair[0]))
                for other_file in remaining_files - matched_files_this_round - matched_files:
                    other_base_name = UnclassifiedFunctions.get_base_filename(os.path.basename(other_file))
                    if base_name1 == other_base_name:
                        continue
                    other_keywords = simplified_keywords[other_file]
                    common = UnclassifiedFunctions.find_longest_common_keywords(simplified_keywords[best_pair[0]], other_keywords)
                    if common == best_common:
                        matched_files_this_round.add(other_file)
                series_groups[best_series_name].extend(matched_files_this_round)
                remaining_files -= matched_files_this_round
                matched_files.update(matched_files_this_round)
                logger.info(f"[#process] âœ¨ ç¬¬ä¸€é˜¶æ®µï¼šé€šè¿‡å…³é”®è¯åŒ¹é…æ‰¾åˆ°ç³»åˆ— '{best_series_name}'")
                for file_path in matched_files_this_round:
                    logger.info(f"[#process] âœ¨ ç¬¬ä¸€é˜¶æ®µï¼šé€šè¿‡å…³é”®è¯åŒ¹é…æ‰¾åˆ°ç³»åˆ— '{best_series_name}'")
                    for file_path in matched_files_this_round:
                        logger.info(f"[#process] âœ¨ ç¬¬ä¸€é˜¶æ®µï¼šé€šè¿‡å…³é”®è¯åŒ¹é…æ‰¾åˆ°ç³»åˆ— '{best_series_name}'")
            else:
                break
        if remaining_files:
            if handler:
                handler.update_panel('series_extract', 'ğŸ” ç¬¬äºŒé˜¶æ®µï¼šå®Œå…¨åŸºç¡€ååŒ¹é…')
            existing_series = list(series_groups.keys())
            dir_path = os.path.dirname(list(remaining_files)[0])
            try:
                for folder_name in os.listdir(dir_path):
                    if os.path.isdir(os.path.join(dir_path, folder_name)):
                        for prefix in SERIES_PREFIXES:
                            if folder_name.startswith(prefix):
                                series_name = folder_name[len(prefix):]
                                if series_name not in existing_series:
                                    existing_series.append(series_name)
                                    if handler:
                                        handler.update_panel('series_extract', f"ğŸ“ ç¬¬äºŒé˜¶æ®µï¼šä»ç›®å½•ä¸­æ‰¾åˆ°å·²æœ‰ç³»åˆ— '{series_name}'")
                                break
            except Exception:
                pass
            matched_files_by_series = defaultdict(list)
            for file in list(remaining_files):
                if file in matched_files:
                    continue
                base_name = simplified_names[file]
                base_name_no_space = re.sub('\\s+', '', base_name)
                for series_name in existing_series:
                    series_base = UnclassifiedFunctions.normalize_chinese(series_name)
                    series_base_no_space = re.sub('\\s+', '', series_base)
                    if series_base_no_space in base_name_no_space:
                        base_name_current = UnclassifiedFunctions.get_base_filename(os.path.basename(file))
                        has_same_base = False
                        for existing_file in matched_files_by_series[series_name]:
                            if UnclassifiedFunctions.get_base_filename(os.path.basename(existing_file)) == base_name_current:
                                has_same_base = True
                                break
                        if not has_same_base:
                            matched_files_by_series[series_name].append(file)
                            matched_files.add(file)
                            remaining_files.remove(file)
                            if handler:
                                handler.update_panel('series_extract', f"âœ¨ ç¬¬äºŒé˜¶æ®µï¼šæ–‡ä»¶ '{os.path.basename(file)}' åŒ¹é…åˆ°å·²æœ‰ç³»åˆ— '{series_name}'ï¼ˆåŒ…å«ç³»åˆ—åï¼‰")
                        break
            for series_name, files in matched_files_by_series.items():
                series_groups[series_name].extend(files)
                if handler:
                    handler.update_panel('series_extract', f"âœ¨ ç¬¬äºŒé˜¶æ®µï¼šå°† {len(files)} ä¸ªæ–‡ä»¶æ·»åŠ åˆ°ç³»åˆ— '{series_name}'")
                    for file_path in files:
                        handler.update_panel('series_extract', f'  â””â”€ {os.path.basename(file_path)}')
        if remaining_files:
            if handler:
                handler.update_panel('series_extract', 'ğŸ” ç¬¬ä¸‰é˜¶æ®µï¼šæœ€é•¿å…¬å…±å­ä¸²åŒ¹é…')
            while remaining_files:
                best_ratio = 0
                best_pair = None
                best_common = None
                original_form = None
                for file1 in remaining_files:
                    if file1 in matched_files:
                        continue
                    base1 = simplified_names[file1]
                    base1_lower = base1.lower()
                    original1 = processed_names[file1]
                    base_name1 = UnclassifiedFunctions.get_base_filename(os.path.basename(file1))
                    for file2 in remaining_files - {file1}:
                        if file2 in matched_files:
                            continue
                        base_name2 = UnclassifiedFunctions.get_base_filename(os.path.basename(file2))
                        if base_name1 == base_name2:
                            continue
                        base2 = simplified_names[file2]
                        base2_lower = base2.lower()
                        matcher = difflib.SequenceMatcher(None, base1_lower, base2_lower)
                        ratio = matcher.ratio()
                        if ratio > best_ratio and ratio > 0.6:
                            best_ratio = ratio
                            best_pair = (file1, file2)
                            match = matcher.find_longest_match(0, len(base1_lower), 0, len(base2_lower))
                            best_common = base1_lower[match.a:match.a + match.size]
                            original_form = original1[match.a:match.a + match.size]
                if best_pair and best_common and (len(best_common.strip()) > 1):
                    matched_files_this_round = set(best_pair)
                    base_name1 = UnclassifiedFunctions.get_base_filename(os.path.basename(best_pair[0]))
                    for other_file in remaining_files - matched_files_this_round - matched_files:
                        other_base_name = UnclassifiedFunctions.get_base_filename(os.path.basename(other_file))
                        if base_name1 == other_base_name:
                            continue
                        other_base = simplified_names[other_file].lower()
                        if best_common in other_base:
                            matched_files_this_round.add(other_file)
                    series_name = UnclassifiedFunctions.validate_series_name(original_form)
                    if series_name:
                        series_groups[series_name].extend(matched_files_this_round)
                        remaining_files -= matched_files_this_round
                        matched_files.update(matched_files_this_round)
                        if handler:
                            handler.update_panel('series_extract', f"âœ¨ ç¬¬ä¸‰é˜¶æ®µï¼šé€šè¿‡å…¬å…±å­ä¸²åŒ¹é…æ‰¾åˆ°ç³»åˆ— '{series_name}'")
                            handler.update_panel('series_extract', f"  â””â”€ å…¬å…±å­ä¸²ï¼š'{best_common}' (ç›¸ä¼¼åº¦: {best_ratio:.2%})")
                            for file_path in matched_files_this_round:
                                handler.update_panel('series_extract', f"  â””â”€ æ–‡ä»¶ '{os.path.basename(file_path)}'")
                    else:
                        remaining_files.remove(best_pair[0])
                        matched_files.add(best_pair[0])
                else:
                    break
        if handler and remaining_files:
            handler.update_panel('series_extract', f'âš ï¸ è¿˜æœ‰ {len(remaining_files)} ä¸ªæ–‡ä»¶æœªèƒ½åŒ¹é…åˆ°ä»»ä½•ç³»åˆ—')
        return dict(series_groups)

    @UnclassifiedFunctions.timeout(60)
    def is_archive_corrupted(self, archive_path):
        """æ£€æŸ¥å‹ç¼©åŒ…æ˜¯å¦æŸå"""
        try:
            cmd = ['7z', 't', archive_path]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=55)
            return result.returncode != 0
        except subprocess.TimeoutExpired:
            raise TimeoutError(f'æ£€æŸ¥å‹ç¼©åŒ…å®Œæ•´æ€§è¶…æ—¶: {archive_path}')
        except Exception:
            return True

    @UnclassifiedFunctions.timeout(60)
    def run_7z_command(self, command, archive_path, operation='', additional_args=None, handler=None):
        """è¿è¡Œ7zå‘½ä»¤å¹¶è¿”å›è¾“å‡º"""
        try:
            cmd = ['7z', command]
            if additional_args:
                cmd.extend(additional_args)
            cmd.append(archive_path)
            result = subprocess.run(cmd, capture_output=True, text=False, timeout=55)
            try:
                output = result.stdout.decode('cp932')
            except UnicodeDecodeError:
                try:
                    output = result.stdout.decode('utf-8')
                except UnicodeDecodeError:
                    output = result.stdout.decode('utf-8', errors='replace')
            return output if output else ''
        except subprocess.TimeoutExpired:
            raise TimeoutError(f'7zå‘½ä»¤æ‰§è¡Œè¶…æ—¶: {archive_path}')
        except Exception as e:
            if handler:
                logger.error(f'[#update] âŒ æ‰§è¡Œ7zå‘½ä»¤æ—¶å‡ºé”™ {archive_path}: {str(e)}')
            return ''

    def preprocess_filename(self, filename):
        """é¢„å¤„ç†æ–‡ä»¶å"""
        name = os.path.basename(filename)
        name = name.rsplit('.', 1)[0]
        for prefix in SERIES_PREFIXES:
            if name.startswith(prefix):
                name = name[len(prefix):]
                break
        name = re.sub('\\[.*?\\]', '', name)
        name = re.sub('\\(.*?\\)', '', name)
        name = ' '.join(name.split())
        return name

    def get_keywords(self, name):
        """å°†æ–‡ä»¶ååˆ†å‰²ä¸ºå…³é”®è¯åˆ—è¡¨"""
        return name.strip().split()

    def get_base_filename(self, filename):
        """è·å–å»é™¤æ‰€æœ‰æ ‡ç­¾åçš„åŸºæœ¬æ–‡ä»¶å"""
        name = os.path.splitext(filename)[0]
        name = re.sub('\\[[^\\]]*\\]', '', name)
        name = re.sub('\\([^)]*\\)', '', name)
        name = re.sub('[\\s!ï¼?ï¼Ÿ_~ï½]+', '', name)
        name = UnclassifiedFunctions.normalize_chinese(name)
        return name

    def find_longest_common_keywords(self, keywords1, keywords2):
        """æ‰¾å‡ºä¸¤ä¸ªå…³é”®è¯åˆ—è¡¨ä¸­æœ€é•¿çš„è¿ç»­å…¬å…±éƒ¨åˆ†"""
        matcher = difflib.SequenceMatcher(None, keywords1, keywords2)
        match = matcher.find_longest_match(0, len(keywords1), 0, len(keywords2))
        return keywords1[match.a:match.a + match.size]

    def validate_series_name(self, name):
        """éªŒè¯å’Œæ¸…ç†ç³»åˆ—åç§°
        
        Args:
            name: åŸå§‹ç³»åˆ—åç§°
            
        Returns:
            æ¸…ç†åçš„æœ‰æ•ˆç³»åˆ—åç§°ï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å›None
        """
        if not name or len(name) <= 1:
            return None
        name = UnclassifiedFunctions.normalize_chinese(name)
        name = re.sub('[\\s.ï¼ã€‚Â·ãƒ»+ï¼‹\\-ï¼â€”_ï¼¿\\d]+$', '', name)
        name = re.sub('[ç¬¬ç« è¯é›†å·æœŸç¯‡å­£éƒ¨å†Œä¸Šä¸­ä¸‹å‰åå®Œå…¨][ç¯‡è¯é›†å·æœŸç« èŠ‚éƒ¨å†Œå…¨]*$', '', name)
        name = re.sub('(?i)vol\\.?\\s*\\d*$', '', name)
        name = re.sub('(?i)volume\\s*\\d*$', '', name)
        name = re.sub('(?i)part\\s*\\d*$', '', name)
        name = name.strip()
        if re.search('(?i)comic', name):
            return None
        words = name.split()
        if all((len(word) <= 1 for word in words)) and len(''.join(words)) <= 3:
            return None
        if not name or len(name) <= 1 or (len(name) > 0 and len(name.split()[-1]) <= 1):
            return None
        return name

    def timeout(self, seconds):
        """è¶…æ—¶è£…é¥°å™¨"""
    
        def decorator(self, func):
    
            @functools.wraps(func)
            def wrapper(self, *args, **kwargs):
    
                def handler(self, signum, frame):
                    raise TimeoutError(f'å‡½æ•°æ‰§è¡Œè¶…æ—¶ ({seconds}ç§’)')
                if sys.platform != 'win32':
                    original_handler = signal.signal(signal.SIGALRM, handler)
                    signal.alarm(seconds)
                else:
                    timer = threading.Timer(seconds, lambda: threading._shutdown())
                    timer.start()
                try:
                    result = func(*args, **kwargs)
                finally:
                    if sys.platform != 'win32':
                        signal.alarm(0)
                        signal.signal(signal.SIGALRM, original_handler)
                    else:
                        timer.cancel()
                return result
            return wrapper
        return decorator

    def normalize_chinese(self, text):
        """æ ‡å‡†åŒ–ä¸­æ–‡æ–‡æœ¬ï¼ˆç»Ÿä¸€è½¬æ¢ä¸ºç®€ä½“ï¼‰"""
        return text
